<!DOCTYPE html>
<html lang="en">
<head>
  <!-- stylesheets -->
	<link rel="stylesheet" type="text/css" href="/theme/css/style.css">
	<link rel="stylesheet" type="text/css" href="/theme/css/pygments.css">
	<link rel="stylesheet" type="text/css" href="/theme/lightbox/css/lightbox.css">
	<link rel="stylesheet" type="text/css" href="/theme/font-awesome-4.3.0/css/font-awesome.min.css">

  <!-- fonts -->
	<link href='http://fonts.googleapis.com/css?family=Open+Sans:800,400,300|Inconsolata' rel='stylesheet' type='text/css'>
	<link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:700,900' rel='stylesheet' type='text/css'>
	<link href='http://fonts.googleapis.com/css?family=Lora:700' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Playball' rel='stylesheet' type='text/css'>

  <!-- RSS/ATOM feeds -->
	<link href="None/" type="application/atom+xml" rel="alternate" title="Strongly Convex ATOM Feed" />


		<title>Strongly Convex</title>
		<meta charset="utf-8" />
</head>
<body>
	<section id="sidebar">
		<figure id="user_logo">
      <a href=""><div class="logo">&nbsp;</div></a>
		</figure>

		<div class="user_meta">
      <h1 id="user">
        <a  href="/"
            class="sitename">
          Strongly Convex
        </a>
      </h1>
			<h2></h2>


			<ul>
        <hr></hr>
					<li><a href="/">Words</a></li>
          <hr></hr>
					<li><a href="/code.html">Code</a></li>
          <hr></hr>
					<li><a href="/about.html">About</a></li>
          <hr></hr>
			</ul>
		</div>
	</section>

	<section id="posts">
	<header>
      <h1 class="title">
			  <a  href="/blog/topic-models-arent-hard.html"
            rel="bookmark"
			  	  title="Permalink to Topic Models aren't hard">
          Topic Models aren't hard
        </a>
      </h1>
		  <abbr class="published">Mon 21 January 2013</abbr>
	</header>
  <article>
    <p>In 2002, <a href="http://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf">Latent Dirichlet Allocation</a> (LDA) was published at NIPS, one
of the most highly regarded conferences for research loosely labeled as
"Artificial Intelligence". The next 5 or so years led to a flurry of
incremental model extensions and alternative inference methods, though none
have achieved the popularity of their namesake.</p>
<p>Latent Dirichlet Allocation -- an extremely complex name for a not-so-complex
idea. In this post, I will explain what the LDA model says, what it does <em>not</em>
say, and how we as researchers should look at it.</p>
<h1><a name="model" href="#model">The Model</a></h1>
<p>Let's begin by appreciating Latent Dirichlet Allocation in its most natural
form -- the graphical model.  I hope you like Greek letters...</p>
<div class="img-center">
  <img src="/assets/img/lda/graphical-model.png"></img>
</div>

<p>About now, you should have an ephemeral feeling of happiness and
understanding beyond anything you've ever experienced before, as if your eyes
had just opened for the first time.  Do you feel it? No? Yeah, I didn't think
so.</p>
<p>Let's break it down a little, without the math.  Take a look at the following
4 plots.  Each subplot contains samples drawn from 1 of 3 clusters, and each
plot contains samples from the same clusters.  The difference between each
subplot is that the <em>number of samples</em> from each cluster is different.</p>
<div class="img-center">
  <img src="/assets/img/lda/gaussians-nocolor.jpg"></img>
</div>

<p>Having trouble?  It's a rather difficult problem, especially with only 4
subplots. What if you had a 100,000 subplots instead? Do you think you could
figure it out then?  Here's a plot of the same data with points colored
according to their cluster,</p>
<div class="img-center">
  <img src="/assets/img/lda/gaussians-color.jpg"></img>
</div>

<p>Even if you don't realize it yet, you now understand Latent Dirichlet
Allocation. In fact, Latent Dirichlet Allocation is just an extension of the
lowly Mixture Model.  "How so?", you ask?  Well let's look at how we might
generate data from a Mixture Model.</p>
<p>In a mixture model, each data point is sampled independently. The algorithm
for generating a sample given the model's parameters is given by the following
python snippet.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sample_mixture_model</span><span class="p">(</span><span class="n">n_data_points</span><span class="p">,</span> <span class="n">cluster_weights</span><span class="p">,</span> <span class="n">cluster_parameters</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_data_points</span><span class="p">):</span>
    <span class="n">cluster</span> <span class="o">=</span> <span class="n">sample_categorical</span><span class="p">(</span><span class="n">cluster_weights</span><span class="p">)</span>
    <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span> <span class="o">=</span> <span class="n">cluster_parameters</span><span class="p">[</span><span class="n">cluster</span><span class="p">]</span>
    <span class="k">yield</span> <span class="n">sample_normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">variance</span><span class="p">)</span>
</pre></div>


<p>Simple, right?  First, a cluster is chosen for this data point. Each cluster
has some probability of being chosen, given by <code>cluster_weights[i]</code>.  Once a
cluster has been chosen, the data point is generated from a Normal distribution
with mean and covariance specific to the cluster.  The idea is that each
cluster has its own mean and covariance, so with enough samples we'll be able
to tell the clusters apart.</p>
<p>So how does this relate to LDA?  In LDA, each "document" (in our case,
subplot) is nothing more than a Mixture Model. The novel part of LDA is that
there isn't just one document that we see a ton of samples from, but many
documents that we only see a few samples from.  Furthermore, each document has
its own version of <code>cluster_weights</code> -- our only boon is that all documents
share the same <code>cluster_parameters</code>.</p>
<p>To make that concrete, let's look at how we would generate samples from LDA.</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sample_lda</span><span class="p">(</span><span class="n">n_data_points_per_document</span><span class="p">,</span> <span class="n">all_cluster_weights</span><span class="p">,</span> <span class="n">cluster_parameters</span><span class="p">):</span>
  <span class="n">n_documents</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_cluster_weights</span><span class="p">)</span>  <span class="c1"># number of documents</span>
  <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_documents</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">sample_mixture_model</span><span class="p">(</span><span class="n">n_data_points_per_document</span><span class="p">,</span>
                                       <span class="n">all_cluster_weights</span><span class="p">[</span><span class="n">d</span><span class="p">],</span>
                                       <span class="n">cluster_parameters</span><span class="p">):</span>
      <span class="k">yield</span> <span class="p">{</span>
        <span class="s1">&#39;document_number&#39;</span><span class="p">:</span> <span class="n">d</span><span class="p">,</span>
        <span class="s1">&#39;data_point&#39;</span><span class="p">:</span> <span class="n">sample</span>
      <span class="p">}</span>
</pre></div>


<p>Notice that here we don't just return the data point by itself.  In LDA, we
know which "document" each data point comes from, which is just a little bit
more information than we have in a regular old Mixture Model.</p>
<p>Finally, I have to admit that I lied a little.  What I've described so far
isn't <em>quite</em> LDA, but it's pretty damn close.  In the above pseudocode, I
assumed that the model parameters were already given, but LDA actually
assumes the parameters are unknown and defines a probability distribution over
them (a <a href="http://en.wikipedia.org/wiki/Dirichlet_distribution">Dirichlet distribution</a>, in fact).  Secondly, the examples
above generate data points from Normal distributions where as LDA generates
samples from the <a href="http://en.wikipedia.org/wiki/Multinomial_distribution">Multinomial distribution</a>. Other than that, you
now understand Latent Dirichlet Allocation, the core of nearly every Topic
Model in existence.</p>
<h1><a name="appendix" href="#appendix">Appendix</a></h1>
<p>Here's the MATLAB code for generating the two plots above.</p>
<div class="highlight"><pre><span></span><span class="c">%% parameters</span>
<span class="n">n_samples</span> <span class="p">=</span> <span class="mi">200</span><span class="p">;</span>
<span class="n">n_clusters</span> <span class="p">=</span> <span class="mi">3</span><span class="p">;</span>
<span class="n">n_documents</span> <span class="p">=</span> <span class="mi">4</span><span class="p">;</span>

<span class="c">%% reset ye olde random seed</span>
<span class="n">s</span> <span class="p">=</span> <span class="n">RandStream</span><span class="p">(</span><span class="s">&#39;mcg16807&#39;</span><span class="p">,</span><span class="s">&#39;Seed&#39;</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span>
<span class="n">RandStream</span><span class="p">.</span><span class="n">setDefaultStream</span><span class="p">(</span><span class="n">s</span><span class="p">);</span>

<span class="c">%% generate parameters for each cluster</span>
<span class="k">for</span> <span class="nb">i</span> <span class="p">=</span> <span class="mi">1</span><span class="p">:</span><span class="n">n_clusters</span>
  <span class="n">mu</span><span class="p">(:,</span><span class="nb">i</span><span class="p">)</span> <span class="p">=</span> <span class="nb">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span>
  <span class="n">sigma</span><span class="p">(:,:,</span><span class="nb">i</span><span class="p">)</span> <span class="p">=</span> <span class="nb">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="nb">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">);</span>
  <span class="n">sigma</span><span class="p">(:,:,</span><span class="nb">i</span><span class="p">)</span> <span class="p">=</span> <span class="n">sigma</span><span class="p">(:,:,</span><span class="nb">i</span><span class="p">)</span> <span class="o">+</span> <span class="n">sigma</span><span class="p">(:,:,</span><span class="nb">i</span><span class="p">)</span><span class="o">&#39;</span><span class="p">;</span>
  <span class="n">sigma</span><span class="p">(:,:,</span><span class="nb">i</span><span class="p">)</span> <span class="p">=</span> <span class="n">sigma</span><span class="p">(:,:,</span><span class="nb">i</span><span class="p">)</span> <span class="o">/</span> <span class="mi">250</span><span class="p">;</span>
<span class="k">end</span>

<span class="n">figure</span><span class="p">;</span>
<span class="n">hold</span> <span class="n">on</span><span class="p">;</span>

<span class="k">for</span> <span class="n">d</span> <span class="p">=</span> <span class="mi">1</span><span class="p">:</span><span class="n">n_documents</span>
  <span class="c">%% generate document-specific weights</span>
  <span class="n">w</span> <span class="p">=</span> <span class="nb">rand</span><span class="p">(</span><span class="n">n_clusters</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span>
  <span class="n">w</span> <span class="p">=</span> <span class="n">w</span> <span class="o">/</span> <span class="n">sum</span><span class="p">(</span><span class="n">w</span><span class="p">);</span>

  <span class="c">%% generate samples for this document</span>
  <span class="k">for</span> <span class="nb">i</span> <span class="p">=</span> <span class="mi">1</span><span class="p">:(</span><span class="n">n_samples</span> <span class="o">*</span> <span class="nb">rand</span><span class="p">())</span>
    <span class="n">c</span><span class="p">(:,</span><span class="nb">i</span><span class="p">)</span> <span class="p">=</span> <span class="n">mnrnd</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">w</span><span class="p">);</span>
    <span class="n">z</span><span class="p">(</span><span class="nb">i</span><span class="p">)</span> <span class="p">=</span> <span class="nb">find</span><span class="p">(</span><span class="n">c</span><span class="p">(:,</span><span class="nb">i</span><span class="p">));</span>
    <span class="n">x</span><span class="p">(:,</span><span class="nb">i</span><span class="p">)</span> <span class="p">=</span> <span class="n">mvnrnd</span><span class="p">(</span><span class="n">mu</span><span class="p">(:,</span><span class="n">z</span><span class="p">(</span><span class="nb">i</span><span class="p">)),</span> <span class="n">sigma</span><span class="p">(:,:,</span><span class="n">z</span><span class="p">(</span><span class="nb">i</span><span class="p">)));</span>
  <span class="k">end</span>

  <span class="c">%% plotting! yay!</span>
  <span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">d</span><span class="p">);</span>

  <span class="c">% without color</span>
  <span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">(</span><span class="mi">1</span><span class="p">,:),</span> <span class="n">x</span><span class="p">(</span><span class="mi">2</span><span class="p">,:));</span>
  <span class="c">% with color</span>
  <span class="c">% scatter(x(1,:), x(2,:), &#39;CData&#39;, c&#39;);</span>
<span class="k">end</span>
</pre></div>

    <div id="article_meta">
        Category:
          <a href="/category/topic-models.html">topic-models</a>
        <br />Tags:
          <a href="/tag/topic-models.html">topic-models</a>
,           <a href="/tag/bayesian.html">bayesian</a>
,           <a href="/tag/lda.html">lda</a>
    </div>
  </article>

  <footer>
    <a href="/" class="button_accent">&larr;&nbsp;&nbsp;&nbsp;Back to blog</a>
  </footer>

  <div id="comments">
    <h2>Comments</h2>
    <div id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_identifier = "blog/topic-models-arent-hard.html";
      (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://duckworthd-blog.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>Please enable JavaScript to view <a href="http://disqus.com/?ref_noscript">comments</a>.</noscript>
  </div>

	</section>


  <!-- scripts -->
  <script
    type= "text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    >
  </script>
  <script
    type= "text/javascript"
    src="/theme/mathjax/MathJaxLocal.js"
    >
  </script>
  <script
      type="text/javascript"
      src="/theme/lightbox/js/jquery-1.11.0.min.js"
      >
  </script>
  <script
      type="text/javascript"
      src="/theme/lightbox/js/lightbox.min.js"
      >
  </script>
</body>
</html>