<!DOCTYPE html>
<html lang="en">
<head>
  <!-- stylesheets -->
	<link rel="stylesheet" type="text/css" href="/theme/css/style.css">
	<link rel="stylesheet" type="text/css" href="/theme/css/pygments.css">
	<link rel="stylesheet" type="text/css" href="/theme/lightbox/css/lightbox.css">
	<link rel="stylesheet" type="text/css" href="/theme/font-awesome-4.3.0/css/font-awesome.min.css">

  <!-- fonts -->
	<link href='http://fonts.googleapis.com/css?family=Open+Sans:800,400,300|Inconsolata' rel='stylesheet' type='text/css'>
	<link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:700,900' rel='stylesheet' type='text/css'>
	<link href='http://fonts.googleapis.com/css?family=Lora:700' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Playball' rel='stylesheet' type='text/css'>

  <!-- RSS/ATOM feeds -->
	<link href="None/" type="application/atom+xml" rel="alternate" title="Strongly Convex ATOM Feed" />


		<title>Strongly Convex</title>
		<meta charset="utf-8" />
</head>
<body>
	<section id="sidebar">
		<figure id="user_logo">
      <a href=""><div class="logo">&nbsp;</div></a>
		</figure>

		<div class="user_meta">
      <h1 id="user">
        <a  href="/"
            class="sitename">
          Strongly Convex
        </a>
      </h1>
			<h2></h2>


			<ul>
        <hr></hr>
					<li><a href="/">Words</a></li>
          <hr></hr>
					<li><a href="/code.html">Code</a></li>
          <hr></hr>
					<li><a href="/about.html">About</a></li>
          <hr></hr>
			</ul>
		</div>
	</section>

	<section id="posts">
	<header>
      <h1 class="title">
			  <a  href="/blog/accelerated-gradient-descent.html"
            rel="bookmark"
			  	  title="Permalink to Accelerated Gradient Descent">
          Accelerated Gradient Descent
        </a>
      </h1>
		  <abbr class="published">Fri 12 April 2013</abbr>
	</header>
  <article>
    <p>In the mid-1980s, Yurii Nesterov hit the equivalent of an academic home run.
At the same time, he established the Accelerated Gradient Method, proved that
its convergence rate superior to Gradient Descent ($O(1/\sqrt{\epsilon})$
iterations instead of $O(1/\epsilon)$), and then proved that no other
first-order (that is, gradient-based) algorithm could ever hope to beat it.
What a boss.</p>
<p><a href="/blog/gradient-descent.html">Continuing</a> <a href="/blog/subgradient-descent.html">my analogy</a>, imagine
that you are standing on the side of a mountain and want to reach the bottom.
If you were to follow the Accelerated Gradient Method, you'd do something like
this,</p>
<div class="pseudocode">
<ol>
<li>Look around you and see which way points the most dowards</li>
<li>Take a step in that direction</li>
<li>Take another step in that direction, even if it starts taking you uphill.
     Then repeat.</li>
</ol>
</div>
<p>As we'll see, that last unintuitive bit is the key to Accelerated Gradient
Descent's speed.</p>
<h1><a name="implementation" href="#implementation">How does it work?</a></h1>
<p>We begin by assuming the we want to minimize an unconstrained function $f$,</p>
<p>$$
  \min_{x} \, f(x)
$$</p>
<p>As with Gradient Descent, we'll assume that $f$ is differentiable and that we
can easily compute its gradient $\nabla f(x)$. Then the Accelerated Gradient
Method is defined as,</p>
<div class="pseudocode">
<p><strong>Input</strong>: initial iterate $y^{(0)}$</p>
<ol>
<li>For $t = 1, 2, \ldots$<ol>
<li>Let $x^{(t)} = y^{(t-1)} - \alpha^{(t)} \nabla f(y^{(t-1)})$</li>
<li>if converged, return $x^{(t)}$</li>
<li>Let $y^{(t)} = x^{(t)} + \frac{t-1}{t+2} (x^{(t)} - x^{(t-1)})$</li>
</ol>
</li>
</ol>
</div>
<h1><a name="example" href="#example">A Small Example</a></h1>
<p>We'll try out Accelerated Gradient on the <a href="/blog/gradient-descent.html#example">same example used to showcase
Gradient Descent</a>. We'll use the objective function
$f(x) = x^4$, meaning that $\nabla_x f(x) = 4 x^3$. For a step size, we'll use
Backtracking Line Search where the largest acceptable step size is $0.05$.
Finally, we'll start at $x^{(0)} = 1$.  Compare these 2 graphs to the ones for
Gradient Descent.</p>
<div class="img-center">
  <img src="/assets/img/accelerated_gradient/convergence.png"></img>
  <span class="caption">
    This plot shows how quickly the objective function decreases as the
    number of iterations increases.
  </span>
</div>

<div class="img-center">
  <img src="/assets/img/accelerated_gradient/iterates.png"></img>
  <span class="caption">
    This plot shows the actual iterates and the objective function evaluated at
    those points. More red indicates a higher iteration number.
  </span>
</div>

<h1><a name="proof" href="#proof">Why does it work?</a></h1>
<p>The assumptions for the Accelerated Gradient Method are identical to Gradient
Descent's. In particular, we assume,</p>
<ol>
<li>$f$ is convex, differentiable, and finite for all $x$</li>
<li>a finite solution $x^{*}$ exists</li>
<li>$\nabla f(x)$ is Lipschitz continuous with constant $L$. That is, there must
   be an $L$ such that,</li>
</ol>
<p>$$
  || \nabla f(x) - \nabla f(y) ||_2 \le L || x - y ||_2 \qquad \forall x,y
$$</p>
<p><strong>Assumptions</strong> As explained in my post on <a href="/blog/gradient-descent.html">Gradient
Descent</a>, these assumptions give us 2 things. Assumption 1
gives us a linear lower bound on $f$,</p>
<p>$$
  f(y) \ge f(x) + \nabla f(x)^T (y-x) \qquad \forall x, y
$$</p>
<p>Assumption 3 then gives us a quadratic upper bound on $f$,</p>
<p>$$
  f(y) \le f(x) + \nabla f(x)^T (y-x) + \frac{L}{2} ||x - y||_2^2 \qquad \forall x, y
$$</p>
<p>Both of these will prove critical in the following proof.</p>
<p><strong>Proof Outline</strong> The proof for the Accelerated Gradient Method is the
trickiest one yet. We'll see that everything fits into place just right, but
that there's very little intuition as to where it all came from.</p>
<p>We begin in Step 1 by defining a third iterate $v^{(t)}$ that's a
linear combination of $x^{(t)}$ and $x^{(t-1)}$. These iterates are purely for
the sake of the proof and are not computed during the algorithm. Using these
iterates, we upper bound $f(x^{(t+1)})$ in terms of $f(x^{(t)})$, $f(x^{<em>})$,
the distance between $v^{(t+1)}$ and $x^{</em>}$, and the distance between
$v^{(t)}$ and $x^{*}$.</p>
<p>Step 2 involves upper bounding the error $f(x^{(t+1)}) - f(x^{<em>})$ by $f(x^{0})
- f(x^{</em>})$ and the distance between $v^{(0)}$ and $x^{*}$ using our very
careful choice of $\frac{t-1}{t+2}$.</p>
<p>Finally, Step 3 brings it all together by bounding $f(x^{(t+1)}) - f(x^{*})$ by
a term depending on $1/(t+2)^2$.</p>
<p><strong>Step 1</strong> upper bounding $f(x^{(t+1)})$.  First, define the following and assume that $\theta^{(0)} = 1$, $v^{(0)} = x^{(0)}$,</p>
<p>$$
  v^{(t)}
  = \frac{t+1}{2} x^{(t)} - \frac{t-1}{2} x^{(t-1)} \qquad
  \theta^{(t)}
  = \frac{2}{t+2}
$$</p>
<p>There are 2 consequences of this definition for $v^{(t)}$. The first is that $y^{(t)}$ is a linear combination of $v^{(t)}$ and $x^{(t)}$,</p>
<p>$$
\begin{align<em>}
  \color{fuchsia} y^{(t)}
  &amp; = x^{(t)} + \frac{t-1}{t+2} ( x^{(t)} - x^{(t-1)} ) \
  &amp; = \frac{(t+2) + (t-1)}{t+2} x^{(t)} - \frac{t-1}{t+2} x^{(t-1)} \
  &amp; = \frac{t+1}{t+2} x^{(t)} - \frac{t-1}{t+2} x^{(t-1)} + \frac{t}{t+2} x^{(t)} \
  &amp; = \frac{2}{t+2} \left( \frac{t+1}{2} x^{(t)} - \frac{t-1}{2} x^{(t-1)} \right) + (1 - \frac{2}{t+2}) x^{(t)} \
  &amp; = \color{fuchsia} \theta^{(t)} v^{(t)} + (1 - \theta^{(t)}) x^{(t)} \
\end{align</em>}
$$</p>
<p>The second is that $v^{(t)}$ is a gradient step on $v^{(t)}$, except that the gradient is evaluated at $y^{(t)}$ (work backwards from the following),</p>
<p>$$
\begin{align<em>}
  \color{OrangeRed} v^{(t+1)}
  &amp; = \color{OrangeRed} v^{(t)} - \frac{\alpha^{(t+1)}}{\theta^{(t)}} \nabla f(y^{(t)}) \
  &amp; = v^{(t)} + \frac{1}{\theta^{(t)}} ( y^{(t)} - \alpha^{(t+1)} \nabla f(y^{(t)}) - y^{(t)} ) \
  &amp; = v^{(t)} + \frac{1}{\theta^{(t)}} ( x^{(t+1)} - y^{(t)} ) \
  \frac{t+2}{2} x^{(t+1)} - \frac{t}{2} x^{(t)}
  &amp; = \frac{t+1}{2} x^{(t)} - \frac{t-1}{2} x^{(t-1)} + \frac{1}{\theta^{(t)}} ( x^{(t+1)} - y^{(t)} ) \
  &amp; = \frac{t+1}{2} x^{(t)} - \frac{t-1}{2} x^{(t-1)} + \frac{t+2}{2} ( x^{(t+1)} - y^{(t)} ) \
  - \frac{t}{2} x^{(t)}
  &amp; = \frac{t+1}{2} x^{(t)} - \frac{t-1}{2} x^{(t-1)} - \frac{t+2}{2} y^{(t)} \
  y^{(t)}
  &amp; = \frac{2t+1}{t+2} x^{(t)} - \frac{t-1}{t+2} x^{(t-1)} \
  &amp; = x^{(t)} + \frac{t-1}{t+2} ( x^{(t)} - x^{(t-1)} )
\end{align</em>}
$$</p>
<p>With these 2 properties in hand, let's upper bound $f(x^{(t)})$.  Let $\alpha^{(t)} \le \frac{1}{L}$ and define
$x^{+} \triangleq x^{(t)}$,
$x \triangleq x^{(t-1)}$,
$v^{+} \triangleq v^{(t)}$,
$v \triangleq v^{(t-1)}$,
$\alpha^{+} \triangleq \alpha^{(t)}$,
$y \triangleq y^{(t-1)}$,
$\theta \triangleq \theta^{(t-1)}$</p>
<p>$$
\begin{align<em>}
  f(x^{+})
  &amp; = f(y - \alpha^{+} \nabla f(y)) \
  &amp; \le {\color{red} f(y) - \frac{\alpha^{+}}{2} || \nabla f(y) ||_2^2} \
  &amp; \le {\color{blue} (1-\theta) f(x) + \theta f(x^{</em>}) + \nabla f(y)^T (y - (1-\theta) x + \theta x^{<em>}) } - \frac{\alpha^{+}}{2} || \nabla f(y) ||_2^2 \
  &amp; = (1-\theta) f(x) + \theta f(x^{</em>}) + \nabla f(y)^T ({\color{fuchsia} \theta v} + \theta x^{<em>}) - \frac{\alpha^{+}}{2} || \nabla f(y) ||_2^2 \
  &amp; = (1-\theta) f(x) + \theta f(x^{</em>}) + \frac{\theta^2}{2 \alpha^{+}} \left(
        \frac{2 \alpha^{+}}{\theta} \nabla f(y)^T (v-x) - \left( \frac{\alpha^{+}}{\theta} \right)^2 || \nabla f(y) ||_2^2
        \pm ||v - x^{<em>}||_2^2
      \right) \
  &amp; = (1-\theta) f(x) + \theta f(x^{</em>}) + \frac{\theta^2}{2 \alpha^{+}} \left(
        ||v - x^{<em>}||_2^2 - ||v - x^{</em>} - \frac{\alpha^{+}}{\theta} \nabla f(y)||_2^2 
      \right) \
  &amp; = (1-\theta) f(x) + \theta f(x^{<em>}) + \frac{\theta^2}{2 \alpha^{+}} \left(
        ||v - x^{</em>}||_2^2 - ||{\color{OrangeRed} v^{+} } - x^{<em>}||_2^2
      \right) \
\end{align</em>}
$$</p>
<p>Using the Lipschitz assumption on $||\nabla f(y)||_2$ and that $\alpha^{+} \le \frac{1}{L}$,</p>
<p>$$
\begin{align<em>}
  \color{red}
  f(y - \alpha^{+} \nabla f(y))
  &amp; \le f(y) + \nabla f(y)^T (y - \alpha^{+} \nabla f(y) - y) + \frac{L}{2}{ ||y - \alpha^{+} \nabla f(y) - y ||_2^2 } \
  &amp; = f(y) - \alpha^{+} || \nabla f(y) ||_2^2 + \frac{L (\alpha^{+})^2}{2} || \nabla f(y) ||_2^2 \
  &amp; = f(y) - \frac{\alpha^{+}}{2} ( 2 - L \alpha^{+} ) || \nabla f(y) ||_2^2 \
  &amp; \le \color{red} f(y) - \frac{\alpha^{+}}{2} || \nabla f(y) ||_2^2 \
\end{align</em>}
$$</p>
<p>Using convexity's linear lower bound and its line-over-function properties,</p>
<p>$$
\begin{align<em>}
  \color{blue}
  f( (1-\theta)x + \theta x^{</em>} )
  &amp; \ge f(y) + \nabla f(y)^T ( (1-\theta)x + \theta x^{<em>} - y) \
  (1-\theta) f(x) + \theta f(x^{</em>}) 
  &amp; \ge f(y) + \nabla f(y)^T ( (1-\theta)x + \theta x^{<em>} - y) \
  f(y)
  &amp; \le \color{blue} (1-\theta) f(x) + \theta f(x^{</em>}) + \nabla f(y)^T(y - (1-\theta) x + \theta x^{<em>} )
\end{align</em>}
$$</p>
<p><strong>Step 2</strong> Upper bounding $f(x^{(t+1)}) - f(x^{*})$. We begin by manipulating the result from step 1 into something such that the left and right side look almost the same,</p>
<p>$$
\begin{align<em>}
  f(x^{(t+1)})
  &amp; \le (1-\theta^{(t)}) f(x^{(t)}) + \theta^{(t)} f(x^{</em>}) \
  &amp; \quad + \frac{ (\theta^{(t)})^2}{2 \alpha^{(t+1)}} \left(
        ||v^{(t)} - x^{<em>}||_2^2 - \underbrace{ ||v^{(t+1)} - x^{</em>}||<em _text_move="\text{move" other side to>2^2 }</em>}
     \right) \
  f(x^{(t+1)}) + \frac{ (\theta^{(t)})^2}{2 \alpha^{(t+1)}} ||v^{(t+1)} - x^{<em>}||_2^2
  &amp; \le (1-\theta^{(t)}) f(x^{(t)}) + \theta^{(t)} f(x^{</em>}) \
  &amp; \quad + \frac{ (\theta^{(t)})^2}{2 \alpha^{(t+1)}} ||v^{(t)} - x^{<em>}||_2^2 \underbrace{ \pm f(x^{</em>}) }<em _text_group="\text{group" together>{=0} \
  f(x^{(t+1)}) - f(x^{<em>}) + \frac{ (\theta^{(t)})^2}{2 \alpha^{(t+1)}} ||v^{(t+1)} - x^{</em>}||_2^2
  &amp; \le (1-\theta^{(t)}) f(x^{(t)}) \underbrace{ - f(x^{<em>}) + \theta^{(t)} f(x^{</em>}) }</em>} \
  &amp; \quad + \frac{ (\theta^{(t)})^2}{2 \alpha^{(t+1)}} ||v^{(t)} - x^{<em>}||_2^2 \
  f(x^{(t+1)}) - f(x^{</em>}) + \frac{ (\theta^{(t)})^2}{2 \alpha^{(t+1)}} ||v^{(t+1)} - x^{<em>}||_2^2
  &amp; \le (1-\theta^{(t)}) f(x^{(t)}) - (1-\theta^{(t)}) f(x^{</em>}) \
  &amp; \quad + \frac{ (\theta^{(t)})^2}{2 \alpha^{(t+1)}} ||v^{(t)} - x^{<em>}||_2^2 \
  \frac{1}{( \theta^{(t)} )^2} \left( f(x^{(t+1)}) - f(x^{</em>}) \right) + \frac{1}{2 \alpha^{(t+1)}} ||v^{(t+1)} - x^{<em>}||_2^2
  &amp; \le \frac{ (1-\theta^{(t)}) }{ (\theta^{(t)})^2 } \left( f(x^{(t)}) - f(x^{</em>}) \right) \
  &amp; \quad + \frac{1}{2 \alpha^{(t+1)}} ||v^{(t)} - x^{<em>}||_2^2 \
\end{align</em>}
$$</p>
<p>Remember how $\theta^{(t)} = \frac{2}{t+2}$? Well that means $\theta^{(t)} &gt; 0$ and thus $\frac{1 - \theta^{(t)}}{ (\theta^{(t)})^2 } \le \frac{1}{ (\theta^{(t)})^2 }$. Subbing that into the previous equation lets us apply it recursively to obtain,</p>
<p>$$
\begin{align<em>}
  \frac{1}{( \theta^{(t)} )^2} \left( f(x^{(t+1)}) - f(x^{</em>}) \right) + \frac{1}{2 \alpha^{(t+1)}} ||v^{(t+1)} - x^{<em>}||_2^2
  &amp; \le \frac{ (1-\theta^{(0)}) }{ (\theta^{(0)})^2 } \left( f(x^{(0)}) - f(x^{</em>}) \right) + \frac{1}{2 \alpha^{(1)}} ||v^{(0)} - x^{<em>}||_2^2 \
\end{align</em>}
$$</p>
<p><strong>Step 3</strong> Bound the error in terms of $\frac{1}{(t+2)^2}$.  Recall that $\theta^{(0)} = 1$ and $v^{(0)} = x^{(0)}$. Then,</p>
<p>$$
\begin{align<em>}
  \frac{1}{( \theta^{(t)} )^2} \left( f(x^{(t+1)}) - f(x^{</em>}) \right) 
    + \underbrace{ \frac{1}{2 \alpha^{(t+1)}} ||v^{(t+1)} - x^{<em>}||<em 0 _ge="\ge" _text_="\text{," drop so>2^2 }</em>}
  &amp; \le \underbrace{ \frac{ (1-\theta^{(0)}) }{ (\theta^{(0)})^2 } }_{= 0} \left( f(x^{(0)}) - f(x^{</em>}) \right)
    + \frac{1}{2 \alpha^{(1)}} ||\underbrace{ v^{(0)} }_{x^{(0)}} - x^{<em>}||_2^2 \
  \frac{1}{( \theta^{(t)} )^2} \left( f(x^{(t+1)}) - f(x^{</em>}) \right)
  &amp; \le \frac{1}{2 \alpha^{(1)}} || x^{(0)} - x^{<em>} ||_2^2 \
  f(x^{(t+1)}) - f(x^{</em>})
  &amp; \le \frac{( \theta^{(t)} )^2}{2 \alpha^{(1)}} || x^{(0)} - x^{<em>} ||_2^2 \
  &amp; = \frac{2}{ (t+2)^2 \alpha^{(1)} } || x^{(0)} - x^{</em>} ||_2^2
\end{align*}
$$</p>
<p>Thus, the convergence rate of the Accelerated Gradient Method is
$O(\frac{1}{\sqrt{\epsilon}})$. Woo!</p>
<p><a id="usage"></a></p>
<h1><a name="usage" href="#usage">When should I use it?</a></h1>
<p>The Accelerated Gradient Method trumps Gradient Descent in every way theoretically, but the latter is still more widely used and preferred.  Why? The fact is that Accelerated Gradient is much more of a pain to implement. Whereas with Gradient Descent you can simply check if your new iterate's score is less than the previous one, Accelerated Gradient's score may increase before decreasing again. Accelerated Gradient is also extremely sensitive to step size -- if $\alpha^{(t)}$ isn't in $(0, \frac{1}{L}]$, <em>it will diverge</em>. Accelerated Gradient is a powerful but fickle tool. Use it when you can, but keep Gradient Descent handy if everything is going awry.</p>
<h1><a name="extensions" href="#extensions">Extensions</a></h1>
<p><strong>Step Size</strong> As mentioned previously, Accelerated Gradient is very fickle
with step size.  While Backtracking Line Search can and should be used when
possible, it is essential that the constants be set such that $\alpha^{(t+1)}
\le \frac{1}{L}$. In other words, when,</p>
<p>$$
\begin{align<em>}
  f(y^{(t)} - \alpha^{(t+1)} \nabla f(y^{(t)}))
  &amp; \le f(y^{(t)}) + \nabla f(y^{(t)})^T ((y^{(t)} - \alpha^{(t+1)} \nabla f(y^{(t)})) - y^{(t)}) \
  &amp; \quad + \frac{1}{2 \alpha^{(t+1)} } ||(y^{(t)} - \alpha^{(t+1)} \nabla f(y^{(t)})) - y^{(t)}||_2^2 \
  &amp; = f(y^{(t)}) - \alpha^{(t+1)} || \nabla f(y^{(t)}) ||_2^2 + \frac{ \alpha^{(t+1)} }{2} ||\nabla f(y^{(t)}) ||_2^2 \
  &amp; = f(y^{(t)}) - \frac{\alpha^{(t+1)}}{2} || \nabla f(y^{(t)}) ||_2^2 \
\end{align</em>}
$$</p>
<p>Typically the $\frac{1}{2}$ in the last part of the last line is traded for another constant. <em>This will not work for Accelerated Gradient!</em></p>
<p><strong>Checking Convergence</strong> As with Gradient Descent and Subgradient Descent,
there is no real way to be certain when $f(x^{(t)}) - f(x^{*}) &lt; \epsilon$
without some problem-specific knowledge. Instead, it is common to stop after a
fixed number of iterations or when $||\nabla f(x^{(t)})||_2$ is small.</p>
<h1><a name="references" href="#references">References</a></h1>
<p><strong>Proof of Convergence</strong> The proof of convergence is thanks to Lieven
Vandenberghe's <a href="http://math.sjtu.edu.cn/faculty/zw2109/course/sp04-2-gradient.pdf">EE236c slides</a> hosted by Zaiwen Wen.</p>
<h1><a name="reference-impl" href="#reference-impl">Reference Implementation</a></h1>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">accelerated_gradient</span><span class="p">(</span><span class="n">gradient</span><span class="p">,</span> <span class="n">y0</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">n_iterations</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Accelerated Gradient Method</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  gradient : function</span>
<span class="sd">      Computes the gradient of the objective function at x</span>
<span class="sd">  y0 : array</span>
<span class="sd">      initial value for x</span>
<span class="sd">  alpha : function</span>
<span class="sd">      function computing step sizes</span>
<span class="sd">  n_iterations : int, optional</span>
<span class="sd">      number of iterations to perform</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  xs : list</span>
<span class="sd">      intermediate values for x</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">ys</span> <span class="o">=</span> <span class="p">[</span><span class="n">y0</span><span class="p">]</span>
  <span class="n">xs</span> <span class="o">=</span> <span class="p">[</span><span class="n">y0</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_iterations</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">ys</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">xs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">x_plus</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="n">g</span>
    <span class="n">y_plus</span> <span class="o">=</span> <span class="n">x_plus</span> <span class="o">+</span> <span class="p">((</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">t</span><span class="o">+</span><span class="mi">2</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="n">x_plus</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_plus</span><span class="p">)</span>
    <span class="n">xs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_plus</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">xs</span>

<span class="k">class</span> <span class="nc">BacktrackingLineSearch</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">function</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">function</span> <span class="o">=</span> <span class="n">function</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span>    <span class="o">=</span> <span class="mf">0.05</span>

  <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">g</span><span class="p">):</span>
    <span class="n">f</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">function</span>
    <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span>
    <span class="k">while</span> <span class="n">f</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">a</span> <span class="o">*</span> <span class="n">g</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">f</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">a</span> <span class="o">*</span> <span class="p">(</span><span class="n">g</span><span class="o">*</span><span class="n">g</span><span class="p">):</span>
      <span class="n">a</span> <span class="o">*=</span> <span class="mf">0.99</span>
    <span class="k">return</span> <span class="n">a</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
  <span class="kn">import</span> <span class="nn">os</span>

  <span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
  <span class="kn">import</span> <span class="nn">yannopt.plotting</span> <span class="kn">as</span> <span class="nn">plotting</span>

  <span class="c1">### ACCELERATED GRADIENT ###</span>

  <span class="c1"># problem definition</span>
  <span class="n">function</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">4</span>     <span class="c1"># the function to minimize</span>
  <span class="n">gradient</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span><span class="mi">3</span>  <span class="c1"># its gradient</span>
  <span class="n">alpha</span> <span class="o">=</span> <span class="n">BacktrackingLineSearch</span><span class="p">(</span><span class="n">function</span><span class="p">)</span>
  <span class="n">x0</span> <span class="o">=</span> <span class="mf">1.0</span>
  <span class="n">n_iterations</span> <span class="o">=</span> <span class="mi">10</span>

  <span class="c1"># run gradient descent</span>
  <span class="n">iterates</span> <span class="o">=</span> <span class="n">accelerated_gradient</span><span class="p">(</span><span class="n">gradient</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">n_iterations</span><span class="o">=</span><span class="n">n_iterations</span><span class="p">)</span>

  <span class="c1">### PLOTTING ###</span>

  <span class="n">plotting</span><span class="o">.</span><span class="n">plot_iterates_vs_function</span><span class="p">(</span><span class="n">iterates</span><span class="p">,</span> <span class="n">function</span><span class="p">,</span>
                                     <span class="n">path</span><span class="o">=</span><span class="s1">&#39;figures/iterates.png&#39;</span><span class="p">,</span> <span class="n">y_star</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
  <span class="n">plotting</span><span class="o">.</span><span class="n">plot_iteration_vs_function</span><span class="p">(</span><span class="n">iterates</span><span class="p">,</span> <span class="n">function</span><span class="p">,</span>
                                      <span class="n">path</span><span class="o">=</span><span class="s1">&#39;figures/convergence.png&#39;</span><span class="p">,</span> <span class="n">y_star</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
</pre></div>

    <div id="article_meta">
        Category:
          <a href="/category/optimization.html">optimization</a>
        <br />Tags:
          <a href="/tag/optimization.html">optimization</a>
,           <a href="/tag/first-order.html">first-order</a>
,           <a href="/tag/accelerated.html">accelerated</a>
    </div>
  </article>

  <footer>
    <a href="/" class="button_accent">&larr;&nbsp;&nbsp;&nbsp;Back to blog</a>
  </footer>

  <div id="comments">
    <h2>Comments</h2>
    <div id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_identifier = "blog/accelerated-gradient-descent.html";
      (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://duckworthd-blog.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>Please enable JavaScript to view <a href="http://disqus.com/?ref_noscript">comments</a>.</noscript>
  </div>

	</section>


  <!-- scripts -->
  <script
    type= "text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    >
  </script>
  <script
    type= "text/javascript"
    src="/theme/mathjax/MathJaxLocal.js"
    >
  </script>
  <script
      type="text/javascript"
      src="/theme/lightbox/js/jquery-1.11.0.min.js"
      >
  </script>
  <script
      type="text/javascript"
      src="/theme/lightbox/js/lightbox.min.js"
      >
  </script>
</body>
</html>