<!DOCTYPE html>
<html lang="en">
<head>
  <!-- enable responsive design -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- stylesheets -->
  <link rel="stylesheet" type="text/css" href="/theme/css/style.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/pygments.css">
  <link rel="stylesheet" type="text/css" href="/theme/font-awesome-4.3.0/css/font-awesome.min.css">

  <!-- fonts -->
  <link href='http://fonts.googleapis.com/css?family=Open+Sans:800,400,300|Inconsolata' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:700,900' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Lora:700' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Playball' rel='stylesheet' type='text/css'>

  <!-- No RSS/ATOM feeds -->


    <title>Strongly Convex</title>
    <meta charset="utf-8" />
</head>
<body>
  <div class="row">
    <div id="sidebar">
      <figure id="user_logo">
        <a href=""><div class="logo">&nbsp;</div></a>
      </figure>

      <div class="user_meta">
        <h1 id="user">
          <a  href="/"
              class="sitename">
            Strongly Convex
          </a>
        </h1>
        <h2></h2>

        <ul>
          <hr></hr>
            <li><a href="/">Blog</a></li>
            <hr></hr>
            <li><a href="/about.html">About</a></li>
            <hr></hr>
        </ul>
      </div>
    </div>

    <div id="posts">
	<header>
      <h1 class="title">
			  <a  href="/blog/big-table-of-convergence-rates.html"
            rel="bookmark"
			  	  title="Permalink to The Big Table of Convergence Rates">
          The Big Table of Convergence Rates
        </a>
      </h1>
		  <abbr class="published">Fri 01 August 2014</abbr>
	</header>
  <article>
    <p>In the past 50+ years of convex optimization research, a great many
algorithms have been developed, each with slight nuances to their assumptions,
implementations, and guarantees. In this article, I'll give a shorthand
comparison of these methods in terms of the number of iterations required
to reach a desired accuracy <span class="math">\(\epsilon\)</span> for convex and strongly convex objective
functions.</p>
<p>Below, methods are grouped according to what "order" of information they
require about the objective function. In general, the more information you
have, the faster you can converge; but beware, you will also need more memory
and computation. Zeroth and first order methods are typically appropriate for
large scale problems, whereas second order methods are limited to
small-to-medium scale problems that require a high degree of precision.</p>
<p>At the bottom, you will find algorithms aimed specifically at minimizing
supervised learning problems and other meta-algorithms useful for distributing
computation across multiple nodes.</p>
<p>Unless otherwise stated, all objectives are assumed to be Lipschitz
continuous (though not necssarily differentiable) and the domain convex. The
variable being optimized is <span class="math">\(x \in \mathbb{R}^n\)</span>.</p>
<h1>Zeroth Order Methods</h1>
<p>Zeroth order methods are characterized by not requiring any gradients or
subgradients for their objective functions. In exchange, however, it is
assumed that the objective is "simple" in the sense that a subset of variables
(a "block") can be minimized exactly while holding all other variables fixed.</p>
<div style="overflow-x: auto">
  <table markdown class="table table-bordered table-centered">
    <colgroup>
      <col style="width:20%">
      <col style="width:10%">
      <col style="width:10%">
      <col style="width:10%">
      <col style="width:10%">
      <col style="width:40%">
    </colgroup>
    <thead>
      <tr>
        <th>Algorithm          </th>
        <th>Problem Formulation</th>
        <th>Convex             </th>
        <th>Strongly Convex    </th>
        <th>Per-Iteration Cost </th>
        <th>Notes              </th>
      </tr>
    </thead>

    <tbody>
      <tr>
        <!-- Algorithm          -->
        <td>Randomized Block Coordinate Descent</td>
        <!-- Problem            -->
        <td>$\displaystyle \min_{x \in \mathbb{R}^{n}} f(x) + g(x)$</td>
        <!-- Convex             -->
        <td>$O(1 / \epsilon)$[^richtarik-2011]</td>
        <!-- Strongly Convex    -->
        <td>$O(\log (1 / \epsilon))$[^richtarik-2011]</td>
        <!-- Per-Iteration Cost -->
        <td>$O(1)$</td>
        <!-- Notes              -->
        <td>
          Applicable when $f(x)$ is differentiable and $g(x)$ is separable in
          each block. $g(x)$ may be a barrier function.
        </td>
      </tr>
    </tbody>
  </table>
</div>

<h1>First Order Methods</h1>
<p>First order methods typically require access to an objective function's
gradient or subgradient. The algorithms typically take the form <span class="math">\(x^{(t+1)}
= x^{(t)} - \alpha^{(t)} g^{(t)}\)</span> for some step size <span class="math">\(\alpha^{(t)}\)</span> and descent
direction <span class="math">\(g^{(t)}\)</span>. As such, each iteration takes approximately <span class="math">\(O(n)\)</span> time.</p>
<div style="overflow-x: auto">
  <table markdown class="table table-bordered table-centered">
    <colgroup>
      <col style="width:20%">
      <col style="width:10%">
      <col style="width:10%">
      <col style="width:10%">
      <col style="width:10%">
      <col style="width:40%">
    </colgroup>

    <thead>
      <tr>
        <th>Algorithm          </th>
        <th>Problem Formulation</th>
        <th>Convex             </th>
        <th>Strongly Convex    </th>
        <th>Per-Iteration Cost </th>
        <th>Notes              </th>
      </tr>
    </thead>

    <tbody>
      <tr>
        <!-- Algorithm          -->
        <td>Subgradient Descent</td>
        <!-- Problem            -->
        <td>$\displaystyle  \min_{x \in \mathbb{R}^n} f(x)$</td>
        <!-- Convex             -->
        <td>$O(1 / \epsilon^{2})$[^blog-sd]</td>
        <!-- Strongly Convex    -->
        <td>...</td>
        <!-- Per-Iteration Cost -->
        <td>$O(n)$</td>
        <!-- Notes              -->
        <td>
          Cannot be improved upon without further assumptions.
        </td>
      </tr>
      <tr>
        <!-- Algorithm          -->
        <td>Mirror Descent</td>
        <!-- Problem            -->
        <td>$\displaystyle \min_{x \in \mathcal{C}} f(x)$</td>
        <!-- Convex             -->
        <td>$O(1 / \epsilon^{2} )$[^ee381-md]</td>
        <!-- Strongly Convex    -->
        <td>$O(1 / \epsilon )$[^nedich-2013]</td>
        <!-- Per-Iteration Cost -->
        <td>$O(n)$</td>
        <!-- Notes              -->
        <td>
          Different parameterizations result in gradient descent and
          exponentiated gradient descent.
        </td>
      </tr>
      <tr>
        <!-- Algorithm          -->
        <td>Dual Averaging</td>
        <!-- Problem            -->
        <td>$\displaystyle  \min_{x \in \mathcal{C}} f(x)$</td>
        <!-- Convex             -->
        <td>$O(1 / \epsilon^{2})$[^nesterov-2007]</td>
        <!-- Strongly Convex    -->
        <td>...</td>
        <!-- Per-Iteration Cost -->
        <td>$O(n)$</td>
        <!-- Notes              -->
        <td>
          Cannot be improved upon without further assumptions.
        </td>
      </tr>
      <tr>
        <!-- Algorithm          -->
        <td>Gradient Descent</td>
        <!-- Problem            -->
        <td>$\displaystyle \min_{x \in \mathbb{R}^n} f(x)$</td>
        <!-- Convex             -->
        <td>$O(1 / \epsilon)$[^blog-gd]</td>
        <!-- Strongly Convex    -->
        <td>$O(\log (1 / \epsilon))$[^ee381-gd]</td>
        <!-- Per-Iteration Cost -->
        <td>$O(n)$</td>
        <!-- Notes              -->
        <td>
          Applicable when $f(x)$ is differentiable.
        </td>
      </tr>
      <tr>
        <!-- Algorithm          -->
        <td>Accelerated Gradient Descent</td>
        <!-- Problem            -->
        <td>$\displaystyle \min_{x \in \mathbb{R}^n} f(x)$</td>
        <!-- Convex             -->
        <td>$O(1 / \sqrt{\epsilon})$[^blog-agd]</td>
        <!-- Strongly Convex    -->
        <td>$O(\log (1 / \epsilon))$[^bubeck-agd]</td>
        <!-- Per-Iteration Cost -->
        <td>$O(n)$</td>
        <!-- Notes              -->
        <td>
          Applicable when $f(x)$ is differentiable.
          Cannot be improved upon without further assumptions.
          Has better constants than Gradient Descent for "Strongly Convex" case.
        </td>
      </tr>
      <tr>
        <!-- Algorithm          -->
        <td>Proximal Gradient Descent</td>
        <!-- Problem            -->
        <td>$\displaystyle \min_{x \in \mathcal{C}} f(x) + g(x)$</td>
        <!-- Convex             -->
        <td>$O(1 / \epsilon)$[^blog-pgd]</td>
        <!-- Strongly Convex    -->
        <td>$O(\log (1 / \epsilon))$[^mairal-2013]</td>
        <!-- Per-Iteration Cost -->
        <td>$O(n)$</td>
        <!-- Notes              -->
        <td>
          Applicable when $f(x)$ is differentiable and
          $\text{prox}_{\tau_t g}(x)$ is easily computable.
        </td>
      </tr>
      <tr>
        <!-- Algorithm          -->
        <td>Proximal Accelerated Gradient Descent</td>
        <!-- Problem            -->
        <td>$\displaystyle \min_{x \in \mathcal{C}} f(x) + g(x)$</td>
        <!-- Convex             -->
        <td>$O(1 / \sqrt{\epsilon})$[^blog-apgd]</td>
        <!-- Strongly Convex    -->
        <td>$O(\log (1 / \epsilon))$[^mairal-2013]</td>
        <!-- Per-Iteration Cost -->
        <td>$O(n)$</td>
        <!-- Notes              -->
        <td>
          Applicable when $f(x)$ is differentiable and
          $\text{prox}_{\tau_t g}(x)$ is easily computable.
          Has better constants than Proximal Gradient Descent for "Strongly
          Convex" case.
        </td>
      </tr>
      <tr>
        <!-- Algorithm          -->
        <td>Frank-Wolfe Algorithm / Conditional Gradient Algorithm</td>
        <!-- Problem            -->
        <td>$\displaystyle \min_{x \in \mathcal{C}} f(x)$</td>
        <!-- Convex             -->
        <td>$O(1/\epsilon)$[^blog-fw]</td>
        <!-- Strongly Convex    -->
        <td>$O(1/\sqrt{\epsilon})$[^garber-2014]</td>
        <!-- Per-Iteration Cost -->
        <td>$O(n)$</td>
        <!-- Notes              -->
        <td>
          Applicable when $\mathcal{C}$ is bounded and $h_{g}(x) = \arg\min_{x \in
          \mathcal{C}} \langle g, x \rangle$ is easily computable. Most useful
          when $\mathcal{C}$ is a polytope in a very high dimensional space with
          sparse extrema.
        </td>
      </tr>
    </tbody>
  </table>
</div>

<h1>Second Order Methods</h1>
<p>Second order methods either use or approximate the hessian (<span class="math">\(\nabla^2 f(x)\)</span>)
of the objective function to result in better-than-linear rates of convergence.
As such, each iteration typically requires <span class="math">\(O(n^2)\)</span> memory and between <span class="math">\(O(n^2)\)</span>
and <span class="math">\(O(n^3)\)</span> computation per iteration.</p>
<div style="overflow-x: auto">
  <table markdown class="table table-bordered table-centered">
    <colgroup>
      <col style="width:20%">
      <col style="width:10%">
      <col style="width:10%">
      <col style="width:10%">
      <col style="width:10%">
      <col style="width:40%">
    </colgroup>

    <thead>
      <tr>
        <th>Algorithm          </th>
        <th>Problem Formulation</th>
        <th>Convex             </th>
        <th>Strongly Convex    </th>
        <th>Per-Iteration Cost </th>
        <th>Notes              </th>
      </tr>
    </thead>

    <tbody>
      <tr>
        <!-- Algorithm          -->
        <td>Newton's Method</td>
        <!-- Problem            -->
        <td>$\displaystyle \min_{x \in \mathbb{R}^n} f(x)$</td>
        <!-- Convex             -->
        <td>...</td>
        <!-- Strongly Convex    -->
        <td>$O(\log \log (1/\epsilon))$[^ee364a-unconstrained]</td>
        <!-- Per-Iteration Cost -->
        <td>$O(n^3)$</td>
        <!-- Notes              -->
        <td>
          Only applicable when $f(x)$ is twice differentiable. Constraints can be
          incorporated via interior point methods.
        </td>
      </tr>
      <tr>
        <!-- Algorithm          -->
        <td>Conjugate Gradient Descent</td>
        <!-- Problem            -->
        <td>$\displaystyle \min_{x \in \mathbb{R}^n} f(x)$</td>
        <!-- Convex             -->
        <td>...</td>
        <!-- Strongly Convex    -->
        <td>$O(n)$</td>
        <!-- Per-Iteration Cost -->
        <td>$O(n^2)$</td>
        <!-- Notes              -->
        <td>
          Converges in exactly $n$ steps for quadratic $f(x)$. May fail to
          converge for non-quadratic objectives.
        </td>
      </tr>
      <tr>
        <!-- Algorithm          -->
        <td>L-BFGS</td>
        <!-- Problem            -->
        <td>$\displaystyle \min_{x \in \mathbb{R}^n} f(x)$</td>
        <!-- Convex             -->
        <td>...</td>
        <!-- Strongly Convex    -->
        <td>Between $O(\log (1/\epsilon))$ and $O(\log \log (1/\epsilon))$[^ee236c-qnewton]</td>
        <!-- Per-Iteration Cost -->
        <td>$O(n^2)$</td>
        <!-- Notes              -->
        <td>
          Applicable when $f(x)$ is differentiable, but works best when twice
          differentiable. Convergence rate is not guaranteed.
        </td>
      </tr>
    </tbody>
  </table>
</div>

<h1>Stochastic Methods</h1>
<p>The following algorithms are specifically designed for supervised machine
learning where the objective can be decomposed into independent "loss"
functions and a regularizer,</p>
<div class="math">$$
\begin{align*}
  \min_{x} \frac{1}{N} \sum_{i=1}^{N} f_{i}(x) + \lambda g(x)
\end{align*}
$$</div>
<p>The intuition is that finding the optimal solution to this problem is
unnecessary as the goal is to minimize the "risk" (read: error) with respect to
a set of <em>samples</em> from the true distribution of potential loss functions.
Thus, the following algorithms' convergence rates are for the <em>expected</em> rate
of convergence (as opposed to the above algorithms which upper bound the <em>true</em>
rate of convergence).</p>
<div style="overflow-x: auto">
  <table markdown class="table table-bordered table-centered">
    <colgroup>
      <col style="width:20%">
      <col style="width:10%">
      <col style="width:10%">
      <col style="width:10%">
      <col style="width:10%">
      <col style="width:40%">
    </colgroup>

    <thead>
      <tr>
        <th>Algorithm          </th>
        <th>Problem Formulation</th>
        <th>Convex             </th>
        <th>Strongly Convex    </th>
        <th>Per-Iteration Cost </th>
        <th>Notes              </th>
      </tr>
    </thead>

    <tbody>
      <tr>
        <!-- Algorithm          -->
        <td>Stochastic Gradient Descent (SGD)</td>
        <!-- Problem            -->
        <td>$\displaystyle \min_{x \in \mathbb{R}^n} \sum_{i} f_{i}(x) + \lambda g(x)$</td>
        <!-- Convex             -->
        <td>$O(n/\epsilon^2)$[^bach-2012]</td>
        <!-- Strongly Convex    -->
        <td>$O(n/\epsilon)$[^bach-2012]</td>
        <!-- Per-Iteration Cost -->
        <td>$O(n)$</td>
        <!-- Notes              -->
        <td>
          Assumes objective is differentiable.
        </td>
      </tr>
      <tr>
        <!-- Algorithm          -->
        <td>Stochastic Dual Coordinate Ascent (SDCA)</td>
        <!-- Problem            -->
        <td>$\displaystyle \min_{x \in \mathbb{R}^n} \sum_{i} f_{i}(x) + \frac{\lambda}{2} \norm{x}_2^2$</td>
        <!-- Convex             -->
        <td>$O(\frac{1}{\lambda \epsilon})$[^shalevshwartz-2012]</td>
        <!-- Strongly Convex    -->
        <td>$O(( \frac{1}{\lambda} ) \log ( \frac{1}{\lambda \epsilon} ))$[^shalevshwartz-2012]</td>
        <!-- Per-Iteration Cost -->
        <td>$O(n)$</td>
        <!-- Notes              -->
        <td>
        </td>
      </tr>
      <tr>
        <!-- Algorithm          -->
        <td>Accelerated Proximal Stochastic Dual Coordinate Ascent (APSDCA)</td>
        <!-- Problem            -->
        <td>$\displaystyle \min_{x \in \mathbb{C}} \sum_{i} f_{i}(x) + \lambda g(x)$</td>
        <!-- Convex             -->
        <td>$O(\min (\frac{1}{\lambda \epsilon}, \sqrt{\frac{N}{\lambda \epsilon}} ))$[^shalevshwartz-2013]</td>
        <!-- Strongly Convex    -->
        <td>$O(\min (\frac{1}{\lambda}, \sqrt{\frac{N}{\lambda}}) \log ( \frac{1}{\epsilon} ))$[^shalevshwartz-2013]</td>
        <!-- Per-Iteration Cost -->
        <td>$O(n)$</td>
        <!-- Notes              -->
        <td>
        </td>
      </tr>
      <tr>
        <!-- Algorithm          -->
        <td>Stochastic Average Gradient (SAG)</td>
        <!-- Problem            -->
        <td>$\displaystyle \min_{x \in \mathbb{R}^n} \sum_{i} f_{i}(x) + \lambda g(x)$</td>
        <!-- Convex             -->
        <td>$O(1 / \epsilon)$[^schmidt-2013]</td>
        <!-- Strongly Convex    -->
        <td>$O(\log (1/\epsilon))$[^schmidt-2013]</td>
        <!-- Per-Iteration Cost -->
        <td>$O(n)$</td>
        <!-- Notes              -->
        <td>
          Applicable when $f_{i}(x)$ is differentiable.
        </td>
      </tr>
      <tr>
        <!-- Algorithm          -->
        <td>Stochastic Variance Reduced Gradient (SVRG)</td>
        <!-- Problem            -->
        <td>$\displaystyle \min_{x \in \mathbb{R}^n} \sum_{i} f_{i}(x) + \lambda g(x)$</td>
        <!-- Convex             -->
        <td>$O(1 / \epsilon)$[^johnson-2013]</td>
        <!-- Strongly Convex    -->
        <td>$O(\log (1/\epsilon))$[^johnson-2013]</td>
        <!-- Per-Iteration Cost -->
        <td>$O(n)$</td>
        <!-- Notes              -->
        <td>
          Applicable when $f_{i}(x)$ is differentiable.
        </td>
      </tr>
      <tr>
        <!-- Algorithm          -->
        <td>MISO</td>
        <!-- Problem            -->
        <td>$\displaystyle \min_{x \in \mathbb{R}^n} \sum_{i} f_{i}(x) + \lambda g(x)$</td>
        <!-- Convex             -->
        <td>$O(1 / \epsilon)$[^mairal-2013]</td>
        <!-- Strongly Convex    -->
        <td>$O(\log (1/\epsilon))$[^mairal-2013]</td>
        <!-- Per-Iteration Cost -->
        <td>$O(n)$</td>
        <!-- Notes              -->
        <td>
          Applicable when $f_{i}(x)$ is differentiable. $g(x)$ may be used as
          a barrier function.
        </td>
      </tr>
    </tbody>
  </table>
</div>

<h1>Other Methods</h1>
<p>The following methods do not fit well into any of the preceding categories.
Included are meta-algorithms like ADMM, which are good for distributing
computation across machines, and methods whose per-iteration complexity depends
on iteration count <span class="math">\(t\)</span>.</p>
<div style="overflow-x: auto">
  <table markdown class="table table-bordered table-centered">
    <colgroup>
      <col style="width:20%">
      <col style="width:10%">
      <col style="width:10%">
      <col style="width:10%">
      <col style="width:10%">
      <col style="width:40%">
    </colgroup>

    <thead>
      <tr>
        <th>Algorithm          </th>
        <th>Problem Formulation</th>
        <th>Convex             </th>
        <th>Strongly Convex    </th>
        <th>Per-Iteration Cost </th>
        <th>Notes              </th>
      </tr>
    </thead>

    <tbody>
      <tr>
        <!-- Algorithm          -->
        <td>Alternating Direction Method of Multipliers (ADMM)</td>
        <!-- Problem            -->
        <td>
          $$
            \begin{align*}
              \min_{x,z} \quad
                & f(x) + g(z) \\
              \text{s.t.} \quad
                & Ax + Bz = c
            \end{align*}
          $$
        </td>
        <!-- Convex             -->
        <td>$O(1/\epsilon)$[^blog-admm]</td>
        <!-- Strongly Convex    -->
        <td>$O(\log (1/\epsilon))$[^hong-2012]</td>
        <!-- Per-Iteration Cost -->
        <td>$O(n)$</td>
        <!-- Notes              -->
        <td>
          The stated convergence rate for "Strongly Convex" only requires $f(x)$ to
          be strongly convex, not $g(x)$. This same rate can also be applied to
          the "Convex" case under several non-standard assumptions[^hong-2012].
          Matrices $A$ and $B$ may also need to be full column rank[^deng-2012] .
        </td>
      </tr>
      <tr>
        <!-- Algorithm          -->
        <td>Bundle Method</td>
        <!-- Problem            -->
        <td>$\displaystyle \min_{x \in \mathcal{C}} f(x)$</td>
        <!-- Convex             -->
        <td>$O(1/\epsilon)$[^smola-2007]</td>
        <!-- Strongly Convex    -->
        <td>$O(\log (1 / \epsilon))$[^smola-2007]</td>
        <!-- Per-Iteration Cost -->
        <td>$O(tn)$</td>
        <!-- Notes              -->
        <td>
        </td>
      </tr>
      <tr>
        <!-- Algorithm          -->
        <td>Center of Gravity Algorithm</td>
        <!-- Problem            -->
        <td>$\displaystyle \min_{x \in \mathcal{C}} f(x)$</td>
        <!-- Convex             -->
        <td>$O(\log (1 / \epsilon))$[^ee236c-localization]</td>
        <!-- Strongly Convex    -->
        <td>$O(\log (1 / \epsilon))$[^ee236c-localization]</td>
        <!-- Per-Iteration Cost -->
        <td>At least $O(tn)$</td>
        <!-- Notes              -->
        <td>
          Applicable when $\mathcal{C}$ is bounded. Each iteration requires
          finding a near-central point in a convex set; this may be
          computationally expensive.
        </td>
      </tr>

    </tbody>
  </table>
</div>

<!-- Footnotes -->
<!-- References -->
<div class="footnote">
<hr>
<ol>
<li id="fn:blog-gd">
<p><a href="http://stronglyconvex.com/blog/gradient-descent.html">Gradient Descent</a>&#160;<a class="footnote-backref" href="#fnref:blog-gd" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:blog-sd">
<p><a href="http://stronglyconvex.com/blog/subgradient-descent.html">Subgradient Descent</a>&#160;<a class="footnote-backref" href="#fnref:blog-sd" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:blog-agd">
<p><a href="http://stronglyconvex.com/blog/accelerated-gradient-descent.html">Accelerated Gradient Descent</a>&#160;<a class="footnote-backref" href="#fnref:blog-agd" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:blog-pgd">
<p><a href="http://stronglyconvex.com/blog/proximal-gradient-descent.html">Proximal Gradient Descent</a>&#160;<a class="footnote-backref" href="#fnref:blog-pgd" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
<li id="fn:blog-apgd">
<p><a href="http://stronglyconvex.com/blog/accelerated-proximal-gradient-descent.html">Accelerated Proximal Gradient Descent</a>&#160;<a class="footnote-backref" href="#fnref:blog-apgd" title="Jump back to footnote 5 in the text">&#8617;</a></p>
</li>
<li id="fn:blog-fw">
<p><a href="http://stronglyconvex.com/blog/frank-wolfe.html">Franke-Wolfe Algorithm</a>&#160;<a class="footnote-backref" href="#fnref:blog-fw" title="Jump back to footnote 6 in the text">&#8617;</a></p>
</li>
<li id="fn:blog-admm">
<p><a href="http://stronglyconvex.com/blog/admm-revisited.html">Alternating Direction Method of Multipliers</a>&#160;<a class="footnote-backref" href="#fnref:blog-admm" title="Jump back to footnote 7 in the text">&#8617;</a></p>
</li>
<li id="fn:richtarik-2011">
<p><a href="http://arxiv.org/abs/1107.2848">Richtarik and Takac, 2011</a>&#160;<a class="footnote-backref" href="#fnref:richtarik-2011" title="Jump back to footnote 8 in the text">&#8617;</a></p>
</li>
<li id="fn:ee381-md">
<p><a href="http://users.ece.utexas.edu/~cmcaram/EE381V_2012F/Lecture_24_Scribe_Notes.final.pdf">EE381 Slides on Mirror Descent</a>&#160;<a class="footnote-backref" href="#fnref:ee381-md" title="Jump back to footnote 9 in the text">&#8617;</a></p>
</li>
<li id="fn:ee381-gd">
<p><a href="http://users.ece.utexas.edu/~cmcaram/EE381V_2012F/Lecture_4_Scribe_Notes.final.pdf">EE381 Slides on Gradient Descent</a>&#160;<a class="footnote-backref" href="#fnref:ee381-gd" title="Jump back to footnote 10 in the text">&#8617;</a></p>
</li>
<li id="fn:bubeck-agd">
<p><a href="https://blogs.princeton.edu/imabandit/2014/03/06/nesterovs-accelerated-gradient-descent-for-smooth-and-strongly-convex-optimization/">Sebastien Bubeck's article on Accelerated Gradient Descent for Smooth and Strongly Convex objectives</a>&#160;<a class="footnote-backref" href="#fnref:bubeck-agd" title="Jump back to footnote 11 in the text">&#8617;</a></p>
</li>
<li id="fn:garber-2014">
<p><a href="http://arxiv.org/abs/1406.1305">Garber and Hazan, 2014</a>&#160;<a class="footnote-backref" href="#fnref:garber-2014" title="Jump back to footnote 12 in the text">&#8617;</a></p>
</li>
<li id="fn:mairal-2013">
<p><a href="http://arxiv.org/abs/1305.3120">Mairal, 2013</a>&#160;<a class="footnote-backref" href="#fnref:mairal-2013" title="Jump back to footnote 13 in the text">&#8617;</a></p>
</li>
<li id="fn:ee364a-unconstrained">
<p><a href="http://web.stanford.edu/class/ee364a/lectures/unconstrained.pdf">EE364a Slides on Unconstrained Optimization Algorithms</a>&#160;<a class="footnote-backref" href="#fnref:ee364a-unconstrained" title="Jump back to footnote 14 in the text">&#8617;</a></p>
</li>
<li id="fn:ee236c-qnewton">
<p><a href="http://www.seas.ucla.edu/~vandenbe/236C/lectures/qnewton.pdf">EE236c Slides on Quasi-Newton Methods</a>&#160;<a class="footnote-backref" href="#fnref:ee236c-qnewton" title="Jump back to footnote 15 in the text">&#8617;</a></p>
</li>
<li id="fn:bach-2012">
<p><a href="http://www.ann.jussieu.fr/~plc/bach2012.pdf">Bach's slides on Stochastic Methods</a>&#160;<a class="footnote-backref" href="#fnref:bach-2012" title="Jump back to footnote 16 in the text">&#8617;</a></p>
</li>
<li id="fn:shalevshwartz-2012">
<p><a href="http://arxiv.org/abs/1209.1873">Shalev-Shwartz and Zhang, 2012</a>&#160;<a class="footnote-backref" href="#fnref:shalevshwartz-2012" title="Jump back to footnote 17 in the text">&#8617;</a></p>
</li>
<li id="fn:shalevshwartz-2013">
<p><a href="http://arxiv.org/abs/1309.2375">Shalev-Shwartz and Zhang, 2013</a>&#160;<a class="footnote-backref" href="#fnref:shalevshwartz-2013" title="Jump back to footnote 18 in the text">&#8617;</a></p>
</li>
<li id="fn:schmidt-2013">
<p><a href="http://arxiv.org/abs/1309.2388">Schmidt et al, 2013</a>&#160;<a class="footnote-backref" href="#fnref:schmidt-2013" title="Jump back to footnote 19 in the text">&#8617;</a></p>
</li>
<li id="fn:deng-2012">
<p><a href="ftp://ftp.math.ucla.edu/pub/camreport/cam12-52.pdf">Deng and Yin, 2012</a>, Table 1.1&#160;<a class="footnote-backref" href="#fnref:deng-2012" title="Jump back to footnote 20 in the text">&#8617;</a></p>
</li>
<li id="fn:hong-2012">
<p><a href="http://arxiv.org/abs/1208.3922">Hong and Luo, 2012</a>, Section 2&#160;<a class="footnote-backref" href="#fnref:hong-2012" title="Jump back to footnote 21 in the text">&#8617;</a></p>
</li>
<li id="fn:johnson-2013">
<p><a href="http://papers.nips.cc/paper/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction.pdf">Johnson and Zhang, 2013</a>&#160;<a class="footnote-backref" href="#fnref:johnson-2013" title="Jump back to footnote 22 in the text">&#8617;</a></p>
</li>
<li id="fn:smola-2007">
<p><a href="http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2007_470.pdf">Smola and Zhang, 2007</a>&#160;<a class="footnote-backref" href="#fnref:smola-2007" title="Jump back to footnote 23 in the text">&#8617;</a></p>
</li>
<li id="fn:ee236c-localization">
<p><a href="http://www.seas.ucla.edu/~vandenbe/236C/lectures/localization.pdf">EE236c Slides on Cutting Planes</a>&#160;<a class="footnote-backref" href="#fnref:ee236c-localization" title="Jump back to footnote 24 in the text">&#8617;</a></p>
</li>
<li id="fn:nesterov-2007">
<p><a href="http://ium.mccme.ru/postscript/s12/GS-Nesterov%20Primal-dual.pdf">Nesterov, 2007</a>&#160;<a class="footnote-backref" href="#fnref:nesterov-2007" title="Jump back to footnote 25 in the text">&#8617;</a></p>
</li>
<li id="fn:nedich-2013">
<p><a href="http://arxiv.org/abs/1307.1879">Nedich and Lee, 2013</a>&#160;<a class="footnote-backref" href="#fnref:nedich-2013" title="Jump back to footnote 26 in the text">&#8617;</a></p>
</li>
</ol>
</div>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>

    <div id="article_meta">
        Category:
          <a href="/category/optimization.html">optimization</a>
        <br />Tags:
          <a href="/tag/optimization.html">optimization</a>
,           <a href="/tag/convergence.html">convergence</a>
,           <a href="/tag/reference.html">reference</a>
    </div>
  </article>

  <footer>
    <a href="/" class="button_accent">&larr;&nbsp;&nbsp;&nbsp;Back to blog</a>
  </footer>


    </div>
  </div>


  <!-- scripts -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script
    type= "text/javascript"
    src="/theme/mathjax/MathJaxLocal.js"
    >
  </script>
</body>
</html>