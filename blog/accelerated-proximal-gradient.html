<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Accelerated Proximal Gradient Descent</title>

    <link href="/assets/css/bootstrap.min.css" rel="stylesheet" type="text/css">
    <link href="/assets/css/syntax.css" rel="stylesheet" type="text/css">
    <link href="/assets/css/blah.css" rel="stylesheet" type="text/css">
    <link href="/assets/css/lightbox/lightbox.css" rel="stylesheet" type="text/css">
  </head>
  <body>
    <div class="container">
    <div class="navbar">
  <div class="navbar-inner">
    <div class="nav-div">
      <a class="brand logo" href="#">
        <img src="/assets/img/transmogrifier2.png"></img>
      </a>
      <ul class="nav pull-right">
        <li><a href="/index.html">Home</a></li>
        <li><a href="/projects.html">Projects</a></li>
        <li><a href="/blog.html">Blog</a></li>
        <li><a href="/feed.xml">
          <img src="/assets/img/glyphicons/glyphicons_417_rss.png"></img>
        </a></li>
      </ul>
    </div> <!-- span10 offset1 -->
  </div> <!-- /navbar-inner -->
</div> <!-- /navbar -->


<div class="row-fluid">
  <div class="post box-shadow">
    <header>
      <h1 class="header-title">Accelerated Proximal Gradient Descent</h1>
      <p class="header-subtext">by Daniel Duckworth on Apr 25, 2013</p>
    </header>
    <article>
      <p><span class="math">\[
  \def\prox{\text{prox}}
\]</span></p>
<p>In the <a href="/blog/proximal-gradient.html">previous post</a>, I presented Proximal Gradient, a method for bypassing the <span class="math">\(O(1 / \epsilon^2)\)</span> convergence rate of Subgradient Descent. This method relied on assuming that the objective function could be expressed as the sum of 2 functions, <span class="math">\(g(x)\)</span> and <span class="math">\(h(x)\)</span>, with <span class="math">\(g\)</span> being differentiable and <span class="math">\(h\)</span> having an easy to compute <a href="/blog/proximal-gradient.html#intuition"><span class="math">\(\prox\)</span> function</a>,</p>
<p><span class="math">\[
  \prox_{\alpha h}(x) = \arg\min_{y} \alpha h(y) + \frac{1}{2}||y - x||_2^2
\]</span></p>
<p>In the <a href="/blog/accelerated-gradient-descent.html">post before that</a>, I presented Accelerated Gradient Descent, a method that outperforms Gradient Descent while making the exact same assumptions. It is then natural to ask, “Can we combine Accelerated Gradient Descent and Proximal Gradient to obtain a new algorithm?” Well if we couldn’t, why the hell would I be writing about something called “Accelerated Proximal Gradient.” C’mon people, work with me. Now let’s get on with it!</p>
<h1 id="how-does-it-work">How does it work?</h1>
<p>As you might guess, the setup is precisely the same as Proximal Gradient. Let our objective be expressed as the sum of 2 functions,</p>
<p><span class="math">\[
  \min_{x} g(x) + h(x)
\]</span></p>
<p>where <span class="math">\(g\)</span> is differentiable and <span class="math">\(h\)</span> is “simple” in the sense that its <span class="math">\(\prox\)</span> function can cheaply be computed. Given that, the algorithm is pretty much what you would expect from the lovechild of Proximal Gradient and Accelerated Gradient Descent,</p>
<div class="pseudocode">
  
<p><strong>Input</strong>: initial iterate <span class="math">\(x^{(0)}\)</span></p>
<ol style="list-style-type: decimal">
<li>Let <span class="math">\(y^{(0)} = x^{(0)}\)</span></li>
<li>For <span class="math">\(t = 1, 2, \ldots\)</span>
<ol start="3" style="list-style-type: decimal">
<li>Let <span class="math">\(x^{(t)} = \prox_{\alpha^{(t)} h} (y^{(t-1)} - \alpha^{(t)} \nabla f(y^{(t-1)}) )\)</span></li>
<li>if converged, return <span class="math">\(x^{(t)}\)</span></li>
<li>Let <span class="math">\(y^{(t)} = x^{(t)} + \frac{t-1}{t+2} (x^{(t)} - x^{(t-1)})\)</span>
</div>

</li>
</ol></li>
</ol>
<h1 id="a-small-example">A Small Example</h1>
<p>To illustrate Accelerated Proximal Gradient, I’ll use the same objective function as I did in illustrating Proximal Gradient Descent. Namely,</p>
<p><span class="math">\[
  \min_{x} \, \log(1 + \exp(-2x)) + ||x||_1
\]</span></p>
<p>which has the following gradient for <span class="math">\(g(x) = \log(1+\exp(-2x))\)</span> and <span class="math">\(\prox\)</span> operator for <span class="math">\(h(x) = ||x||_1\)</span>,</p>
<p><span class="math">\[
\begin{align*}
  \nabla g(x) &amp;= \frac{1}{1 + \exp(-2x)} \left( \exp(-2x) \right) (-2) \\
  \prox_{\alpha h}(x) &amp; = \text{sign}(x) \max(0, \text{abs}(x) - \alpha) \\
\end{align*}
\]</span></p>
<p>As before, we employ Backtracking Line Search to select the step size. In this example, regular Proximal Gradient seems to beat out Accelerated Proximal Gradient, but rest assured this is an artifact of the tiny problem size.</p>
<div class="img-center">
  
<img src="/assets/img/accelerated_proximal_gradient_descent/convergence.png"></img> <span class="caption"> This plot shows how quickly the objective function decreases as the number of iterations increases. Notice how the objective function doesn’t necessarily decrease at each step. </span>
</div>

<div class="img-center">
  
<img src="/assets/img/accelerated_proximal_gradient_descent/iterates.png"></img> <span class="caption"> This plot shows the actual iterates and the objective function evaluated at those points. More red indicates a higher iteration number. </span>
</div>

<h1 id="why-does-it-work">Why does it work?</h1>
<p>For the proof of Accelerated Proximal Gradient, we’ll make the same assumptions we did in Proximal Gradient. Namely,</p>
<ol style="list-style-type: decimal">
<li><span class="math">\(g(x)\)</span> is convex, differentiable, and finite for all <span class="math">\(x\)</span></li>
<li>a finite solution <span class="math">\(x^{*}\)</span> exists</li>
<li><span class="math">\(\nabla g(x)\)</span> is Lipschitz continuous with constant <span class="math">\(L\)</span>. That is, there must be an <span class="math">\(L\)</span> such that,</li>
</ol>
<p><span class="math">\[
  || \nabla g(x) - \nabla g(y) ||_2 \le L || x - y ||_2 \qquad \forall x,y
\]</span></p>
<ol start="4" style="list-style-type: decimal">
<li><span class="math">\(h(x)\)</span> is convex</li>
</ol>
<p><strong>Proof Outline</strong> In the same way that the proof for Proximal Gradient largely follows the proof for regular Gradient Descent, the proof for Accelerated Proximal Gradient follows the proof for Accelerated Gradient Descent. Our goal is to prove a statement of the form,</p>
<p><span class="math">\[
\begin{align*}
    (g+h)(x^{+})
    \le (1-\theta) (g+h)(x) + \theta (g+h)(x^{*}) + \frac{\theta^2}{2 \alpha^{+}} \left(
        ||v - x^{*}||_2^2 - ||v^{+} - x^{*}||_2^2
      \right) \\
\end{align*}
\]</span></p>
<p>Once we achieve this, the proof follows that of Accelerated Gradient with <span class="math">\(f \rightarrow g+h\)</span> from Step 2 onwards.</p>
<p>How will we do this? As with Accelerated Gradient, we define a new set of iterates <span class="math">\(v^{(t)}\)</span> in terms of <span class="math">\(x^{(t)}\)</span> and <span class="math">\(x^{(t-1)}\)</span> and then define <span class="math">\(y^{(t)}\)</span> in terms of <span class="math">\(v^{(t)}\)</span> and <span class="math">\(x^{(t)}\)</span>. We then exploit the Lipschitz bound on <span class="math">\(g\)</span> and a particular subgradient bound on <span class="math">\(h\)</span> to establish an upper bound on <span class="math">\((g+h)(x^{(t)})\)</span>. Finally, through algebraic manipulations we show the equation presented above, and we can simply copy-paste the Accelerated Gradient Descent proof to completion.</p>
<p><strong>Step 1</strong> Define a new set of iterates <span class="math">\(v^{(t)}\)</span>. As with Accelerated Gradient, we define a new set of iterates <span class="math">\(v^{(t)}\)</span> and a particular <span class="math">\(\theta^{(t)}\)</span> as follows,</p>
<p><span class="math">\[
\begin{align*}
  v^{(t)}
  &amp; = \frac{t+1}{2} x^{(t)} - \frac{t-1}{2} x^{(t-1)}
    = x^{(t-1)} + \frac{1}{\theta^{(t)}} (x^{(t)} - x^{(t-1)}) \\
  \theta^{(t)}
  &amp; = \frac{2}{t+1} \\
\end{align*}
\]</span></p>
<p>This definition also allows us to redefine <span class="math">\(y^{(t)}\)</span>,</p>
<p><span class="math">\[
  y^{(t)}
  = (1 - \theta^{(t)}) x^{(t)} + \theta^{(t)} v^{(t)} \\
\]</span></p>
<p><strong>Step 2</strong> Use the Lipschitz property of <span class="math">\(g\)</span> and subgradient property of <span class="math">\(h\)</span> to upper bound <span class="math">\((g+h)(x^{(t+1)})\)</span>. Let’s begin by defining <span class="math">\(x^{+} \triangleq x^{(t)}\)</span>, <span class="math">\(x \triangleq x^{(t-1)}\)</span>, <span class="math">\(y \triangleq y^{(t-1)}\)</span>, <span class="math">\(\theta \triangleq \theta^{(t-1)}\)</span>, <span class="math">\(v^{+} \triangleq v^{(t)}\)</span>, and <span class="math">\(v \triangleq v^{(t-1)}\)</span>. From the Lipschitz property of <span class="math">\(g\)</span>, we immediately get,</p>
<p><span class="math">\[
  g(x^{+}) \le g(y) + \nabla g(y)^T (x^{+} - y) + \frac{L}{2} ||x^{+} - y||_2^2
\]</span></p>
<p>Let’s immediately assume <span class="math">\(\alpha \le \frac{1}{L}\)</span>, so we can replace <span class="math">\(\frac{L}{2}\)</span> with <span class="math">\(\frac{1}{2 \alpha}\)</span>. Now let’s derive the subgradient property of <span class="math">\(h\)</span> we need. Recall the subgradient definition,</p>
<p><span class="math">\[
  h(z) \ge h(\tilde{x}) + G^T (z-\tilde{x}) \qquad G \in \partial h(\tilde{x})
\]</span></p>
<p>Now let <span class="math">\(x^{+} = \prox_{\alpha h}(\tilde{x}) = \arg\min_{w} \alpha h(w) + \frac{1}{2}||w - \tilde{x}||_2^2\)</span>. According to the KKT conditions, 0 must be in the subdifferential of <span class="math">\(\alpha h(x^{+}) + \frac{1}{2} || x^{+} - \tilde{x} ||_2^2\)</span>. Plugging this in, we see that,</p>
<p><span class="math">\[
\begin{align*}
  0 &amp; \in \alpha \partial h(x^{+}) + (x^{+} - \tilde{x}) \\
  \frac{1}{\alpha} \left( \tilde{x} - x^{+} \right) &amp; \in \partial h(x^{+})
\end{align*}
\]</span></p>
<p>We now have a subgradient for <span class="math">\(h(x^{+})\)</span>. Plugging this back into the subgradient condition with <span class="math">\(\tilde{x} \rightarrow x^{+}\)</span>,</p>
<p><span class="math">\[
\begin{align*}
  h(z)
  &amp;\ge h(x^{+}) + \frac{1}{\alpha} \left( \tilde{x} - x^{+} \right)^T(z - x^{+}) \\
  h(z) + \frac{1}{\alpha} \left( x^{+} - \tilde{x} \right)^T (z - x^{+})
  &amp;\ge h(x^{+}) \\
\end{align*}
\]</span></p>
<p>Finally, substitute <span class="math">\(\tilde{x} = y - \alpha \nabla g(y)\)</span> to obtain our desired upper bound on <span class="math">\(h(x^{+})\)</span>,</p>
<p><span class="math">\[
\begin{align*}
  h(x^{+})
  &amp; \le h(z) + \frac{1}{\alpha} \left( x^{+} - \left(y - \alpha \nabla g(y) \right) \right)^T (z - x^{+}) \\
  &amp; = h(z) + \nabla g(y)^T (z - x^{+}) + \frac{1}{\alpha} ( x^{+} - y )^T (z - x^{+})
\end{align*}
\]</span></p>
<p>Nice. Now add the Lipschitz bound on <span class="math">\(g\)</span> and the subgradient bound on <span class="math">\(h\)</span> to obtain an upper bound on <span class="math">\((g+h)(x^{+})\)</span>, then invoke convexity on <span class="math">\(g(z) \ge g(y) + \nabla g(y)^T (z-y)\)</span> to get rid of the linear term involving <span class="math">\(\nabla g(y)\)</span>,</p>
<p><span class="math">\[
\begin{align*}
  (g+h)(x^{+})
  &amp; \le g(y) + h(z) + \nabla g(y)^T (z-y) + \frac{1}{\alpha} (x^{+} - y)^T (z - x^{+}) + \frac{1}{2\alpha} ||x^{+} - y||_2^2 \\
  &amp; \le g(z) + h(z) + \frac{1}{\alpha} (x^{+} - y)^T (z - x^{+}) + \frac{1}{2\alpha} ||x^{+} - y||_2^2
\end{align*}
\]</span></p>
<p><strong>Step 3</strong> Use the previous upper bound to obtain the equation necessary for invoking Accelerated Gradient Descent’s proof. The core of this is to manipulate and bound the following statement,</p>
<p><span class="math">\[
  (g+h)(x^{+}) - \theta (g+h)(x^{*}) - (1-\theta) (g+h)(x)
\]</span></p>
<p>First, upper bound <span class="math">\(-(g+h)(x^{*})\)</span> and <span class="math">\(-(g+h)(x)\)</span> with <span class="math">\(z = x^{*}\)</span> and <span class="math">\(z = x^{+}\)</span> using the result of Step 2, then add zero and factor the quadratic,</p>
<p><span class="math">\[
\begin{align*}
  &amp; (g+h)(x^{+}) - \theta (g+h)(x^{*}) - (1-\theta) (g+h)(x) \\
  &amp; \le (g+h)(x^{+}) + \theta \left(
      - (g+h)(x^{+}) + \frac{1}{\alpha} (x^{+} - y)^T (x^{*} - x^{+}) + \frac{1}{2 \alpha} ||x^{+} - y||_2^2
    \right) \\
  &amp; \qquad + (1-\theta) \left(
      - (g+h)(x    ) + \frac{1}{\alpha} (x^{+} - y)^T (x     - x^{+}) + \frac{1}{2 \alpha} ||x     - y||_2^2
    \right) \\
  &amp; = \frac{1}{\alpha} (x^{+} - y)^T ( \theta x^{*} + (1-\theta) x - x^{+} ) + \frac{1}{2 \alpha} ||x^{+} - y||_2^2 \pm \frac{1}{2 \alpha} ||\theta x^{*} + (1-\theta) x - x^{+} ||_2^2 \\
  &amp; = \frac{1}{2 \alpha} \left(
    ||x^{+} - y + \theta x^{*} + (1 - \theta) x - x^{+}||_2^2
    - ||\theta x^{*} + (1 - \theta) x - x^{+}||_2^2
  \right) \\
  &amp; = \frac{1}{2 \alpha} \left(
    ||y - \theta x^{*} - (1 - \theta) x||_2^2
    - || x^{+} - \theta x^{*} - (1 - \theta) x||_2^2
  \right) \\
\end{align*}
\]</span></p>
<p>Finally, use <span class="math">\(y = (1 - \theta) x + \theta v\)</span> to get <span class="math">\(y - (1-\theta) x = \theta v\)</span> and then <span class="math">\(v^{+} = x + \frac{1}{\theta} ( x^{+} - x )\)</span> to obtain <span class="math">\(\theta v^{+} = x^{+} - (1-\theta) x\)</span>. Substituting these in,</p>
<p><span class="math">\[
\begin{align*}
  &amp; (g+h)(x^{+}) - \theta (g+h)(x^{*}) - (1-\theta) (g+h)(x) \\
  &amp; \le \frac{1}{2 \alpha} \left(
    ||\theta v - \theta x^{*}||_2^2 - || \theta v^{+} - \theta x^{*} ||_2^2
  \right) \\
  &amp; = \frac{\theta^2}{2 \alpha} \left(
    || v - x^{*}||_2^2 - || v^{+} - x^{*} ||_2^2
  \right) \\
\end{align*}
\]</span></p>
<p>Which was our original goal. We then follow the proof for Accelerated Gradient Descent with <span class="math">\(f \rightarrow g + h\)</span> starting from Step 2 to obtain the desired rate of convergence, <span class="math">\(O(1 / \sqrt{\epsilon})\)</span>.</p>
<p>As a final note, you’ll notice that in this proof <span class="math">\(\theta^{(t)} = \frac{2}{t+1}\)</span>, but in the original Accelerated Gradient proof <span class="math">\(\theta^{(t)} = \frac{2}{t+2}\)</span>. This ends up no mattering, as the only property we need being <span class="math">\(\frac{1 - \theta^{(t)}}{ (\theta^{(t)})^2 } \le \frac{1}{ (\theta^{(t)})^2 }\)</span>, which holds for either definition.</p>
<h1 id="when-should-i-use-it">When should I use it?</h1>
<p>As with <a href="/blog/accelerated-gradient-descent.html#usage">Accelerated Gradient</a>, the algorithm works well <em>as long as you get the step size right</em>. That means Backtracking Line Search is an absolute must if you don’t know <span class="math">\(g\)</span>’s Lipschitz constant analytically. If Line Search is possible, you can only gain over Proximal Gradient by employing Accelerated Proximal Gradient; with that said, test a Proximal Gradient algorithm first, and advance to Accelerated Proximal Gradient only if you’re sure you need the faster convergence rate.</p>
<h1 id="extensions">Extensions</h1>
<p><strong>Step Size</strong> As with Accelerated Gradient, getting the correct step size is of utmost importance. If <span class="math">\(\alpha^{(t)} &gt; \frac{1}{L}\)</span>, <em>the algorithm will diverge</em>. With that said, Backtracking Line Search will guarantee convergence. You can find an implementation in <a href="/blog/proximal-gradient.html#line_search">my previous post on Proximal Gradient</a>.</p>
<h1 id="references">References</h1>
<p><strong>Proof of convergence</strong> The proof of convergence is taken from Lieven Vandenberghe’s fantastic <a href="http://www.ee.ucla.edu/~vandenbe/236C/lectures/fgrad.pdf">EE236c slides</a>.</p>
<h1 id="reference-implementation">Reference Implementation</h1>
<div class="highlight"><pre><code class="python"><span class="k">def</span> <span class="nf">accelerated_proximal_gradient</span><span class="p">(</span><span class="n">g_gradient</span><span class="p">,</span> <span class="n">h_prox</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span>
                                  <span class="n">alpha</span><span class="p">,</span> <span class="n">n_iterations</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Proximal Gradient Descent</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  g_gradient : function</span>
<span class="sd">      Compute the gradient of `g(x)`</span>
<span class="sd">  h_prox : function</span>
<span class="sd">      Compute prox operator for h * alpha</span>
<span class="sd">  x0 : array</span>
<span class="sd">      initial value for x</span>
<span class="sd">  alpha : function</span>
<span class="sd">      function computing step sizes</span>
<span class="sd">  n_iterations : int, optional</span>
<span class="sd">      number of iterations to perform</span>

<span class="sd">  Returns</span>
<span class="sd">  -------</span>
<span class="sd">  xs : list</span>
<span class="sd">      intermediate values for x</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">xs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x0</span><span class="p">]</span>
  <span class="n">ys</span> <span class="o">=</span> <span class="p">[</span><span class="n">x0</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iterations</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">xs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">ys</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">g_gradient</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">step</span> <span class="o">=</span> <span class="n">alpha</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">x_plus</span> <span class="o">=</span> <span class="n">h_prox</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">step</span> <span class="o">*</span> <span class="n">g</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
    <span class="n">y_plus</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="p">(</span><span class="n">t</span> <span class="o">/</span> <span class="p">(</span><span class="n">t</span> <span class="o">+</span> <span class="mf">3.0</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="n">x_plus</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">xs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_plus</span><span class="p">)</span>
    <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_plus</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">xs</span>

<span class="k">def</span> <span class="nf">backtracking_line_search</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">g_gradient</span><span class="p">,</span> <span class="n">h_prox</span><span class="p">):</span>
  <span class="n">alpha_0</span> <span class="o">=</span> <span class="mf">1.0</span>
  <span class="n">beta</span>    <span class="o">=</span> <span class="mf">0.9</span>
  <span class="k">def</span> <span class="nf">search</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha_0</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
      <span class="n">x_plus</span> <span class="o">=</span> <span class="n">h_prox</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">g_gradient</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">alpha</span><span class="p">)</span>
      <span class="n">G</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span><span class="o">/</span><span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">x_plus</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">g</span><span class="p">(</span><span class="n">x_plus</span><span class="p">)</span> <span class="o">+</span> <span class="n">h</span><span class="p">(</span><span class="n">x_plus</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">g</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="n">h</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">G</span><span class="o">*</span><span class="n">G</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">alpha</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">beta</span>
  <span class="k">return</span> <span class="n">search</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">&#39;__main__&#39;</span><span class="p">:</span>
  <span class="kn">import</span> <span class="nn">os</span>

  <span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
  <span class="kn">import</span> <span class="nn">yannopt.plotting</span> <span class="kn">as</span> <span class="nn">plotting</span>

  <span class="c">### ACCELERATED PROXIMAL GRADIENT ###</span>

  <span class="c"># problem definition</span>
  <span class="n">g</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="p">))</span>
  <span class="n">h</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">function</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">g</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">h</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="n">g_gradient</span>  <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
  <span class="n">h_prox</span>      <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span>
  <span class="n">alpha</span>       <span class="o">=</span> <span class="n">backtracking_line_search</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">g_gradient</span><span class="p">,</span> <span class="n">h_prox</span><span class="p">)</span>
  <span class="n">x0</span>          <span class="o">=</span> <span class="mf">5.0</span>
  <span class="n">n_iterations</span> <span class="o">=</span> <span class="mi">10</span>

  <span class="c"># run gradient descent</span>
  <span class="n">iterates</span> <span class="o">=</span> <span class="n">accelerated_proximal_gradient</span><span class="p">(</span>
                  <span class="n">g_gradient</span><span class="p">,</span> <span class="n">h_prox</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span>
                  <span class="n">n_iterations</span><span class="o">=</span><span class="n">n_iterations</span>
             <span class="p">)</span>

  <span class="c">### PLOTTING ###</span>

  <span class="n">plotting</span><span class="o">.</span><span class="n">plot_iterates_vs_function</span><span class="p">(</span><span class="n">iterates</span><span class="p">,</span> <span class="n">function</span><span class="p">,</span>
                                     <span class="n">path</span><span class="o">=</span><span class="s">&#39;figures/iterates.png&#39;</span><span class="p">,</span>
                                     <span class="n">y_star</span><span class="o">=</span><span class="mf">0.69314718055994529</span><span class="p">)</span>
  <span class="n">plotting</span><span class="o">.</span><span class="n">plot_iteration_vs_function</span><span class="p">(</span><span class="n">iterates</span><span class="p">,</span> <span class="n">function</span><span class="p">,</span>
                                      <span class="n">path</span><span class="o">=</span><span class="s">&#39;figures/convergence.png&#39;</span><span class="p">,</span>
                                      <span class="n">y_star</span><span class="o">=</span><span class="mf">0.69314718055994529</span><span class="p">)</span>
</code></pre></div>
    </article>
  </div>
</div> <!-- row -->
<!-- Disqus Comments -->
<div class="row-fluid disqus">
  <div id="disqus_thread"></div>
  <script type="text/javascript">
      /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
      var disqus_shortname = 'duckworthd-blog'; // required: replace example with your forum shortname

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function() {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>

    </div> <!-- container -->
    <!-- jquery -->
<script type="text/javascript"
        src="http://code.jquery.com/jquery-1.9.0.min.js"></script>

<!-- MathJax -->
<script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      //equationNumbers: {
      //  autoNumber: "all"
      //},
      extensions: ["color.js"]
    }
  });
</script>

<!-- Bootstrap -->
<script type="text/javascript"
        src="/assets/js/bootstrap.min.js"></script>

<!-- Lightbox -->
<script type="text/javascript"
        src="/assets/js/lightbox.js"></script>

  </body>
</html>

