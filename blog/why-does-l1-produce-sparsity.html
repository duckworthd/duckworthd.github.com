<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Why does \(L_1\) regularization produce sparse solution?</title>

    <link href="/assets/css/bootstrap.min.css" rel="stylesheet" type="text/css">
    <link href="/assets/css/syntax.css" rel="stylesheet" type="text/css">
    <link href="/assets/css/blah.css" rel="stylesheet" type="text/css">
    <link href="/assets/css/lightbox/lightbox.css" rel="stylesheet" type="text/css">
  </head>
  <body>
    <div class="container">
    <div class="navbar">
  <div class="navbar-inner">
    <div class="nav-div">
      <a class="brand logo" href="#">
        <img src="/assets/img/transmogrifier2.png"></img>
      </a>
      <ul class="nav pull-right">
        <li><a href="/index.html">Home</a></li>
        <li><a href="/projects.html">Projects</a></li>
        <li><a href="/blog.html">Blog</a></li>
        <li><a href="/feed.xml">
          <img src="/assets/img/glyphicons/glyphicons_417_rss.png"></img>
        </a></li>
      </ul>
    </div> <!-- span10 offset1 -->
  </div> <!-- /navbar-inner -->
</div> <!-- /navbar -->


<div class="row-fluid">
  <div class="post box-shadow">
    <header>
      <h1 class="header-title">Why does \(L_1\) regularization produce sparse solution?</h1>
      <p class="header-subtext">by Daniel Duckworth on Apr 22, 2013</p>
    </header>
    <article>
      <p>Supervised machine learning problems are typically of the form “minimize your error while regularizing your parameters.” The idea is that while many choices of parameters may make your training error low, the goal isn’t low training error – it’s low test-time error. Thus, parameters should be minimize training error while remaining “simple,” where the definition of “simple” is left up to the regularization function. Typically, supervised learning can be phrased as minimizing the following objective function,</p>
<p><span class="math">\[
  w^{*} = \arg\min_{w} \sum_{i} L(y_i, f(x_i; w)) + \lambda \Omega(w)
\]</span></p>
<p>where <span class="math">\(L(y_i, f(x_i; w))\)</span> is the loss for predicting <span class="math">\(f(x_i; w)\)</span> when the true label is for sample <span class="math">\(i\)</span> is <span class="math">\(y_i\)</span> and <span class="math">\(\Omega(w)\)</span> is a regularization function.</p>
<h1 id="sparsifying-regularizers">Sparsifying Regularizers</h1>
<p>There are many choices for <span class="math">\(\Omega(w)\)</span>, but the ones I’m going to talk about today are so called “sparsifying regularizers” such as <span class="math">\(||w||_1\)</span>. These norms are most often employed because they produce “sparse” <span class="math">\(w^{*}\)</span> – that is, <span class="math">\(w^{*}\)</span> with many zeros. This is in stark contrast to other regularizers such as <span class="math">\(\frac{1}{2}||w||_2^2\)</span> which leads to lots of small but nonzero entries in <span class="math">\(w^{*}\)</span>.</p>
<h1 id="why-sparse-solutions">Why Sparse Solutions?</h1>
<p><strong>Feature Selection</strong> One of the key reasons people turn to sparsifying regularizers is that they lead to automatic feature selection. Quite often, many of the entries of <span class="math">\(x_i\)</span> are irrelevant or uninformative to predicting the output <span class="math">\(y_i\)</span>. Minimizing the objective function using these extra features will lead to lower training error, but when the learned <span class="math">\(w^{*}\)</span> is employed at test-time it will depend on these features to be more informative than they are. By employing a sparsifying regularizer, the hope is that these features will automatically be eliminated.</p>
<p><strong>Interpretability</strong> A second reason for favoring sparse solutions is that the model is easier to interpret. For example, a simple sentiment classifier might use a binary vector where an entry is 1 if a word is present and 0 otherwise. If the resulting learned weights <span class="math">\(w^{*}\)</span> has only a few non-zero entries, we might believe that those are the most indicative words in deciding sentiment.</p>
<h1 id="how-does-it-work">How does it work?</h1>
<p>We now come to the 100 million question: why do regularizers like the 1-norm lead to sparse solutions? At some point someone probably told you “they’re our best convex approximation to <span class="math">\(\ell_0\)</span> norm,” but there’s a better reason than that. In fact, I claim that any regularizer that is non-differentiable at <span class="math">\(w_i = 0\)</span> can lead to sparse solutions.</p>
<p><strong>Intuition</strong> The intuition lies in the idea of subgradients. Recall that the subgradient of a (convex) function <span class="math">\(\Omega\)</span> at <span class="math">\(x\)</span> is any vector <span class="math">\(v\)</span> such that,</p>
<p><span class="math">\[
  \Omega(y) \ge \Omega(x) + v^T (y-x)
\]</span></p>
<p>The set of all subgradients for <span class="math">\(\Omega\)</span> at <span class="math">\(x\)</span> is called the subdifferential and is denoted <span class="math">\(\partial \Omega(x)\)</span>. If <span class="math">\(\Omega\)</span> is differentiable at <span class="math">\(x\)</span>, then <span class="math">\(\partial \Omega(x) = \{ \nabla \Omega(x) \}\)</span> – in other words, <span class="math">\(\partial \Omega(x)\)</span> contains 1 vector, the gradient. Where the subdifferential begins to matter is when <span class="math">\(\Omega\)</span> <em>isn’t</em> differentiable at <span class="math">\(x\)</span>. Then, it becomes something more interesting.</p>
<p>Suppose we want to minimize an unconstrained objective like the following,</p>
<p><span class="math">\[
  \min_{x} f(x) + \lambda \Omega(x)
\]</span></p>
<p>By the <a href="http://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions">KKT conditions</a>, 0 must be in the subdifferential at the minimizer <span class="math">\(x^{*}\)</span>,</p>
<p><span class="math">\[
\begin{align*}
  0 &amp; \in \nabla f(x^{*}) + \partial \lambda \Omega(x^{*}) \\
  - \frac{1}{\lambda} \nabla f(x^{*}) &amp; \in \partial \Omega(x^{*}) \\
\end{align*}
\]</span></p>
<p>Looking forward, we’re particularly interested in when the previous inequality holds when <span class="math">\(x^{*} = 0\)</span>. What conditions are necessary for this to be true?</p>
<p><strong>Dual Norms</strong> Since we’re primarily concerned with <span class="math">\(\Omega(x) = ||x||_1\)</span>, let’s plug that in. In the following, it’ll actually be easier to prove things about any norm, so we’ll drop the 1 from here on out.</p>
<p>Recal the definition of a dual norm. In particular, the dual norm of a norm <span class="math">\(||\cdot||\)</span> is defined as,</p>
<p><span class="math">\[
  ||y||_{*} = \sup_{||x|| \le 1} x^{T} y
\]</span></p>
<p>A cool fact is that the dual of a dual norm is the original norm. In other words,</p>
<p><span class="math">\[
  ||x|| = \sup_{||y||_{*} \le 1} y^{T} x
\]</span></p>
<p>Let’s take the gradient of the previous expression on both sides. A nice fact to keep in mind is that if we take the gradient of an expression of the form <span class="math">\(\sup_{y} g(y, x)\)</span>, then its gradient with respect to x is <span class="math">\(\nabla_x g(y^{*}, x)\)</span> where <span class="math">\(y^{*}\)</span> is any <span class="math">\(y\)</span> that achieves the <span class="math">\(\sup\)</span>. Since <span class="math">\(g(y, x) = y^{T} x\)</span>, that means <span class="math">\(\nabla_x g(y, x) = y^{*}\)</span></p>
<p><span class="math">\[
  \partial ||x|| = \{ y^{*} :  y^{*} = \arg\max_{||y||_{*} \le 1} y^{T} x \}
\]</span></p>
<p>Now let <span class="math">\(x = 0\)</span>. Then <span class="math">\(y^{T} x = 0\)</span> for all <span class="math">\(y\)</span>, so any <span class="math">\(y\)</span> with <span class="math">\(||y||_{*} \le 1\)</span> is in <span class="math">\(\partial ||x||\)</span> for <span class="math">\(x = 0\)</span>.</p>
<p>Back to our original goal, recall that</p>
<p><span class="math">\[
  -\frac{1}{\lambda} \nabla f(x) \in \partial ||x||
\]</span></p>
<p>If <span class="math">\(||-\frac{1}{\lambda} \nabla f(x)||_{*} \le 1\)</span>, then we’ve already established that <span class="math">\(-\frac{1}{\lambda} \nabla f(x)\)</span> is in <span class="math">\(\partial ||x||\)</span> for <span class="math">\(x = 0\)</span>. In other words, <span class="math">\(x^{*} = 0\)</span> solves the original problem!</p>
<h1 id="conclusion">Conclusion</h1>
<p>In the previous section, we showed that in order to solve the problem <span class="math">\(\min_{x} f(x) + \lambda \Omega(x)\)</span>, it is necessary that <span class="math">\(-\frac{1}{\lambda} \nabla f(x^{*}) \in \partial \Omega(x^{*})\)</span>. If <span class="math">\(\Omega(x^{*})\)</span> is differentiable at <span class="math">\(x^{*}\)</span>, then there can be only 1 possible choice for <span class="math">\(x^{*}\)</span>, but in all other cases there are a multitude of potential solutions. When we are particularly concerned with sparse solutions and when <span class="math">\(\Omega(x)\)</span> isn’t differentiable at <span class="math">\(x = 0\)</span>, there is a set of values which <span class="math">\(-\frac{1}{\lambda} \nabla f(x^{*})\)</span> can take on such that <span class="math">\(x^{*} = 0\)</span> is still an optimal solution. This is why <span class="math">\(\Omega(x) = ||x||_1\)</span> leads to sparsification.</p>
<h1 id="references">References</h1>
<p>Everything written here was explained to me by the ever-knowledgable MetaOptimize king, <a href="https://twitter.com/atpassos">Alexandre Passos</a>.</p>
    </article>
  </div>
</div> <!-- row -->
<!-- Disqus Comments -->
<div class="row-fluid disqus">
  <div id="disqus_thread"></div>
  <script type="text/javascript">
      /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
      var disqus_shortname = 'duckworthd-blog'; // required: replace example with your forum shortname

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function() {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>

    </div> <!-- container -->
    <!-- jquery -->
<script type="text/javascript"
        src="http://code.jquery.com/jquery-1.9.0.min.js"></script>

<!-- MathJax -->
<script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      //equationNumbers: {
      //  autoNumber: "all"
      //},
      extensions: ["color.js"]
    }
  });
</script>

<!-- Bootstrap -->
<script type="text/javascript"
        src="/assets/js/bootstrap.min.js"></script>

<!-- Lightbox -->
<script type="text/javascript"
        src="/assets/js/lightbox.js"></script>

  </body>
</html>

