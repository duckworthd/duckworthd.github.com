
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Sparse Features and L2 Regularization - Bayesian World</title>
  <meta name="author" content="duckworthd">

  
  <meta name="description" content="Suppose you’re doing some typical supervised learning on a gigantic dataset where the total loss over all samples for parameter is simply the sum of &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://duckworthd.github.com/blog/2012/05/10/sparse-features-and-l2-regularization">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="Bayesian World" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  


  <!-- MathJax support -->
  <script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Bayesian World</a></h1>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:duckworthd.github.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/about.html">About</a></li>
  <li><a href="http://twitter.com/#!/duck">Twitter</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Sparse Features and L2 Regularization</h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-05-10T17:21:00-07:00" pubdate data-updated="true">May 10<span>th</span>, 2012</time>
        
      </p>
    
  </header>


<div class="entry-content"><p>Suppose you’re doing some typical supervised learning on a gigantic dataset where the total loss over all samples for parameter <script type="math/tex">w</script> is simply the sum of the losses of each sample <script type="math/tex">i</script>, i.e.,</p>

<script type="math/tex; mode=display">
  L(w) = \sum_{i} l(x_i, y_i, w)
</script>

<p>Basically any loss function you can think of in the i.i.d sample regime can be composed this way.  Since we assumed that your dataset was huge, there’s no way you’re going to be able to load it all into memory for BFGS, so you choose to use <a href="http://en.wikipedia.org/wiki/Stochastic_gradient_descent">Stochastic Gradient Descent</a>.  The update for sample <script type="math/tex">i</script> with step size <script type="math/tex">\eta_t</script> would then be,</p>

<script type="math/tex; mode=display">
  w_{t+1} = w_t - \eta_t \nabla_w l(x_i, y_i, w_t)
</script>

<p>So far, so good.  If <script type="math/tex">\nabla_w l(x_i, y_i, w)</script> is sparse, then you only need to change a handful of <script type="math/tex">w</script>’s components.  Of course, being the astute Machine Learning expert that you are, you know that you’re going to need some regularization.  Let’s redefine the total loss and take a look at our new update equation,</p>

<script type="math/tex; mode=display">
\begin{align}
  L(w) & = \sum_{i} l(x_i, y_i, w) + \frac{\lambda}{2}||w||_2^2  \\
  w_{t+1} & = w_t - \eta_t \left( \nabla_w l(x_i, y_i, w_t) + \lambda w_t \right)
\end{align}
</script>

<p>Uh oh.  Now that <script type="math/tex">w</script> appears in our Stochastic Gradient Descent update equation, you’re going to have change <em>every</em> non-zero element of <script type="math/tex">w</script> at <em>every</em> iteration, even if <script type="math/tex">\nabla_w l(x_i, y_i, w)</script> is sparse!  Whatever shall you do?</p>

<p>The answer isn’t as scary as you might think.  Let’s do some algebraic manipulation from <script type="math/tex">t=0</script>,</p>

<script type="math/tex; mode=display">
\begin{align}
  w_{1} 
  & = w_0 - \eta_0 \left( \nabla_w l(x_i, y_i, w_0) + \lambda w_0 \right) \\
  & = w_0 - \eta_0 \nabla_w l(x_i, y_i, w_0) - \eta_0 \lambda w_0 \\
  & = (1 - \eta_0 \lambda ) w_0 - \eta_0 \nabla_w l(x_i, y_i, w_0) \\
  & = (1 - \eta_0 \lambda ) \left(
      w_0 - \frac{\eta_0}{1-\eta_0 \lambda } \nabla_w l(x_i, y_i, w_0)
    \right) \\
\end{align}
</script>

<p>Do you see it now?  <script type="math/tex">L_2</script> regularization is really just a <em>rescaling</em> of <script type="math/tex">w_t</script> at <em>every</em> iteration.  Thus instead of keeping <script type="math/tex">w_t</script>, let’s keep track of,</p>

<script type="math/tex; mode=display">
\begin{align}
  c_t & = \prod_{\tau=0}^t (1-\eta_{\tau} \lambda )  \\
  \bar{w}_t & = \frac{w_t}{c_t}
\end{align}
</script>

<p>where you update <script type="math/tex">\bar{w}_t</script> and <script type="math/tex">c_t</script> by,</p>

<script type="math/tex; mode=display">
\begin{align}
  \bar{w}_{t+1} 
  & = \bar{w}_t - \frac{\eta_t}{(1 - \eta_t) c_t} \nabla_w l(x_i, w_i, c_t \bar{w}_t) \\
  c_{t+1} 
  & = (1 - \eta_t \lambda) c_t
\end{align}
</script>

<p>And that’s it!  As a final note, depending what value you choose for <script type="math/tex">\lambda</script>, <script type="math/tex">c_t</script> is going to get really big or really small pretty fast.  The usual “take the log” tricks aren’t going to fly, either, as <script type="math/tex">c_t</script> need not be positive.  The only way around it I’ve found is to check every iteration if <script type="math/tex">c_t</script> is getting out of hand, then transform <script type="math/tex">\bar{w}_{t} \leftarrow \bar{w}_t c_t</script> and <script type="math/tex">c_t \leftarrow 1</script> if it is.</p>

<p>Finally, credit should be given where credit is due.  This is a slightly more detailed explanation of <a href="http://blog.smola.org/post/940672544/fast-quadratic-regularization-for-online-learnin">Alex Smola’s Blog Post</a> from about a year ago, which in turn is accredited to Leon Bottou.</p>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">duckworthd</span></span>

      








  


<time datetime="2012-05-10T17:21:00-07:00" pubdate data-updated="true">May 10<span>th</span>, 2012</time>
      


    </p>
    
      <div class="sharing">
  
  <a href="http://twitter.com/share" class="twitter-share-button" data-url="http://duckworthd.github.com/blog/2012/05/10/sparse-features-and-l2-regularization/" data-via="duck" data-counturl="http://duckworthd.github.com/blog/2012/05/10/sparse-features-and-l2-regularization/" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
      
    </p>
  </footer>
</article>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2012/05/10/sparse-features-and-l2-regularization/">Sparse Features and L2 Regularization</a>
      </li>
    
  </ul>
</section>


<section>
  <h1>Latest Tweets</h1>
  <ul id="tweets">
    <li class="loading">Status updating...</li>
  </ul>
  <script type="text/javascript">
    $.domReady(function(){
      getTwitterFeed("duck", 4, false);
    });
  </script>
  <script src="/javascripts/twitter.js" type="text/javascript"> </script>
  
    <a href="http://twitter.com/duck" class="twitter-follow-button" data-show-count="false">Follow @duck</a>
  
</section>





  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2012 - duckworthd -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
