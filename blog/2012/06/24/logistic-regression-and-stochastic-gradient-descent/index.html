
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Logistic Regression and Stochastic Gradient Descent - Bayesian World</title>
  <meta name="author" content="duckworthd">

  
  <meta name="description" content="It seems that the de facto standard for large scale optimization these days is Stochastic Gradient Descent, and there’s a good reason why. Stochatic &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://duckworthd.github.com/blog/2012/06/24/logistic-regression-and-stochastic-gradient-descent">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="Bayesian World" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  


  <!-- MathJax support -->
  <script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Bayesian World</a></h1>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:duckworthd.github.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/about.html">About</a></li>
  <li><a href="http://twitter.com/#!/duck">Twitter</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">Logistic Regression and Stochastic Gradient Descent</h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-06-24T13:48:00-07:00" pubdate data-updated="true">Jun 24<span>th</span>, 2012</time>
        
      </p>
    
  </header>


<div class="entry-content"><p>It seems that the <em>de facto</em> standard for large scale optimization these days is Stochastic Gradient Descent, and there’s a good reason why.  Stochatic Gradient Descent is extremely easy to implement and parallelizes directly for “separable” functions (that is, when your objective is the sum of a bunch of other functions).</p>

<p>Let’s look at an example. Suppose you are trying to create a spam classifier.  Your boss has handed you a set of training samples <script type="math/tex"> \{ x_i, y_i \}</script> pairs where <script type="math/tex">x_i</script> is a feature vector describing the <script type="math/tex">i</script>th labeled email and <script type="math/tex">y_i</script> is its corresponding label, <script type="math/tex">1</script> for “ham” and <script type="math/tex">0</script> for “spam”.  In logistic regression, we want to learn a weight vector <script type="math/tex">w</script> such that <script type="math/tex">f(x,w) = \frac{1}{1 + \exp(- w^{T} x)}</script> is close to <script type="math/tex">1</script> if <script type="math/tex">x</script>’s label is <script type="math/tex">1</script> and close to <script type="math/tex">0</script> if its label is <script type="math/tex">0</script> for all training samples.  Since <script type="math/tex">f(x,w) \in [0,1]</script>, we can interpret our prediction as the <em>probability</em> that x belongs is ham (the probability it is spam is simply one minus that).  With this in mind, we will try to make our training data as likely as possible; that is,</p>

<script type="math/tex; mode=display">
  \max_{w} f(w) = \prod_{i} f(x_i, w)^{y_i} (1 - f(x_i, w))^{1 - y_i}
</script>

<p>Exponentiating <script type="math/tex">f(x_i, w)</script> by <script type="math/tex">y_i</script> is simply a trick for saying “if <script type="math/tex">y_i</script> is 1, choose this, and if it’s 0, choose that”.  It is customary to take the log of such loss functions, as doing so will give us a sum rather than a product,</p>

<script type="math/tex; mode=display">
  \max_{w} \log f(w) = \sum_{i} y_i \log f(x_i, w)  + (1 - y_i) \log (1 - f(x_i, w))
</script>

<p>It’s now clear that our objective function is “separable”, so let’s first derive a normal Gradient Ascent algorithm.  First, we compute the gradient of <script type="math/tex">f(w)</script>,</p>

<script type="math/tex; mode=display">
\begin{align}
  \nabla_w \log f(w) 
  &= \nabla_w \sum_{i} y_i \log f(x_i, w)  + (1 - y_i) \log (1 - f(x_i, w)) \\
  &= \nabla_w \sum_{i} y_i \log \frac{ f(x_i, w)}{ 1 - f(x_i, w)}  + \log (1 - f(x_i, w)) \\
  &= \nabla_w \sum_{i} y_i w^T x_i  + \log (1 - f(x_i, w)) \\
  &= \nabla_w \sum_{i} y_i w^T x_i  - \log (1 + \exp w^T x_i ) \\
  &= \sum_{i} \nabla_w \left( y_i w^T x_i  - \log (1 + \exp w^T x_i ) \right) \\
  &= \sum_{i} y_i x_i  - \frac{1}{1 + \exp (w^T x_i)} \exp(w^T x_i) x_i  \\
  &= \sum_{i} y_i x_i  - \frac{1}{1 + \exp (- w^T x_i)} x_i  \\
  &= \sum_{i} ( y_i - f(x_i, w)) x_i  \\
\end{align}
</script>

<p>All that work for something so simple!  Armed with our newfound gradient, the Gradient Ascent algorithm (for step sizes <script type="math/tex">\alpha_t</script>) is,</p>

<ol>
  <li>Initialize <script type="math/tex">w_0</script> arbitrarily</li>
  <li>for <script type="math/tex">t = 0, 1, \ldots</script> until convergence
    <ol>
      <li>Calculate gradient <script type="math/tex">\nabla \log f(w_t)</script></li>
      <li>Set <script type="math/tex">w_{t+1} = w_{t} + \alpha_t \nabla \log f(w_t)</script></li>
    </ol>
  </li>
</ol>

<p>The beauty of Stocahstic Gradient Descent for separable functions is in the separability of its gradient.  See how <script type="math/tex">\nabla \log f(w)</script> is a sum?  This is a direct consequence of <script type="math/tex">\log f(w)</script> also being a sum, and it means that we can calculate each of its components <em>independently</em>.  This leads to another intuitive algorithm,</p>

<ol>
  <li>Initialize <script type="math/tex">w_0</script> arbitrarily</li>
  <li>for <script type="math/tex">t = 0, 1, \ldots</script>
    <ol>
      <li>For <script type="math/tex">i = 0, 1, \ldots</script>
        <ol>
          <li>Calculate gradient with respect to sample <script type="math/tex">i</script>; that is, <script type="math/tex">y_i - f(x_i, w_{t,i}) x_i</script></li>
          <li>Set <script type="math/tex">w_{t, i+1} = w_{t, i} + \alpha_{t,i} (y_i - f(x_i, w_{t,i}) x_i)</script></li>
        </ol>
      </li>
      <li>Set <script type="math/tex">w_{t+1, 0}</script> to the last  <script type="math/tex">w_{t, i}</script> calculated</li>
    </ol>
  </li>
</ol>

<p>This is Stochastic Gradient Ascent.  The beauty of it is its simplicity – take a single training sample, calculate the gradient of the objective function with respect to it, and take a step.  Now we can begin optimizing even without seeing all of the data!</p>

<h2 id="references">References</h2>

<ul>
  <li><a href="http://www.stat.cmu.edu/~cshalizi/350/lectures/26/lecture-26.pdf"> Logistic Regression and Newton’s Method </a></li>
</ul>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">duckworthd</span></span>

      








  


<time datetime="2012-06-24T13:48:00-07:00" pubdate data-updated="true">Jun 24<span>th</span>, 2012</time>
      


    </p>
    
      <div class="sharing">
  
  <a href="http://twitter.com/share" class="twitter-share-button" data-url="http://duckworthd.github.com/blog/2012/06/24/logistic-regression-and-stochastic-gradient-descent/" data-via="duck" data-counturl="http://duckworthd.github.com/blog/2012/06/24/logistic-regression-and-stochastic-gradient-descent/" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2012/05/10/the-limits-of-bayesian-networks/" title="Previous Post: The Limits of Bayesian Networks">&laquo; The Limits of Bayesian Networks</a>
      
      
        <a class="basic-alignment right" href="/blog/2012/06/24/admm-alternating-direction-method-of-multipliers/" title="Next Post: ADMM: Alternating Direction Method of Multipliers">ADMM: Alternating Direction Method of Multipliers &raquo;</a>
      
    </p>
  </footer>
</article>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2012/06/24/admm-alternating-direction-method-of-multipliers/">ADMM: Alternating Direction Method of Multipliers</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/06/24/logistic-regression-and-stochastic-gradient-descent/">Logistic Regression and Stochastic Gradient Descent</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/05/10/the-limits-of-bayesian-networks/">The Limits of Bayesian Networks</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/05/10/sparse-features-and-l2-regularization/">Sparse Features and L2 Regularization</a>
      </li>
    
  </ul>
</section>


<section>
  <h1>Latest Tweets</h1>
  <ul id="tweets">
    <li class="loading">Status updating...</li>
  </ul>
  <script type="text/javascript">
    $.domReady(function(){
      getTwitterFeed("duck", 4, false);
    });
  </script>
  <script src="/javascripts/twitter.js" type="text/javascript"> </script>
  
    <a href="http://twitter.com/duck" class="twitter-follow-button" data-show-count="false">Follow @duck</a>
  
</section>





  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2012 - duckworthd -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
