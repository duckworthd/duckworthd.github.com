<!DOCTYPE html>
<html lang="en">
<head>
  <!-- stylesheets -->
	<link rel="stylesheet" type="text/css" href="http://stronglyconvex.com/theme/css/style.css">
	<link rel="stylesheet" type="text/css" href="http://stronglyconvex.com/theme/css/pygments.css">
	<link rel="stylesheet" type="text/css" href="http://stronglyconvex.com/assets/lightbox/css/lightbox.css">

  <!-- fonts -->
	<link href='http://fonts.googleapis.com/css?family=Open+Sans:800,400,300|Inconsolata' rel='stylesheet' type='text/css'>
	<link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:700,900' rel='stylesheet' type='text/css'>
	<link href='http://fonts.googleapis.com/css?family=Lora:700' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Playball' rel='stylesheet' type='text/css'>
	<link href='//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css' rel='stylesheet' type='text/css'>

  <!-- RSS/ATOM feeds -->
	<link href="http://stronglyconvex.com/" type="application/atom+xml" rel="alternate" title="Strongly Convex ATOM Feed" />


		<title>Strongly Convex</title>
		<meta charset="utf-8" />
</head>
<body>
	<section id="sidebar">
		<figure id="user_logo">
      <a href="http://stronglyconvex.com"><div class="logo">&nbsp;</div></a>
		</figure>

		<div class="user_meta">
      <h1 id="user">
        <a  href="http://stronglyconvex.com/"
            class="sitename">
          Strongly Convex
        </a>
      </h1>
			<h2></h2>


			<ul>
        <hr></hr>
					<li><a href="/">Words</a></li>
          <hr></hr>
					<li><a href="/code.html">Code</a></li>
          <hr></hr>
					<li><a href="/about.html">About</a></li>
          <hr></hr>
			</ul>
		</div>
		<footer>
			<address>
				Powered by <a href="http://pelican.notmyidea.org/">Pelican</a>,
		    Theme by <a href="https://github.com/wting/pelican-svbtle">wting</a>.
			</address>
		</footer>
	</section>

	<section id="posts">
	<header>
      <h1 class="title">
			  <a  href="http://stronglyconvex.com/blog/admm-revisited.html"
            rel="bookmark"
			  	  title="Permalink to ADMM revisited">
          ADMM revisited
        </a>
      </h1>
		  <abbr class="published">Sun 20 July 2014</abbr>
	</header>
  <article>
    <p>When I originally wrote about the <a href="http://stronglyconvex.com/blog/admm.html">Alternating Direction Method of
Multipliers</a> algorithm, the community's understanding of its
convergence properties was light to say the least. While it has long been
known (See <a href="http://web.stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf">Boyd's excellent article</a>, Appendix A) that ADMM <em>will</em>
converge, it is only recently that the community has begun to establish <em>how
fast</em> it converges (e.g. <a href="http://arxiv.org/abs/1208.3922">Hong</a>, <a href="ftp://ftp.math.ucla.edu/pub/camreport/cam12-52.pdf">Deng</a>, <a href="http://iqua.ece.toronto.edu/~cfeng/notes/cfeng-admm12.pdf">Feng</a>, <a href="http://www.math.hkbu.edu.hk/~xmyuan/Paper/HeYuan-SecondRevision.pdf">He</a>).</p>
<p>In this article, we'll explore one way to establish an <mathjax>$O(1 / \epsilon)$</mathjax> rate
of convergence. Unlike previous convergence proofs presented in this blog, we
won't directly show that the primal objective value alone converges to its
optimal value; instead, we'll show that a particular function involving the
primal objective and a <a href="http://supernet.isenberg.umass.edu/austria_lectures/fvisli.pdf">Variational Inequality</a>
converges at the desired rate.</p>
<h1><a name="implementation" href="#implementation">How does it work?</a></h1>
<p>Let's begin by introducing the optimization problem ADMM solves,</p>
<p><mathjax>$$
\begin{align}
\begin{split}
  \underset{x,z}{\text{minimize}} \qquad
    &amp; f(x) + g(z) \\
  \text{s.t.}                     \qquad
    &amp; Ax + Bz = c \\
\end{split} \label{eqn:objective}
\end{align}
$$</mathjax></p>
<p>This problem is characterized by 2 primal variables, <mathjax>$x$</mathjax> and <mathjax>$z$</mathjax>, which are
related by a linear equation. In machine learning, a common scenario is to
choose <mathjax>$A$</mathjax>, <mathjax>$B$</mathjax>, and <mathjax>$c$</mathjax> such that <mathjax>$x = z$</mathjax>, making the setup particularly
simple. For the rest of this article, we'll assume that <mathjax>$Ax + Bz = c$</mathjax> is the
only constraint we consider -- other constraints can be incorporated into
<mathjax>$f$</mathjax> and <mathjax>$g$</mathjax> by letting them be infinite when constraints are broken.</p>
<p>The ADMM algorithm then finds the "saddle point" of the Augmented
Lagrangian for the corresponding problem,</p>
<p><mathjax>$$
\begin{align} \label{eqn:lagrangian}
  L_{\rho}(x, z, y) = f(x) + g(z) + \langle y, Ax + Bz - c \rangle
                      + \frac{\rho}{2} || Ax + Bz - c ||_2^2
\end{align}
$$</mathjax></p>
<p>Note that we say <em>Augmented</em> Lagrangian, as the typical Lagrangian does not
include the final quadratic term. It's easy to see, however, that the quadratic
does not affect the problem's optimal solution, as the constraint <mathjax>$Ax + Bz = c$</mathjax>
holds for all valid solutions.</p>
<p>The ADMM algorithm iteratively minimizes <mathjax>$L_{\rho}$</mathjax> with respect to <mathjax>$x$</mathjax> for
fixed <mathjax>$z$</mathjax> and <mathjax>$y$</mathjax>, then minimizes <mathjax>$z$</mathjax> for fixed <mathjax>$x$</mathjax> and <mathjax>$y$</mathjax>, and finally
takes a gradient step with respect to <mathjax>$y$</mathjax> for fixed <mathjax>$x$</mathjax> and <mathjax>$z$</mathjax>.</p>
<div class="pseudocode">
<p><strong>Input</strong> Step size <mathjax>$\rho$</mathjax>, initial primal iterates <mathjax>$x^{(0)}$</mathjax> and <mathjax>$z^{(0)}$</mathjax>,
            initial dual iterate <mathjax>$y^{(0)}$</mathjax></p>
<ol>
<li>For <mathjax>$t = 0, 1, \ldots$</mathjax><ol>
<li>Let <mathjax>$x^{(t+1)} = \underset{x}{\text{argmin}} \quad L_{\rho}( x        , z^{(t)}, y^{(t)} )$</mathjax></li>
<li>Let <mathjax>$z^{(t+1)} = \underset{z}{\text{argmin}} \quad L_{\rho}( x^{(t+1)}, z      , y^{(t)} )$</mathjax></li>
<li>Let <mathjax>$y^{(t+1)} = y^{(t)} + \rho ( Ax^{(t+1)} + Bz^{(t+1)} - c )$</mathjax></li>
</ol>
</li>
</ol>
</div>
<p>Intuitively, the extra quadratic term prevents each iteration of the
algorithm from stepping "too far" from the last iteration, an idea that's also
at the core of <a href="http://stronglyconvex.com/blog/proximal-gradient-descent.html">Proximal Gradient Descent</a>.</p>
<div class="img-center">
<p><img src="/assets/img/admm/convergence.gif"></img>
  <span class="caption">
    Animation of <mathjax>$x_t$</mathjax> and <mathjax>$z_t$</mathjax> converging to the minimum of the sum of
    2 quadratics.
  </span></p>
</div>
<p>In the remainder of the article, we'll often use the following notation for
conciseness,</p>
<p><mathjax>$$
\begin{align*}
  w    &amp;= \begin{bmatrix}
            x \\
            z \\
            y
          \end{bmatrix} \\
  h(w) &amp;= f(x) + g(z) \\
  F(w) &amp;= \begin{bmatrix}
            A^T y \\
            B^T y \\
            - (Ax + Bz - c)
          \end{bmatrix}
\end{align*}
$$</mathjax></p>
<h1><a name="proof" href="#proof">Why does it work?</a></h1>
<p>Unlike other convergence proofs presented on this website, we won't directly
show that the objective converges to its minimum as <mathjax>$t \rightarrow \infty$</mathjax>.
Indeed, limiting ourselves to analysis of the objective completely ignores the
constraint <mathjax>$Ax + Bz = c$</mathjax>. Instead, we'll use the following variational
inequality condition to describe an optimal solution. In particular, a solution
<mathjax>$w^{*}$</mathjax> is optimal if,</p>
<p><mathjax>$$
\begin{align} \label{vi}
  \forall w \in \mathbb{R}^{n} \qquad
    h(w) - h(w^{*}) + \langle w - w^{*}, F(w^{*}) \rangle &amp;\ge 0
\end{align}
$$</mathjax></p>
<div class="img-center">
<p><img src="/assets/img/admm/vi.jpg"></img>
  <span class="caption">
    Geometric interpretation of a the optimality condition for a variational
    inequality when ignoring <mathjax>$h(w)$</mathjax> from <a href="http://supernet.isenberg.umass.edu/austria_lectures/fvisli.pdf">Anna Nagurney</a>.
  </span></p>
</div>
<p>For the following proof, we'll replace <mathjax>$w^{*}$</mathjax> with
<mathjax>$\bar{w}_t = (1/t) \sum_{\tau=1}^{t} w_{\tau}$</mathjax> and <mathjax>$0$</mathjax> on the right hand side
with <mathjax>$-\epsilon_t$</mathjax> where <mathjax>$\epsilon_t = O(1/t)$</mathjax>. By showing that we can
approximately satisfy this inequality at a rate <mathjax>$O(1/t)$</mathjax>, we establish the
desired convergence rate.</p>
<p><strong>Assumptions</strong></p>
<p>The assumptions on ADMM are almost as light as we can imagine. This is
largely due to the fact that we needn't use gradients or subgradients for
<mathjax>$h(z)$</mathjax>.</p>
<ol>
<li><mathjax>$f(x) + g(z)$</mathjax> is convex.</li>
<li>There exists a solution <mathjax>$[ x^{*}; z^{*} ]$</mathjax> that minimizes <mathjax>$f(x) + g(z)$</mathjax>
  while respecting the constraint <mathjax>$Ax + Bz = c$</mathjax>.</li>
</ol>
<p><strong>Proof Outline</strong></p>
<p>The proof presented hereafter is a particularly simple if unintuitive one.
Theoretically, the only tools necessary are the linear lower bound definition
of a convex function, the subgradient condition for optimality in an
unconstrained optimization problem, and Jensen's Inequality. Steps 1 and
2 below rely purely on the first 2 of these tools. Step 3 merely massages
a preceding equation into a simpler form via completing squares. Step 4 closes
by exploiting a telescoping sum and Jensen's Inequality to obtain the desired
result,</p>
<p><mathjax>$$
\forall w \qquad
h(\bar{w}_t) - h(w) + \langle
  F(\bar{w}^{(t)}),
  \bar{w}^{(t)} - w
\rangle
\le \frac{1}{t} \left(
  \frac{\rho}{2} \norm{Ax-c}_2^2 + \frac{1}{2\rho} \norm{y}_2^2
\right)
$$</mathjax></p>
<p>As <mathjax>$t \rightarrow \infty$</mathjax>, the right hand side of this equation goes to 0,
rendering the same statement as the variational inequality optimality condition
in Equation <mathjax>$\ref{vi}$</mathjax>.</p>
<p><strong>Step 1</strong> Optimality conditions for Step A. In this portion of the proof,
we'll use the fact that <mathjax>$x^{(t+1)}$</mathjax> is defined as the solution of an
optimization problem to derive a subgradient for <mathjax>$f$</mathjax> at <mathjax>$x^{(t+1)}$</mathjax>. We'll then
substitute this into <mathjax>$f$</mathjax>'s definition of convexity. Finally, terms are
rearranged and the contents of Step C of the algorithm are used to derive
a final expression.</p>
<p>We begin by recognizing that <mathjax>$x^{(t+1)}$</mathjax> minimizes
<mathjax>$L_{\rho}(x, z^{(t)}, y^{(t)})$</mathjax> as a function of <mathjax>$x$</mathjax>. As <mathjax>$x$</mathjax> is unconstrained,
zero must be a valid subgradient for <mathjax>$L_{\rho}$</mathjax> evaluated at <mathjax>$x^{(t+1)},
z^{(t)}, y^{(t)}$</mathjax>,</p>
<p><mathjax>$$
\begin{align*}
  0
  &amp;\in \partial_x L_{\rho}(x^{(t+1)}, z^{(t)}, y^{(t)})                               \\
  &amp;= \partial_{x} f(x^{(t+1)}) + A^T y^{(t)} + \rho A^T (Ax^{(t+1)} + Bz^{(t)} - c)   \\
  - A^T \left( y^{(t)} + \rho (Ax^{(t+1)} + Bz^{(t)} - c) \right)
  &amp;\in \partial_x f(x^{(t+1)})
\end{align*}
$$</mathjax></p>
<p>As <mathjax>$f$</mathjax> is convex, we further know that it is lower bounded by its linear
approximation everywhere,</p>
<p><mathjax>$$
\begin{align*}
  \forall x \qquad
    f(x) &amp;\ge f(x^{(t+1)}) + \langle
      \partial_x f(x^{(t+1)}),
      x - x^{(t+1)}
    \rangle
\end{align*}
$$</mathjax></p>
<p>Substituting in our subgradient for <mathjax>$\partial_x f(x^{(t+1)})$</mathjax> and subtracting
the contents of the right hand side from both sides, we obtain,</p>
<p><mathjax>$$
\begin{align*}
  \forall x \qquad
    0 &amp;\le f(x) - f(x^{(t+1)}) + \langle
      A^T (y^{(t)} + \rho (A x^{(t+1)} + B z^{(t)} - c ),
      x - x^{(t+1)}
    \rangle
\end{align*}
$$</mathjax></p>
<p>Now recall Step C of the algorithm:
<mathjax>$y^{(t+1)} = y^{(t)} + \rho (A x^{(t+1)} + Bz^{(t+1)} - c)$</mathjax>. The left side of
the inner product looks very similar to this, so we'll substitute it in as best
we can,</p>
<p><mathjax>$$
\begin{align*}
  \forall x \qquad
    0 &amp;\le f(x) - f(x^{(t+1)}) + \langle
      A^T (y^{(t+1)} + \rho Bz^{(t)} - \rho Bz^{(t+1)}),
      x - x^{(t+1)}
    \rangle
\end{align*}
$$</mathjax></p>
<p>We finish by moving everything <em>not</em> multiplied by <mathjax>$\rho$</mathjax> to the opposite
side of the inequality,</p>
<p><mathjax>$$
\begin{align} \label{eqn:36}
  \forall x \qquad
    f(x^{(t+1)}) - f(x) + \langle
      x^{(t+1)} - x,
      A^T y^{(t+1)}
    \rangle
    &amp;\le \rho \langle
      Bz^{(t)} - Bz^{(t+1)},
      A x - A x^{(t+1)}
    \rangle
\end{align}
$$</mathjax></p>
<p><strong>Step 2</strong> Optimality conditions for Step B. Similar to Step 1, we'll use the
fact that <mathjax>$z^{(t+1)}$</mathjax> is the solution to an unconstrained optimization problem
and will substitute in Step C's definition for <mathjax>$y^{(t+1)}$</mathjax>.</p>
<p><mathjax>$$
\begin{align*}
  0
  &amp;\in \partial_z L(x^{(t+1)}, z, y^{(t)})                                          \\
  &amp;= \partial_z g(z^{(t+1)}) + B^T y^{(t)} + \rho B^T (Ax^{(t+1)} + Bz^{(t+1)} - c) \\
  - B^T \left( y^{(t)} + \rho (Ax^{(t+1)} + Bz^{(t+1)} - c) \right)
  &amp;\in \partial_z g(z^{(t+1)})
\end{align*}
$$</mathjax></p>
<p>As <mathjax>$g$</mathjax> is convex, it is lower bounded by its linear approximation,</p>
<p><mathjax>$$
\begin{align*}
  \forall z \qquad
    g(z) &amp;\ge g(z^{(t+1)}) + \langle
      \partial_z g(z^{(t+1)}),
      z - z^{(t+1)}
    \rangle
\end{align*}
$$</mathjax></p>
<p>Substituting in the previously derived subgradient and moving all terms to
the left side, we obtain,</p>
<p><mathjax>$$
\begin{align*}
  \forall z \qquad
    0 &amp;\le g(z) - g(z^{(t+1)}) + \langle
      B^T (y^{(t)} + \rho (A x^{(t+1)} + B z^{(t+1)} - c )),
      z - z^{(t+1)}
    \rangle
\end{align*}
$$</mathjax></p>
<p>Substituting in Step C's definition for <mathjax>$y^{(t+1)}$</mathjax> again and moving
everything to the opposite side of the inequality, we conclude that,</p>
<p><mathjax>$$
\begin{align} \label{eqn:37}
  \forall z \qquad
    g(z^{(t+1)}) - g(z) + \langle
      B^T y^{(t+1)},
      z^{(t+1)} - z
    \rangle
    &amp;\le 0
\end{align}
$$</mathjax></p>
<p><strong>Step 3</strong> We now sum Equation <mathjax>$\ref{eqn:36}$</mathjax> with Equation <mathjax>$\ref{eqn:37}$</mathjax>.
We'll end up with an expression that is not easy to understand initially, but
by factoring several of its terms into quadratic forms and substituting them
back in, we obtain a simpler expression that can be described as a sum of
squared 2-norms.</p>
<p>We begin by summing equations <mathjax>$\ref{eqn:36}$</mathjax> and <mathjax>$\ref{eqn:37}$</mathjax>.</p>
<p><mathjax>$$
\begin{align*}
  &amp; f(x^{(t+1)}) + g(z^{(t+1)}) - f(x) - g(z) + \langle
    B^T y^{(t+1)},
    z^{(t+1)} - z
  \rangle + \langle
    A^T y^{(t+1)},
    x^{(t+1)} - x
  \rangle \\
  &amp; \qquad \le \rho \langle
    Ax - Ax^{(t+1)},
    Bz^{(t)} - Bz^{(t+1)}
  \rangle
\end{align*}
$$</mathjax></p>
<p>Next, we use the definitions of <mathjax>$h(w)$</mathjax> and <mathjax>$F(w)$</mathjax> on the left hand side,</p>
<p><mathjax>$$
\begin{align*}
  &amp; h(w^{(t+1)}) - h(w) + \langle
    F(w^{(t+1)}),
    w^{(t+1)} - w
  \rangle + \langle
    Ax^{(t+1)} + Bz^{(t+1)} - c,
    y^{(t+1)} - y
  \rangle \\
  &amp; \qquad \le \rho \langle
    Ax - Ax^{(t+1)},
    Bz^{(t)} - Bz^{(t+1)}
  \rangle
\end{align*}
$$</mathjax></p>
<p>Then, moving the last term on the left side of the inequality over and
observing that Step C implies
<mathjax>$(1/\rho) (y^{(t+1)} - y^{(t)}) = Ax^{(t+1)} + Bz^{(t+1)} - c$</mathjax>,</p>
<p><mathjax>$$
\begin{align}
\begin{split}
  &amp; h(w^{(t+1)}) - h(w) + \langle
    F(w^{(t+1)}),
    w^{(t+1)} - w
  \rangle  \\
  &amp; \qquad \le \rho \langle
    Ax - Ax^{(t+1)},
    Bz^{(t)} - Bz^{(t+1)}
  \rangle + \frac{1}{\rho} \langle
    y^{(t+1)} - y^{(t)},
    y - y^{(t+1)}
  \rangle
\end{split} \label{eqn:38}
\end{align}
$$</mathjax></p>
<p>We will now tackle the two components on the right hand side of the
inequality in isolation. Our goal is to rewrite these inner products in terms
of sums of <mathjax>$\norm{\cdot}_2^2$</mathjax> terms.</p>
<p>We'll start with <mathjax>$\langle Ax - Ax^{(t+1)}, Bz^{(t)} - Bz^{(t+1)} \rangle$</mathjax>. In
the next equations, we'll add many terms that will cancel themselves out, then
we'll group them together into a sum of 4 terms,</p>
<p><mathjax>$$
\begin{align}
  &amp;
  \begin{split}
    2 \langle Ax - Ax^{(t+1)}, Bz^{(t)} - Bz^{(t+1)} \rangle
  \end{split} \notag \\
  &amp;
  \begin{split}
  = &amp; + \norm{Ax        -c}_2^2 &amp; + 2 \langle Ax         - c, B z^{(t  )} \rangle &amp; + \norm{Bz^{(t  )}}_2^2 \\
    &amp; - \norm{Ax        -c}_2^2 &amp; - 2 \langle Ax         - c, B z^{(t+1)} \rangle &amp; - \norm{Bz^{(t+1)}}_2^2 \\
    &amp; + \norm{Ax^{(t+1)}-c}_2^2 &amp; + 2 \langle Ax^{(t+1)} - c, B z^{(t+1)} \rangle &amp; + \norm{Bz^{(t+1)}}_2^2 \\
    &amp; - \norm{Ax^{(t+1)}-c}_2^2 &amp; - 2 \langle Ax^{(t+1)} - c, B z^{(t  )} \rangle &amp; - \norm{Bz^{(t  )}}_2^2
  \end{split} \notag \\
  &amp;
  \begin{split}
  = &amp; + \norm{Ax         + Bz^{(t)}   - c}_2^2 &amp; - \norm{Ax         + Bz^{(t+1)} - c}_2^2 \\
    &amp; + \norm{Ax^{(t+1)} + Bz^{(t+1)} - c}_2^2 &amp; - \norm{Ax^{(t+1)} + Bz^{(t  )} - c}_2^2
  \end{split} \label{eqn:39}
\end{align}
$$</mathjax></p>
<p>We'll do the same for <mathjax>$\langle y^{(t+1)} - y^{(t)}, y - y^{(t+1)} \rangle$</mathjax>,</p>
<p><mathjax>$$
\begin{align}
  &amp;
  \begin{split}
    2 \langle y^{(t+1)} - y^{(t)}, y - y^{(t+1)} \rangle
  \end{split} \notag \\
  &amp;
  \begin{split}
  = &amp; + \norm{y      }_2^2 &amp; + 2 \langle y      , - y^{(t  )} \rangle &amp; + \norm{y^{(t  )}}_2^2 \\
    &amp; - \norm{y      }_2^2 &amp; - 2 \langle y      , - y^{(t+1)} \rangle &amp; - \norm{y^{(t+1)}}_2^2 \\
    &amp; - \norm{y^{(t)}}_2^2 &amp; - 2 \langle y^{(t)}, - y^{(t+1)} \rangle &amp; - \norm{y^{(t+1)}}_2^2 \\
  \end{split} \notag \\
  &amp;
  \begin{split}
  = &amp; + \norm{y       - y^{(t  )}}_2^2
      - \norm{y       - y^{(t+1)}}_2^2
      - \norm{y^{(t)} - y^{(t+1)}}_2^2
  \end{split} \label{eqn:40}
\end{align}
$$</mathjax></p>
<p>Finally, let's plug equations <mathjax>$\ref{eqn:39}$</mathjax> and <mathjax>$\ref{eqn:40}$</mathjax> into
<mathjax>$\ref{eqn:38}$</mathjax>.</p>
<p><mathjax>$$
\begin{align}
\begin{split}
  &amp; h(w^{(t+1)}) - h(w) + \langle
    F(w^{(t+1)}),
    w^{(t+1)} - w
  \rangle  \\
  &amp; \qquad \le \frac{\rho}{2} \left( \begin{split}
    &amp; + \norm{Ax         + Bz^{(t  )} - c}_2^2 &amp; - \norm{Ax         + Bz^{(t+1)} - c}_2^2 \\
    &amp; + \norm{Ax^{(t+1)} + Bz^{(t+1)} - c}_2^2 &amp; - \norm{Ax^{(t+1)} + Bz^{(t  )} - c}_2^2
  \end{split} \right) \\
  &amp; \qquad + \frac{1}{2\rho} \left( \begin{split}
    &amp; + \norm{y       - y^{(t  )}}_2^2 \\
    &amp; - \norm{y       - y^{(t+1)}}_2^2 \\
    &amp; - \norm{y^{(t)} - y^{(t+1)}}_2^2
  \end{split} \right)
\end{split}
\end{align}
$$</mathjax></p>
<p>Recall that <mathjax>$(1/\rho)(y^{(t+1)} - y^{(t)}) = Ax^{(t+1)} + Bz^{(t+1)} - c$</mathjax>. Then,</p>
<p><mathjax>$$
\begin{align*}
  \frac{\rho}{2} \norm{ Ax^{(t+1)} + Bz^{(t+1)} - c }
  &amp;= \frac{\rho}{2}  \norm{ \frac{1}{\rho} (y^{(t+1)} - y^{(t  )}) } \\
  &amp;= \frac{1}{2\rho} \norm{                 y^{(t  )} - y^{(t+1)}  }
\end{align*}
$$</mathjax></p>
<p>We can substitute that into the right hand side of the preceding equation to
cancel out a couple terms,</p>
<p><mathjax>$$
\begin{align*}
  = &amp;
  \frac{\rho}{2} \left( \begin{split}
    &amp; + \norm{Ax + Bz^{(t)} - c}_2^2 &amp;             - \norm{Ax         + Bz^{(t+1)} - c}_2^2 \\
    &amp;                                &amp; \underbrace{- \norm{Ax^{(t+1)} + Bz^{(t  )} - c}_2^2}_{ \text{ always $\le 0$ } }
  \end{split} \right) \\
  &amp; + \frac{1}{2\rho} \left( \begin{split}
    &amp; + \norm{y - y^{(t  )}}_2^2 \\
    &amp; - \norm{y - y^{(t+1)}}_2^2
  \end{split} \right)
\end{align*}
$$</mathjax></p>
<p>Finally dropping the portion of the equation that's always non-positive
(doing so doesn't affect the validity of the inequality), we obtain a concise
inequality in terms of sums of <mathjax>$\norm{\cdot}_2^2$</mathjax>.</p>
<p><mathjax>$$
\begin{align*}
  &amp; h(w^{(t+1)}) - h(w) + \langle
    F(w^{(t+1)}),
    w^{(t+1)} - w
  \rangle  \\
  &amp; \qquad \le \frac{\rho}{2} \left(
      \norm{Ax + Bz^{(t  )} - c}_2^2
    - \norm{Ax + Bz^{(t+1)} - c}_2^2
  \right) \\
  &amp; \qquad + \frac{1}{2\rho} \left(
      \norm{y - y^{(t  )}}_2^2
    - \norm{y - y^{(t+1)}}_2^2
  \right)
\end{align*}
$$</mathjax></p>
<p><strong>Step 4</strong> Averaging across iterations. We're now in the home stretch. In this
step, we'll sum the previous equation across <mathjax>$t$</mathjax>. The sum will "telescope",
crossing out terms until we're left only with the initial and final conditions.
A quick application of Jensen's inequality will get us the desired result.</p>
<p>We begin by summing the previous equation across iterations,</p>
<p><mathjax>$$
\begin{align*}
  &amp; \sum_{\tau=0}^{t-1} h(w^{(\tau+1)}) - h(w) + \langle
    F(w^{(\tau+1)}),
    w^{(\tau+1)} - w
  \rangle  \\
  &amp; \qquad \le \frac{\rho}{2} \left(
                  \norm{Ax + Bz^{(0)} - c}_2^2
    \underbrace{- \norm{Ax + Bz^{(t)} - c}_2^2}_{\le 0}
  \right) + \frac{1}{2\rho} \left(
                  \norm{y - y^{(0)}}_2^2
    \underbrace{- \norm{y - y^{(t)}}_2^2}_{\le 0}
  \right)
\end{align*}
$$</mathjax></p>
<p>For convenience, we'll choose <mathjax>$z^{(0)}$</mathjax> and <mathjax>$y^{(0)}$</mathjax> equal to zero. We'll
also drop the terms <mathjax>$-\norm{Ax + Bz^{(t)} - c}_2^2$</mathjax> and
<mathjax>$-\norm{y - y^{(t)}}_2^2$</mathjax> from the expression, as both terms are always
non-positive. This gives us,</p>
<p><mathjax>$$
\begin{align*}
  \sum_{\tau=0}^{t-1} h(w^{(\tau+1)}) - h(w) + \langle
    F(w^{(\tau+1)}),
    w^{(\tau+1)} - w
  \rangle
  \le \frac{\rho}{2}  \norm{Ax - c}_2^2
             + \frac{1}{2\rho} \norm{ y    }_2^2
\end{align*}
$$</mathjax></p>
<p>Finally, recall that for a convex function <mathjax>$h(w)$</mathjax>, Jensen's Inequality states that</p>
<p><mathjax>$$
  h(\bar{w}_t)
  = h \left( \frac{1}{t} \sum_{\tau=1}^{t} w_{\tau} \right)
  \le \frac{1}{t} \sum_{\tau=1}^{t} h(w_{\tau})
$$</mathjax></p>
<p>The same is true for each of <mathjax>$F(w)$</mathjax>'s components (they're linear in <mathjax>$w$</mathjax>).
Thus, we can apply this statement to the left hand side of the preceding
equation after multiplying by <mathjax>$1/t$</mathjax> to obtain,</p>
<p><mathjax>$$
\begin{align*}
  h(\bar{w}^{(t)}) - h(w) + \langle
    F(\bar{w}^{(t)}),
    \bar{w}^{(t)} - w
  \rangle
  \le \frac{1}{t} \left(
      \frac{\rho}{2}  \norm{Ax - c}_2^2
    + \frac{1}{2\rho} \norm{ y    }_2^2
  \right)
\end{align*}
$$</mathjax></p>
<p>The right hand side decreases as <mathjax>$O(1/t)$</mathjax>, thus ADMM converges at a rate of
at least <mathjax>$O(1/\epsilon)$</mathjax> as desired.</p>
<h1><a name="usage" href="#usage">When should I use it?</a></h1>
<p>Similar to the proximal methods presented on this website, ADMM is only
efficient if we can perform each of its steps efficiently. Solving
2 optimization problems at each iteration may be very fast or very slow,
depending on if a closed form solution exists for <mathjax>$x^{(t+1)}$</mathjax> and <mathjax>$z^{(t+1)}$</mathjax>.</p>
<p>ADMM has been particularly useful in supervised machine learning, where <mathjax>$A$</mathjax>,
<mathjax>$B$</mathjax>, and <mathjax>$c$</mathjax> are chosen such that <mathjax>$x = z$</mathjax>. In this scenario, <mathjax>$f$</mathjax> is taken to be
the prediction loss on the training set, and <mathjax>$g$</mathjax> an appropriate regularizer,
typically a norm such as <mathjax>$\ell_1$</mathjax> or a <a href="http://arxiv.org/pdf/1104.1872.pdf">group sparsity norm</a>.
<a href="http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Martins_150.pdf">ADMM also lends</a> itself to inferring the most likely setting for
settings for latent variables in a factor graph. The primary benefit of ADMM in
both of these cases is not its rate of convergence but how <a href="http://www.ece.umn.edu/users/alfonso/pubs/jmlr2010.pdf">easily it
lends itself to distributed computation</a>. <a href="http://arxiv.org/pdf/1009.1128.pdf">Applications in
Compressed Sensing</a> see similar benefits.</p>
<p>All in all, ADMM is <em>not</em> a quick method, but it is a scalable one. ADMM is
best suited when data is too large to fit on a single machine or when
<mathjax>$x^{(t+1)}$</mathjax> and <mathjax>$z^{(t+1)}$</mathjax> can be solved for in closed form. While very
interesting in its own right, ADMM should rarely your algorithm of choice.</p>
<h1><a name="extensions" href="#extensions">Extensions</a></h1>
<p><strong>Accelerated</strong> As ADMM is so closely related to Proximal Gradient-based
methods, one might ask if there exists an accelerated variant with a better
convergence rate. The answer is a resounding yes, as shown by <a href="ftp://ftp.math.ucla.edu/pub/camreport/cam12-35.pdf">Goldstein et
al.</a>, though care must be taken for non-strongly convex
objectives. In their article, Goldstein et al. show that a convergence rate of
<mathjax>$O(1/\sqrt{\epsilon})$</mathjax> can be guaranteed if both <mathjax>$f$</mathjax> and <mathjax>$g$</mathjax> are strongly
convex. If this isn't the case, only a rate of <mathjax>$O(1/\epsilon)$</mathjax> is shown.</p>
<p><strong>Online</strong> In online learning, one is interested in solving a series of
supervised machine learning instances in sequence with minimal error. At each
iteration, the algorithm is presented with an input <mathjax>$x_t$</mathjax>, to which it responds
with a prediction <mathjax>$\hat{y}_t$</mathjax>. The world then presents the algorithm with the
correct answer <mathjax>$y_t$</mathjax>, and the algorithm suffers loss <mathjax>$l_t(y_t, \hat{y}_t)$</mathjax>. The
goal of the algorithm is to minimize the sum of errors <mathjax>$\sum_{t} l_t(y_t,
\hat{y}_t)$</mathjax>.</p>
<p>In this setting, <a href="http://icml.cc/2012/papers/577.pdf">Wang</a> has shown that an online variant to ADMM
can achieve regret competitive with the best possible (<mathjax>$O(\sqrt{T})$</mathjax> for
convex loss functions, <mathjax>$O(\log(T))$</mathjax> for strongly convex loss functions).</p>
<p><strong>Stochastic</strong> In a stochastic setting, one is interested in minimizing the
<em>average</em> value of <mathjax>$f(x)$</mathjax> via a series of samples. In <a href="http://arxiv.org/pdf/1211.0632.pdf">Ouyang et
al</a>, convergence rates for a linearized variant of ADMM when
<mathjax>$f$</mathjax> can only be accessed through samples.</p>
<p><strong>Multi Component</strong> Traditional ADMM considers an objective with only
2 components <mathjax>$f(x)$</mathjax> and <mathjax>$g(z)$</mathjax>. While applying the same logic to 3 or more is
straightforward, proving convergence for this scenario is more difficult. This
was the task taken by <a href="http://www.optimization-online.org/DB_FILE/2010/12/2871.pdf">He et al</a>. In particular, they showed that
a special variant of ADMM using "Gaussian back substitution" is ensured to
converge.</p>
<h1><a name="references" href="#references">References</a></h1>
<p><strong>ADMM</strong> While ADMM has existed for decades, it has only recently been brought
to light by <a href="http://stronglyconvex.com/blog/admm.html">Boyd</a>'s article describing its applications for statistical
machine learning. It is from this work from which I initially learned of ADMM.</p>
<p><strong>Proof of Convergence</strong> The proof of convergence presented here is a verbose
expansion of that presented in <a href="http://icml.cc/2012/papers/577.pdf">Wang</a>'s paper on Online ADMM.</p>
<!-- internal references -->

<!-- papers -->

<!-- convergence proofs -->

<!-- extensions -->

<!-- uses -->

<h1><a name="reference-impl" href="#reference-impl">Reference Implementation</a></h1>
<p>Using the <a href="https://github.com/duckworthd/optim"><code>optim</code></a> Python package, we can generate the animation above,</p>
<div class="highlight"><pre><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Example usage of ADMM solver.</span>

<span class="sd">A gif is generated showing the iterates as they converge.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">animation</span>
<span class="kn">from</span> <span class="nn">optim.admm</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">optim.tests.test_admm</span> <span class="kn">import</span> <span class="n">quadratic1</span>
<span class="kn">import</span> <span class="nn">itertools</span> <span class="kn">as</span> <span class="nn">it</span>
<span class="kn">import</span> <span class="nn">numpy</span>     <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pylab</span> <span class="kn">as</span> <span class="nn">pl</span>
<span class="kn">import</span> <span class="nn">sys</span>


<span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
  <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s">&quot;Usage: </span><span class="si">%s</span><span class="s"> OUTPUT</span><span class="se">\n</span><span class="s">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">0</span><span class="p">],))</span>
  <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
  <span class="n">output</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="n">prob</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">quadratic1</span><span class="p">()</span>
<span class="n">admm</span>        <span class="o">=</span> <span class="n">ADMM</span><span class="p">(</span><span class="n">rho</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">iterates</span>    <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">it</span><span class="o">.</span><span class="n">islice</span><span class="p">(</span><span class="n">admm</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">state</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>

<span class="n">pl</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">_</span>   <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">xs</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="n">s</span><span class="o">.</span><span class="n">x</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">iterates</span><span class="p">])</span>
<span class="n">zs</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="n">s</span><span class="o">.</span><span class="n">z</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">iterates</span><span class="p">])</span>
<span class="n">xs2</span> <span class="o">=</span> <span class="p">[</span><span class="n">prob</span><span class="o">.</span><span class="n">primal</span><span class="p">(</span><span class="n">State</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">v</span><span class="p">,</span><span class="n">z</span><span class="o">=</span><span class="n">v</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">xs</span><span class="p">]</span>
<span class="n">zs2</span> <span class="o">=</span> <span class="p">[</span><span class="n">prob</span><span class="o">.</span><span class="n">primal</span><span class="p">(</span><span class="n">State</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">v</span><span class="p">,</span><span class="n">z</span><span class="o">=</span><span class="n">v</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">zs</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">animate</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
  <span class="k">print</span> <span class="s">&#39;iteration:&#39;</span><span class="p">,</span> <span class="n">i</span>
  <span class="n">pl</span><span class="o">.</span><span class="n">cla</span><span class="p">()</span>
  <span class="n">pl</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">&quot;Iteration #</span><span class="si">%d</span><span class="s">&quot;</span> <span class="o">%</span> <span class="n">i</span><span class="p">)</span>
  <span class="n">pl</span><span class="o">.</span><span class="n">plot</span>   <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="p">[</span><span class="n">prob</span><span class="o">.</span><span class="n">f</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">+</span> <span class="n">prob</span><span class="o">.</span><span class="n">g</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">_</span><span class="p">],</span> <span class="s">&#39;k-&#39;</span> <span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&#39;f(x)+g(z)&#39;</span><span class="p">)</span>
  <span class="n">pl</span><span class="o">.</span><span class="n">plot</span>   <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="p">[</span><span class="n">prob</span><span class="o">.</span><span class="n">f</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>             <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">_</span><span class="p">],</span> <span class="s">&#39;g--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&#39;f(x)&#39;</span>     <span class="p">)</span>
  <span class="n">pl</span><span class="o">.</span><span class="n">plot</span>   <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="p">[</span>            <span class="n">prob</span><span class="o">.</span><span class="n">g</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">_</span><span class="p">],</span> <span class="s">&#39;b--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span>     <span class="s">&#39;g(z)&#39;</span><span class="p">)</span>
  <span class="n">pl</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">xs2</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s">&#39;g&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&#39;x&#39;</span><span class="p">)</span>
  <span class="n">pl</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">zs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">zs2</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s">&#39;b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&#39;z&#39;</span><span class="p">)</span>
  <span class="n">pl</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">_</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="n">_</span><span class="p">))</span>
  <span class="n">pl</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">anim</span> <span class="o">=</span> <span class="n">animation</span><span class="o">.</span><span class="n">FuncAnimation</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">gcf</span><span class="p">(),</span> <span class="n">animate</span><span class="p">,</span> <span class="n">frames</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">iterates</span><span class="p">))</span>
<span class="n">anim</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">writer</span><span class="o">=</span><span class="s">&#39;imagemagick&#39;</span><span class="p">,</span> <span class="n">fps</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>

    <div id="article_meta">
        Category:
          <a href="http://stronglyconvex.com/category/optimization.html">optimization</a>
        <br />Tags:
          <a href="http://stronglyconvex.com/tag/optimization.html">optimization</a>
,           <a href="http://stronglyconvex.com/tag/distributed.html">distributed</a>
,           <a href="http://stronglyconvex.com/tag/admm.html">admm</a>
    </div>
  </article>

  <footer>
    <a href="http://stronglyconvex.com/" class="button_accent">&larr;&nbsp;&nbsp;&nbsp;Back to blog</a>
  </footer>

  <div id="comments">
    <h2>Comments</h2>
    <div id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_identifier = "blog/admm-revisited.html";
      (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://duckworthd-blog.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>Please enable JavaScript to view <a href="http://disqus.com/?ref_noscript">comments</a>.</noscript>
  </div>

	</section>


  <!-- scripts -->
  <script
    type= "text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    >
  </script>
  <script
    type= "text/javascript"
    src="http://stronglyconvex.com/assets/js/MathJaxLocal.js"
    >
  </script>
  <script
      type="text/javascript"
      src="http://stronglyconvex.com/assets/lightbox/js/jquery-1.11.0.min.js"
      >
  </script>
  <script
      type="text/javascript"
      src="http://stronglyconvex.com/assets/lightbox/js/lightbox.min.js"
      >
  </script>
</body>
</html>