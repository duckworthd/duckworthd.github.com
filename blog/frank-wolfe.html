<!DOCTYPE html>
<html lang="en">
<head>
  <!-- stylesheets -->
	<link rel="stylesheet" type="text/css" href="/theme/css/style.css">
	<link rel="stylesheet" type="text/css" href="/theme/css/pygments.css">
	<link rel="stylesheet" type="text/css" href="/theme/lightbox/css/lightbox.css">
	<link rel="stylesheet" type="text/css" href="/theme/font-awesome-4.3.0/css/font-awesome.min.css">

  <!-- fonts -->
	<link href='http://fonts.googleapis.com/css?family=Open+Sans:800,400,300|Inconsolata' rel='stylesheet' type='text/css'>
	<link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:700,900' rel='stylesheet' type='text/css'>
	<link href='http://fonts.googleapis.com/css?family=Lora:700' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Playball' rel='stylesheet' type='text/css'>

  <!-- RSS/ATOM feeds -->
	<link href="None/" type="application/atom+xml" rel="alternate" title="Strongly Convex ATOM Feed" />


		<title>Strongly Convex</title>
		<meta charset="utf-8" />
</head>
<body>
	<section id="sidebar">
		<figure id="user_logo">
      <a href=""><div class="logo">&nbsp;</div></a>
		</figure>

		<div class="user_meta">
      <h1 id="user">
        <a  href="/"
            class="sitename">
          Strongly Convex
        </a>
      </h1>
			<h2></h2>


			<ul>
        <hr></hr>
					<li><a href="/">Words</a></li>
          <hr></hr>
					<li><a href="/code.html">Code</a></li>
          <hr></hr>
					<li><a href="/about.html">About</a></li>
          <hr></hr>
			</ul>
		</div>
	</section>

	<section id="posts">
	<header>
      <h1 class="title">
			  <a  href="/blog/frank-wolfe.html"
            rel="bookmark"
			  	  title="Permalink to Frank-Wolfe Algorithm">
          Frank-Wolfe Algorithm
        </a>
      </h1>
		  <abbr class="published">Sat 04 May 2013</abbr>
	</header>
  <article>
    <p>In this post, we'll take a look at the <a href="http://en.wikipedia.org/wiki/Frank%E2%80%93Wolfe_algorithm">Frank-Wolfe Algorithm</a>
also known as the Conditional Gradient Method, an algorithm particularly suited
for solving problems with compact domains. Like the <a href="/blog/proximal-gradient-descent.html">Proximal
Gradient</a> and <a href="/blog/accelerated-proximal-gradient-descent.html">Accelerated Proximal
Gradient</a> algorithms, Frank-Wolfe requires we
exploit problem structure to quickly solve a mini-optimization problem. Our
reward for doing so is a converge rate of $O(1/\epsilon)$ and the potential for
<em>extremely sparse solutions</em>.</p>
<p>Returning to my <a href="/blog/gradient-descent.html">valley-finding metaphor</a>, Frank-Wolfe is a
bit like this,</p>
<div class="pseudocode">
<ol>
<li>Look around you and see which way points the most downwards</li>
<li>Walk as far as possible in that direction until you hit a wall</li>
<li>Go back in the direction you started, stop part way along the path, then
     repeat.</li>
</ol>
</div>
<h1><a name="implementation" href="#implementation">How does it work?</a></h1>
<p>Frank-Wolfe is designed to solve problems of the form,</p>
<p>$$
  \min_{x \in D} f(x)
$$</p>
<p>where $D$ is compact and $f$ is differentiable. For example, in $R^n$ any
closed and bounded set is compact. The algorithm for Frank-Wolfe is then,</p>
<div class="pseudocode">
<p><strong>Input</strong>: Initial iterate $x^{(0)}$</p>
<ol>
<li>For $t = 0, 1, 2, \ldots$<ol>
<li>Let $s^{(t+1)} = \arg\min_{s \in D} \langle \nabla f(x^{(t)}), s \rangle$</li>
<li>If $g(x) = \langle \nabla f(x^{(t)}), x - s^{(t+1)} \rangle \le \epsilon$, break</li>
<li>Let $x^{(t+1)} = (1 - \alpha^{(t)}) x^{(t)} + \alpha^{(t)} s^{(t+1)}$</li>
</ol>
</li>
</ol>
</div>
<p>The proof relies on $\alpha^{(t)} = 2 / (t+2)$, but line search works as
well.  The intuition for the algorithm is that at each iteration, we minimize
a linear approximation to $f$,</p>
<p>$$
  s^{(t+1)} = \arg\min_{s \in D} f(x^{(t)}) + \nabla f(x^{(t)})^T (s - x^{(t)})
$$</p>
<p>then take a step in that direction. We can immediately see that if $D$
weren't compact, $s^{(t)}$ would go off to infinity.</p>
<p><a id="upper_bound"></a>
  <strong>Upper Bound</strong> One nice property of Frank-Wolfe is that it comes with its
own upper bound on $f(x^{(t)}) - f(x^{*})$ calculated during the course of
the algorithm. Recall the linear upper bound on $f$ due to convexity,</p>
<p>$$
\begin{align<em>}
  f(x^{</em>})
  &amp; \ge f(x) + \nabla f(x)^T (x^{<em>} - x) \
  f(x) - f(x^{</em>})
  &amp; \le \nabla f(x)^T (x - x^{<em>}) \
\end{align</em>}
$$</p>
<p>Since,</p>
<p>$$
  s^{(t+1)}
  = \arg\min_{s} \nabla f(x^{(t)})^T s
  = \arg\max_{s} \nabla f(x^{(t)})^T (x^{(t)} - s)
$$
  we know that $\nabla f(x^{(t)})^T (x^{(t)} - x^{*}) \le \nabla f(x^{(t)})^T
(x^{(t)} - s^{(t+1)})$ and thus,</p>
<p>$$
  f(x) - f(x^{*}) \le \nabla f(x^{(t)})^T (x^{(t)} - s^{(t+1)})
$$</p>
<p><a id="example"></a></p>
<h1><a name="example" href="#example">A Small Example</a></h1>
<p>For this example, we'll minimize a simple univariate quadratic function
constrained to lie in an interval,</p>
<p>$$
  \min_{x \in [-1,2]} (x-0.5)^2 + 2x
$$</p>
<p>Its derivative is given by $2(x-0.5) + 2$, and since we are dealing with real
numbers, the minimizers of the linear approximation must be either $-1$ or
$2$ if the gradient is positive or negative, respectively. We'll use a stepsize
of $\alpha^{(t)} = 2 / (t+2)$ as prescribed by the convergence proof in the
next section.</p>
<div class="img-center">
  <img src="/assets/img/frank_wolfe/animation.gif"></img>
  <span class="caption">
    Frank-Wolfe in action. The red circle is the current value for
    $f(x^{(t)})$, and the green diamond is $f(x^{(t+1)})$. The dotted line is
    the linear approximation to $f$ at $x^{(t)}$. Notice that at each step,
    Frank-Wolfe stays closer and closer to $x^{(t)}$ when moving in the
    direction of $s^{(t+1)}$.
  </span>
</div>

<div class="img-center">
  <img src="/assets/img/frank_wolfe/convergence.png"></img>
  <span class="caption">
    This plot shows how quickly the objective function decreases as the
    number of iterations increases. Notice that it does not monotonically
    decrease, as with Gradient Descent.
  </span>
</div>

<div class="img-center">
  <img src="/assets/img/frank_wolfe/iterates.png"></img>
  <span class="caption">
    This plot shows the actual iterates and the objective function evaluated at
    those points. More red indicates a higher iteration number. Since
    Frank-Wolfe uses linear combinations of $s^{(t+1)}$ and $x^{(t)}$, it
    tends to "bounce around" a lot, especially in earlier iterations.
  </span>
</div>

<p><a id="proof"></a></p>
<h1><a name="proof" href="#proof">Why does it work?</a></h1>
<p>We begin by making the two assumptions given earlier,</p>
<ol>
<li>$f$ is convex, differentiable, and finite for all $x \in D$</li>
<li>$D$ is compact</li>
</ol>
<p><strong>Assumptions</strong> First, notice that we never needed to assume that a solution
$x^{*}$ exists. This is because $D$ is compact and $f$ is finite, meaning $x$
cannot get bigger and bigger to make $f(x)$ arbitrarily small.</p>
<p>Secondly, we never made a Lipschitz assumption on $f$ or its gradient. Since
$D$ is compact, we don't have to -- instead, we get the following for free.
Define $C_f$ as,</p>
<p>$$
  C_f = \max_{\substack{
                x,s \in D \
                \alpha \in [0,1] \
                y = x + \alpha (s-x)
              }}
          \frac{2}{\alpha^2} \left(
            f(y) - f(x) - \langle \nabla f(x), y - x \rangle
          \right)
$$</p>
<p>This immediate implies the following upper bound on $f$ for all $x, y \in
D$ and $\alpha \in [0,1]$,</p>
<p>$$
  f(y) \le f(x) + \langle \nabla f(x), y-x \rangle + \frac{\alpha^2}{2} C_f
$$</p>
<p><strong>Proof Outline</strong> The proof for Frank-Wolfe is surprisingly simple. The idea
is to first upper bound $f(x^{(t+1)})$ in terms of $f(x^{(t)})$, $g(x^{(t)})$,
and $C_f$. We then transform this per-iteration bound into a bound on
$f(x^{(t)}) - f(x^{*})$ depending on $t$ using induction. That's it!</p>
<p><strong>Step 1</strong> Upper bound $f(x^{(t+1)})$. As usual, we'll denote $x^{+} \triangleq
x^{(t+1)}$, $x \triangleq x^{(t)}$, $s^{+} \triangleq s^{(t+1)}$, and $\alpha
\triangleq \alpha^{(t)}$. We begin by using the upper bound we just obtained for
$f$ in terms of $C_f$, substituting $x^{+} = (1 - \alpha) x + \alpha s^{+}$ and
then $g(x) = \nabla f(x)^T (x - s^{+})$,</p>
<p>$$
\begin{align<em>}
  f(x^{+}) 
  &amp; \le f(x) + \nabla f(x)^T (x^{+} - x) + \frac{\alpha^2}{2} C_f \
  &amp; = f(x) + \nabla f(x)^T ( (1-\alpha) x + \alpha s^{+} - x ) + \frac{\alpha^2}{2} C_f \
  &amp; = f(x) + \nabla f(x)^T ( \alpha s^{+} - \alpha x ) + \frac{\alpha^2}{2} C_f \
  &amp; = f(x) - \alpha \nabla f(x)^T ( x - s^{+} ) + \frac{\alpha^2}{2} C_f \
  &amp; = f(x) - \alpha g(x) + \frac{\alpha^2}{2} C_f \
\end{align</em>}
$$</p>
<p><strong>Step 2</strong> Use induction on $t$. First, recall the upper bound on $f(x) -
f(x^{<em>}) \le g(x)$ <a href="#upper_bound">we derived above</a>. Let's add $-f(x^{</em>})$ into
what we got from Step 1, then use the upper bound on $f(x) - f(x^{*})$ to get,</p>
<p>$$
\begin{align<em>}
  f(x^{+}) - f(x^{</em>})
  &amp; \le f(x) - f(x^{<em>}) - \alpha g(x) + \frac{\alpha^2}{2} C_f \
  &amp; \le f(x) - f(x^{</em>}) - \alpha ( f(x) - f(x^{<em>}) ) + \frac{\alpha^2}{2} C_f \
  &amp; = (1 - \alpha) (f(x) - f(x^{</em>})) + \frac{\alpha^2}{2} C_f \
\end{align*}
$$</p>
<p>Now, we employ induction on $t$ to show that,</p>
<p>$$
  f(x^{(t)}) - f(x^{*}) \le \frac{4 C_f / 2}{t+2}
$$</p>
<p>We'll assume that the step size is $\alpha^{(t)} = \frac{2}{t+2}$, giving us
$\alpha^{(0)} = 2 / (0+2) = 1$ and the base case,</p>
<p>$$
\begin{align<em>}
  f(x^{(1)} - f(x^{</em>})
  &amp; \le (1 - \alpha^{(0)}) ( f(x^{(0)}) - f(x^{<em>}) ) + \frac{\alpha^2}{2} C_f \
  &amp; = (1 - 1) ( f(x^{(0)}) - f(x^{</em>}) ) + \frac{1}{2} C_f \
  &amp; \le \frac{4 C_f / 2}{(0 + 1) + 2}
\end{align*}
$$</p>
<p>Next, for the recursive case, we use the inductive assumption on $f(x) - f(x^{*})$, the definition of $\alpha^{(t)}$, and some algebra,</p>
<p>$$
\begin{align<em>}
  f(x^{+}) - f(x^{</em>})
  &amp; \le (1 - \alpha) ( f(x) - f(x^{<em>}) ) + \frac{ \alpha^2}{2} C_f \
  &amp; \le \left(1 - \frac{2}{t+2} \right) \frac{4 C_f / 2}{t + 2} + \left( \frac{2}{t+2} \right)^2 C_f / 2 \
  &amp; \le \frac{4 C_f / 2}{t + 2} \left( 1 - \frac{2}{t+2} + \frac{1}{t+2} \right) \
  &amp; = \frac{4 C_f / 2}{t + 2} \left( \frac{t+1}{t+2} \right) \
  &amp; \le \frac{4 C_f / 2}{t + 2} \left( \frac{t+2}{t+3} \right) \
  &amp; = \frac{4 C_f / 2}{(t + 1) + 2} \
\end{align</em>}
$$</p>
<p>Thus, if we want an error tolerance of $\epsilon$, we need
$O(\frac{1}{\epsilon})$ iterations to find it. This matches the convergence
rate of Gradient Descent an Proximal Gradient Descent, but falls short of their
accelerated brethren.</p>
<h1><a name="usage" href="#usage">When should I use it?</a></h1>
<p>Like Proximal Gradient, efficient use of Frank-Wolfe requires solving a
mini-optimization problem at each iteration. Unlike Proximal Gradient, however,
this mini-problem will lead to unbounded iterates if the input space is not
compact -- in other words, Frank-Wolfe cannot directly be applied when your
domain is all of $R^{n}$. However, there is a very special case wherein
Frank-Wolfe shines.</p>
<p><a id="sparsity"></a>
  <strong>Sparsity</strong> The primary reason machine learning researchers have recently
taken an interest in Frank-Wolfe is because in certain problems the iterates
$x^{(t)}$ will be extremely sparse.  Suppose that $D$ is a polyhedron defined
by a set of linear constraints. Then $s^{(t)}$ is a solution to a Linear
Program, meaning that each $s^{(t)}$ lies on one of the vertices of the
polyhedron. If these vertices have only a few non-zero entries, then $x^{(t)}$
will too, as $x^{(t)}$ is a linear combination of $s^{(1)} \ldots s^{(t)}$.
This is in direct contrast to gradient and proximal based methods, wherein
$x^{(t)}$ is the linear combination of a set of non-sparse <em>gradients</em>.</p>
<p><strong>Atomic Norms</strong> One particular case where Frank-Wolfe shines is when
minimizing $f(x)$ subject to $||x|| \le c$ where $|| \cdot ||$ is an "atomic
norm". We say that $||\cdot||$ is an atomic norm if $||x||$ is the smallest $t$
such that $x/t$ is in the convex hull of a finite set of points $\mathcal{A}$,
that is,</p>
<p>$$
  ||x|| = \inf { t : x \in t \, \text{Conv}(\mathcal{A}) }
$$</p>
<p>For example, $||x||<em _le="\le" _s_="||s||" c>1$ is an atomic norm with $\mathcal{A}$ being the set of
all vectors with only one $+1$ or one $-1$ entry. In these cases, finding
$\arg\min</em> \langle \nabla f(x), s \rangle$ is tantamount to
finding which element of $\mathcal{A}$ minimizes $\langle \nabla f(x), s
\rangle$ (since $\text{Conv}(\mathcal{A})$ defines a polyhedron). For a whole
lot more on Atomic Norms, see <a href="http://pages.cs.wisc.edu/~brecht/papers/2010-crpw_inverse_problems.pdf">this tome</a> by
Chandrasekaranm et al.</p>
<h1><a name="extensions" href="#extensions">Extensions</a></h1>
<p><strong>Step Size</strong> The proof above relied on a step size of $\alpha^{(t)} =
\frac{2}{t+2}$, but as usual <a href="/blog/gradient-descent.html#line_search">Line Search</a> can be applied to
accelerate convergence.</p>
<p><strong>Approximate Linear Solutions</strong> Though not stated in the proof above,
another cool point about Frank-Wolfe is that you don't actually need to solve
the linear mini-problem exactly, but you will still converge to the optimal
solution (albet at a slightly slower rate). In particular, assume that each
mini-problem can be solved approximately with additive error $\frac{\delta
C_f}{t+2}$ at iteration $t$,</p>
<p>$$
  \langle s^{(t+1)}, \nabla f(x^{(t)}) \rangle
  \le \min_{s} \langle s, \nabla f(x^{(t)}) \rangle + \frac{\delta C_f}{t+2}
$$</p>
<p>then Frank-Wolfe's rate of convergence is</p>
<p>$$
  f(x^{(t)}) - f(x^{*}) \le \frac{2 C_f}{t+2} (1 + \delta)
$$</p>
<p>The proof for this can be found in the supplement to <a href="http://jmlr.csail.mit.edu/proceedings/papers/v28/jaggi13-supp.pdf">Jaggi's</a>
excellent survey on Frank-Wolfe for machine learning.</p>
<h1><a name="invariance" href="#invariance">Linear Invariance</a></h1>
<p>Another cool fact about Frank-Wolfe is that it's <em>linearly invariant</em> -- that
is, if you rotate and scale the space, nothing changes about the convergence
rate. This is in direct contrast to many other methods which depend on the
<a href="http://en.wikipedia.org/wiki/Condition_number">condition number</a> of a function (for functions with
Hessians, this is the ratio between the largest and smallest eigenvalues,
$\sigma_{\max} / \sigma_{\min})$.</p>
<p>Suppose we transform our input space with a surjective (that is, onto) linear
transformation $M: \hat{D} \rightarrow D$. Let's now try to solve the problem,</p>
<p>$$
  \min_{\hat{x} \in \hat{D}} \hat{f}(\hat{x}) = f(M \hat{x}) = f(x)
$$</p>
<p>Let's look at the solution to the per-iteration mini-problem we need to solve
for Frank-Wolfe,</p>
<p>$$
\begin{align<em>}
  \min_{\hat{s} \in \hat{D}} \langle \nabla \hat{f}(\hat{x}), \hat{s} \rangle
  = \min_{\hat{s} \in \hat{D}} \langle M^T \nabla f( M \hat{x}), \hat{s} \rangle
  = \min_{\hat{s} \in \hat{D}} \langle \nabla f( x ), M \hat{s} \rangle
  = \min_{s \in D} \langle \nabla f( x ), s \rangle
\end{align</em>}
$$</p>
<p>In other words, we will find the same $s$ if we solve in the original space,
or if we find $\hat{s}$ and then map it back to $s$. No matter how $M$ warps
the space, Frank-Wolfe will do the same thing. This also means that if there's
a linear transformation you can do to make the points of your polyhedron
sparse, you can do it with no penalty!</p>
<h1><a name="references" href="#references">References</a></h1>
<p><strong>Proof of Convergence, Linear Invariance</strong> Pretty much everything in this
article comes from <a href="http://jmlr.csail.mit.edu/proceedings/papers/v28/jaggi13-supp.pdf">Jaggi's</a> fantastic article on Frank-Wolfe for
machine learning.</p>
<h1><a name="reference-impl" href="#reference-impl">Reference Implementation</a></h1>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">frank_wolfe</span><span class="p">(</span><span class="n">minisolver</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Frank-Wolfe Algorithm</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  minisolver : function</span>
<span class="sd">      minisolver(x) = argmin_{s \in D} &lt;x, s&gt;</span>
<span class="sd">  gradient : function</span>
<span class="sd">      gradient(x) = gradient[f](x)</span>
<span class="sd">  alpha : function</span>
<span class="sd">      learning rate</span>
<span class="sd">  x0 : array</span>
<span class="sd">      initial value for x</span>
<span class="sd">  epsilon : float</span>
<span class="sd">      desired accuracy</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">xs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x0</span><span class="p">]</span>
  <span class="n">iteration</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">xs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">s_next</span> <span class="o">=</span> <span class="n">minisolver</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">g</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">s_next</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">epsilon</span><span class="p">:</span>
      <span class="k">break</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">alpha</span><span class="p">(</span><span class="n">iteration</span><span class="o">=</span><span class="n">iteration</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="n">s_next</span><span class="p">)</span>
    <span class="n">x_next</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">a</span> <span class="o">*</span> <span class="n">s_next</span>
    <span class="n">xs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_next</span><span class="p">)</span>
    <span class="n">iteration</span> <span class="o">+=</span> <span class="mi">1</span>
  <span class="k">return</span> <span class="n">xs</span>


<span class="k">def</span> <span class="nf">default_learning_rate</span><span class="p">(</span><span class="n">iteration</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="k">return</span> <span class="mf">2.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">iteration</span> <span class="o">+</span> <span class="mf">2.0</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
  <span class="kn">import</span> <span class="nn">os</span>

  <span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
  <span class="kn">import</span> <span class="nn">pylab</span> <span class="kn">as</span> <span class="nn">pl</span>
  <span class="kn">import</span> <span class="nn">yannopt.plotting</span> <span class="kn">as</span> <span class="nn">plotting</span>

  <span class="c1">### FRANK WOLFE ALGORITHM ###</span>

  <span class="c1"># problem definition</span>
  <span class="n">function</span>    <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span>
  <span class="n">gradient</span>    <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span>
  <span class="n">minisolver</span>  <span class="o">=</span> <span class="k">lambda</span> <span class="n">y</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="n">y</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">2</span> <span class="c1"># D = [-1, 2]</span>
  <span class="n">x0</span> <span class="o">=</span> <span class="mf">1.0</span>

  <span class="c1"># run gradient descent</span>
  <span class="n">iterates</span> <span class="o">=</span> <span class="n">frank_wolfe</span><span class="p">(</span><span class="n">minisolver</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">default_learning_rate</span><span class="p">,</span> <span class="n">x0</span><span class="p">)</span>

  <span class="c1">### PLOTTING ###</span>

  <span class="n">plotting</span><span class="o">.</span><span class="n">plot_iterates_vs_function</span><span class="p">(</span><span class="n">iterates</span><span class="p">,</span> <span class="n">function</span><span class="p">,</span>
                                     <span class="n">path</span><span class="o">=</span><span class="s1">&#39;figures/iterates.png&#39;</span><span class="p">,</span> <span class="n">y_star</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
  <span class="n">plotting</span><span class="o">.</span><span class="n">plot_iteration_vs_function</span><span class="p">(</span><span class="n">iterates</span><span class="p">,</span> <span class="n">function</span><span class="p">,</span>
                                      <span class="n">path</span><span class="o">=</span><span class="s1">&#39;figures/convergence.png&#39;</span><span class="p">,</span> <span class="n">y_star</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>

  <span class="c1"># make animation</span>
  <span class="n">iterates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">iterates</span><span class="p">)</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="s1">&#39;figures/animation&#39;</span><span class="p">)</span>
  <span class="k">except</span> <span class="ne">OSError</span><span class="p">:</span>
    <span class="k">pass</span>

  <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">iterates</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">iterates</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
    <span class="n">x_plus</span> <span class="o">=</span> <span class="n">iterates</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">s_plus</span> <span class="o">=</span> <span class="n">minisolver</span><span class="p">(</span><span class="n">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="n">f</span> <span class="o">=</span> <span class="n">function</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">gradient</span>
    <span class="n">f_hat</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">y</span><span class="p">:</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">g</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span>

    <span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span> <span class="o">=</span> <span class="n">plotting</span><span class="o">.</span><span class="n">limits</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
    <span class="n">ymin</span><span class="p">,</span> <span class="n">ymax</span> <span class="o">=</span> <span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span>

    <span class="n">pl</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xmin</span> <span class="p">,</span><span class="n">xmax</span><span class="p">),</span> <span class="n">function</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="p">)),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="p">])</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="n">ymin</span><span class="p">,</span> <span class="n">ymax</span><span class="p">])</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;f(x)&#39;</span><span class="p">)</span>

    <span class="n">pl</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="p">],</span> <span class="p">[</span><span class="n">f_hat</span><span class="p">(</span><span class="n">xmin</span><span class="p">),</span> <span class="n">f_hat</span><span class="p">(</span><span class="n">xmax</span><span class="p">)],</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">vlines</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">ymin</span><span class="p">,</span> <span class="n">ymax</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span> <span class="o">*</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;solid&#39;</span><span class="p">)</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_plus</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">x_plus</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;D&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">x_plus</span><span class="p">,</span> <span class="n">f_hat</span><span class="p">(</span><span class="n">x_plus</span><span class="p">),</span> <span class="n">f</span><span class="p">(</span><span class="n">x_plus</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.8</span><span class="p">,</span><span class="mf">0.0</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dotted&#39;</span><span class="p">)</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">s_plus</span><span class="p">,</span> <span class="n">f_hat</span><span class="p">(</span><span class="n">s_plus</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="mf">0.35</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>

    <span class="n">pl</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;figures/animation/</span><span class="si">%02d</span><span class="s1">.png&#39;</span> <span class="o">%</span> <span class="n">t</span><span class="p">)</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>

    <div id="article_meta">
        Category:
          <a href="/category/optimization.html">optimization</a>
        <br />Tags:
          <a href="/tag/optimization.html">optimization</a>
,           <a href="/tag/first-order.html">first-order</a>
,           <a href="/tag/sparsity.html">sparsity</a>
    </div>
  </article>

  <footer>
    <a href="/" class="button_accent">&larr;&nbsp;&nbsp;&nbsp;Back to blog</a>
  </footer>

  <div id="comments">
    <h2>Comments</h2>
    <div id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_identifier = "blog/frank-wolfe.html";
      (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://duckworthd-blog.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>Please enable JavaScript to view <a href="http://disqus.com/?ref_noscript">comments</a>.</noscript>
  </div>

	</section>


  <!-- scripts -->
  <script
    type= "text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    >
  </script>
  <script
    type= "text/javascript"
    src="/theme/mathjax/MathJaxLocal.js"
    >
  </script>
  <script
      type="text/javascript"
      src="/theme/lightbox/js/jquery-1.11.0.min.js"
      >
  </script>
  <script
      type="text/javascript"
      src="/theme/lightbox/js/lightbox.min.js"
      >
  </script>
</body>
</html>