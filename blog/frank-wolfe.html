<!DOCTYPE html>
<html lang="en">
<head>
  <!-- stylesheets -->
	<link rel="stylesheet" type="text/css" href="/theme/css/style.css">
	<link rel="stylesheet" type="text/css" href="/theme/css/pygments.css">
	<link rel="stylesheet" type="text/css" href="/theme/lightbox/css/lightbox.css">
	<link rel="stylesheet" type="text/css" href="/theme/font-awesome-4.3.0/css/font-awesome.min.css">

  <!-- fonts -->
	<link href='http://fonts.googleapis.com/css?family=Open+Sans:800,400,300|Inconsolata' rel='stylesheet' type='text/css'>
	<link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:700,900' rel='stylesheet' type='text/css'>
	<link href='http://fonts.googleapis.com/css?family=Lora:700' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Playball' rel='stylesheet' type='text/css'>

  <!-- RSS/ATOM feeds -->
	<link href="None/" type="application/atom+xml" rel="alternate" title="Strongly Convex ATOM Feed" />


		<title>Strongly Convex</title>
		<meta charset="utf-8" />
</head>
<body>
	<section id="sidebar">
		<figure id="user_logo">
      <a href=""><div class="logo">&nbsp;</div></a>
		</figure>

		<div class="user_meta">
      <h1 id="user">
        <a  href="/"
            class="sitename">
          Strongly Convex
        </a>
      </h1>
			<h2></h2>


			<ul>
        <hr></hr>
					<li><a href="/">Words</a></li>
          <hr></hr>
					<li><a href="/code.html">Code</a></li>
          <hr></hr>
					<li><a href="/about.html">About</a></li>
          <hr></hr>
			</ul>
		</div>
	</section>

	<section id="posts">
	<header>
      <h1 class="title">
			  <a  href="/blog/frank-wolfe.html"
            rel="bookmark"
			  	  title="Permalink to Frank-Wolfe Algorithm">
          Frank-Wolfe Algorithm
        </a>
      </h1>
		  <abbr class="published">Sat 04 May 2013</abbr>
	</header>
  <article>
    <p>In this post, we'll take a look at the <a href="http://en.wikipedia.org/wiki/Frank%E2%80%93Wolfe_algorithm">Frank-Wolfe Algorithm</a>
also known as the Conditional Gradient Method, an algorithm particularly suited
for solving problems with compact domains. Like the <a href="/blog/proximal-gradient-descent.html">Proximal
Gradient</a> and <a href="/blog/accelerated-proximal-gradient-descent.html">Accelerated Proximal
Gradient</a> algorithms, Frank-Wolfe requires we
exploit problem structure to quickly solve a mini-optimization problem. Our
reward for doing so is a converge rate of <span class="math">\(O(1/\epsilon)\)</span> and the potential for
<em>extremely sparse solutions</em>.</p>
<p>Returning to my <a href="/blog/gradient-descent.html">valley-finding metaphor</a>, Frank-Wolfe is a
bit like this,</p>
<div class="pseudocode">
<ol>
<li>Look around you and see which way points the most downwards</li>
<li>Walk as far as possible in that direction until you hit a wall</li>
<li>Go back in the direction you started, stop part way along the path, then
     repeat.</li>
</ol>
</div>
<h1><a name="implementation" href="#implementation">How does it work?</a></h1>
<p>Frank-Wolfe is designed to solve problems of the form,</p>
<div class="math">$$
  \min_{x \in D} f(x)
$$</div>
<p>where <span class="math">\(D\)</span> is compact and <span class="math">\(f\)</span> is differentiable. For example, in <span class="math">\(R^n\)</span> any
closed and bounded set is compact. The algorithm for Frank-Wolfe is then,</p>
<div class="pseudocode">
<p><strong>Input</strong>: Initial iterate <span class="math">\(x^{(0)}\)</span></p>
<ol>
<li>For <span class="math">\(t = 0, 1, 2, \ldots\)</span><ol>
<li>Let <span class="math">\(s^{(t+1)} = \arg\min_{s \in D} \langle \nabla f(x^{(t)}), s \rangle\)</span></li>
<li>If <span class="math">\(g(x) = \langle \nabla f(x^{(t)}), x - s^{(t+1)} \rangle \le \epsilon\)</span>, break</li>
<li>Let <span class="math">\(x^{(t+1)} = (1 - \alpha^{(t)}) x^{(t)} + \alpha^{(t)} s^{(t+1)}\)</span></li>
</ol>
</li>
</ol>
</div>
<p>The proof relies on <span class="math">\(\alpha^{(t)} = 2 / (t+2)\)</span>, but line search works as
well.  The intuition for the algorithm is that at each iteration, we minimize
a linear approximation to <span class="math">\(f\)</span>,</p>
<div class="math">$$
  s^{(t+1)} = \arg\min_{s \in D} f(x^{(t)}) + \nabla f(x^{(t)})^T (s - x^{(t)})
$$</div>
<p>then take a step in that direction. We can immediately see that if <span class="math">\(D\)</span>
weren't compact, <span class="math">\(s^{(t)}\)</span> would go off to infinity.</p>
<p><a id="upper_bound"></a>
  <strong>Upper Bound</strong> One nice property of Frank-Wolfe is that it comes with its
own upper bound on <span class="math">\(f(x^{(t)}) - f(x^{*})\)</span> calculated during the course of
the algorithm. Recall the linear upper bound on <span class="math">\(f\)</span> due to convexity,</p>
<div class="math">$$
\begin{align*}
  f(x^{*})
  &amp; \ge f(x) + \nabla f(x)^T (x^{*} - x) \\
  f(x) - f(x^{*})
  &amp; \le \nabla f(x)^T (x - x^{*}) \\
\end{align*}
$$</div>
<p>Since,</p>
<div class="math">$$
  s^{(t+1)}
  = \arg\min_{s} \nabla f(x^{(t)})^T s
  = \arg\max_{s} \nabla f(x^{(t)})^T (x^{(t)} - s)
$$</div>
<p>
  we know that <span class="math">\(\nabla f(x^{(t)})^T (x^{(t)} - x^{*}) \le \nabla f(x^{(t)})^T
(x^{(t)} - s^{(t+1)})\)</span> and thus,</p>
<div class="math">$$
  f(x) - f(x^{*}) \le \nabla f(x^{(t)})^T (x^{(t)} - s^{(t+1)})
$$</div>
<p><a id="example"></a></p>
<h1><a name="example" href="#example">A Small Example</a></h1>
<p>For this example, we'll minimize a simple univariate quadratic function
constrained to lie in an interval,</p>
<div class="math">$$
  \min_{x \in [-1,2]} (x-0.5)^2 + 2x
$$</div>
<p>Its derivative is given by <span class="math">\(2(x-0.5) + 2\)</span>, and since we are dealing with real
numbers, the minimizers of the linear approximation must be either <span class="math">\(-1\)</span> or
<span class="math">\(2\)</span> if the gradient is positive or negative, respectively. We'll use a stepsize
of <span class="math">\(\alpha^{(t)} = 2 / (t+2)\)</span> as prescribed by the convergence proof in the
next section.</p>
<div class="img-center">
  <img src="/assets/img/frank_wolfe/animation.gif"></img>
  <span class="caption">
    Frank-Wolfe in action. The red circle is the current value for
    $f(x^{(t)})$, and the green diamond is $f(x^{(t+1)})$. The dotted line is
    the linear approximation to $f$ at $x^{(t)}$. Notice that at each step,
    Frank-Wolfe stays closer and closer to $x^{(t)}$ when moving in the
    direction of $s^{(t+1)}$.
  </span>
</div>

<div class="img-center">
  <img src="/assets/img/frank_wolfe/convergence.png"></img>
  <span class="caption">
    This plot shows how quickly the objective function decreases as the
    number of iterations increases. Notice that it does not monotonically
    decrease, as with Gradient Descent.
  </span>
</div>

<div class="img-center">
  <img src="/assets/img/frank_wolfe/iterates.png"></img>
  <span class="caption">
    This plot shows the actual iterates and the objective function evaluated at
    those points. More red indicates a higher iteration number. Since
    Frank-Wolfe uses linear combinations of $s^{(t+1)}$ and $x^{(t)}$, it
    tends to "bounce around" a lot, especially in earlier iterations.
  </span>
</div>

<p><a id="proof"></a></p>
<h1><a name="proof" href="#proof">Why does it work?</a></h1>
<p>We begin by making the two assumptions given earlier,</p>
<ol>
<li><span class="math">\(f\)</span> is convex, differentiable, and finite for all <span class="math">\(x \in D\)</span></li>
<li><span class="math">\(D\)</span> is compact</li>
</ol>
<p><strong>Assumptions</strong> First, notice that we never needed to assume that a solution
<span class="math">\(x^{*}\)</span> exists. This is because <span class="math">\(D\)</span> is compact and <span class="math">\(f\)</span> is finite, meaning <span class="math">\(x\)</span>
cannot get bigger and bigger to make <span class="math">\(f(x)\)</span> arbitrarily small.</p>
<p>Secondly, we never made a Lipschitz assumption on <span class="math">\(f\)</span> or its gradient. Since
<span class="math">\(D\)</span> is compact, we don't have to -- instead, we get the following for free.
Define <span class="math">\(C_f\)</span> as,</p>
<div class="math">$$
  C_f = \max_{\substack{
                x,s \in D \\
                \alpha \in [0,1] \\
                y = x + \alpha (s-x)
              }}
          \frac{2}{\alpha^2} \left(
            f(y) - f(x) - \langle \nabla f(x), y - x \rangle
          \right)
$$</div>
<p>This immediate implies the following upper bound on <span class="math">\(f\)</span> for all <span class="math">\(x, y \in
D\)</span> and <span class="math">\(\alpha \in [0,1]\)</span>,</p>
<div class="math">$$
  f(y) \le f(x) + \langle \nabla f(x), y-x \rangle + \frac{\alpha^2}{2} C_f
$$</div>
<p><strong>Proof Outline</strong> The proof for Frank-Wolfe is surprisingly simple. The idea
is to first upper bound <span class="math">\(f(x^{(t+1)})\)</span> in terms of <span class="math">\(f(x^{(t)})\)</span>, <span class="math">\(g(x^{(t)})\)</span>,
and <span class="math">\(C_f\)</span>. We then transform this per-iteration bound into a bound on
<span class="math">\(f(x^{(t)}) - f(x^{*})\)</span> depending on <span class="math">\(t\)</span> using induction. That's it!</p>
<p><strong>Step 1</strong> Upper bound <span class="math">\(f(x^{(t+1)})\)</span>. As usual, we'll denote <span class="math">\(x^{+} \triangleq
x^{(t+1)}\)</span>, <span class="math">\(x \triangleq x^{(t)}\)</span>, <span class="math">\(s^{+} \triangleq s^{(t+1)}\)</span>, and <span class="math">\(\alpha
\triangleq \alpha^{(t)}\)</span>. We begin by using the upper bound we just obtained for
<span class="math">\(f\)</span> in terms of <span class="math">\(C_f\)</span>, substituting <span class="math">\(x^{+} = (1 - \alpha) x + \alpha s^{+}\)</span> and
then <span class="math">\(g(x) = \nabla f(x)^T (x - s^{+})\)</span>,</p>
<div class="math">$$
\begin{align*}
  f(x^{+}) 
  &amp; \le f(x) + \nabla f(x)^T (x^{+} - x) + \frac{\alpha^2}{2} C_f \\
  &amp; = f(x) + \nabla f(x)^T ( (1-\alpha) x + \alpha s^{+} - x ) + \frac{\alpha^2}{2} C_f \\
  &amp; = f(x) + \nabla f(x)^T ( \alpha s^{+} - \alpha x ) + \frac{\alpha^2}{2} C_f \\
  &amp; = f(x) - \alpha \nabla f(x)^T ( x - s^{+} ) + \frac{\alpha^2}{2} C_f \\
  &amp; = f(x) - \alpha g(x) + \frac{\alpha^2}{2} C_f \\
\end{align*}
$$</div>
<p><strong>Step 2</strong> Use induction on <span class="math">\(t\)</span>. First, recall the upper bound on <span class="math">\(f(x) -
f(x^{*}) \le g(x)\)</span> <a href="#upper_bound">we derived above</a>. Let's add <span class="math">\(-f(x^{*})\)</span> into
what we got from Step 1, then use the upper bound on <span class="math">\(f(x) - f(x^{*})\)</span> to get,</p>
<div class="math">$$
\begin{align*}
  f(x^{+}) - f(x^{*})
  &amp; \le f(x) - f(x^{*}) - \alpha g(x) + \frac{\alpha^2}{2} C_f \\
  &amp; \le f(x) - f(x^{*}) - \alpha ( f(x) - f(x^{*}) ) + \frac{\alpha^2}{2} C_f \\
  &amp; = (1 - \alpha) (f(x) - f(x^{*})) + \frac{\alpha^2}{2} C_f \\
\end{align*}
$$</div>
<p>Now, we employ induction on <span class="math">\(t\)</span> to show that,</p>
<div class="math">$$
  f(x^{(t)}) - f(x^{*}) \le \frac{4 C_f / 2}{t+2}
$$</div>
<p>We'll assume that the step size is <span class="math">\(\alpha^{(t)} = \frac{2}{t+2}\)</span>, giving us
<span class="math">\(\alpha^{(0)} = 2 / (0+2) = 1\)</span> and the base case,</p>
<div class="math">$$
\begin{align*}
  f(x^{(1)} - f(x^{*})
  &amp; \le (1 - \alpha^{(0)}) ( f(x^{(0)}) - f(x^{*}) ) + \frac{\alpha^2}{2} C_f \\
  &amp; = (1 - 1) ( f(x^{(0)}) - f(x^{*}) ) + \frac{1}{2} C_f \\
  &amp; \le \frac{4 C_f / 2}{(0 + 1) + 2}
\end{align*}
$$</div>
<p>Next, for the recursive case, we use the inductive assumption on <span class="math">\(f(x) - f(x^{*})\)</span>, the definition of <span class="math">\(\alpha^{(t)}\)</span>, and some algebra,</p>
<div class="math">$$
\begin{align*}
  f(x^{+}) - f(x^{*})
  &amp; \le (1 - \alpha) ( f(x) - f(x^{*}) ) + \frac{ \alpha^2}{2} C_f \\
  &amp; \le \left(1 - \frac{2}{t+2} \right) \frac{4 C_f / 2}{t + 2} + \left( \frac{2}{t+2} \right)^2 C_f / 2 \\
  &amp; \le \frac{4 C_f / 2}{t + 2} \left( 1 - \frac{2}{t+2} + \frac{1}{t+2} \right) \\
  &amp; = \frac{4 C_f / 2}{t + 2} \left( \frac{t+1}{t+2} \right) \\
  &amp; \le \frac{4 C_f / 2}{t + 2} \left( \frac{t+2}{t+3} \right) \\
  &amp; = \frac{4 C_f / 2}{(t + 1) + 2} \\
\end{align*}
$$</div>
<p>Thus, if we want an error tolerance of <span class="math">\(\epsilon\)</span>, we need
<span class="math">\(O(\frac{1}{\epsilon})\)</span> iterations to find it. This matches the convergence
rate of Gradient Descent an Proximal Gradient Descent, but falls short of their
accelerated brethren.</p>
<h1><a name="usage" href="#usage">When should I use it?</a></h1>
<p>Like Proximal Gradient, efficient use of Frank-Wolfe requires solving a
mini-optimization problem at each iteration. Unlike Proximal Gradient, however,
this mini-problem will lead to unbounded iterates if the input space is not
compact -- in other words, Frank-Wolfe cannot directly be applied when your
domain is all of <span class="math">\(R^{n}\)</span>. However, there is a very special case wherein
Frank-Wolfe shines.</p>
<p><a id="sparsity"></a>
  <strong>Sparsity</strong> The primary reason machine learning researchers have recently
taken an interest in Frank-Wolfe is because in certain problems the iterates
<span class="math">\(x^{(t)}\)</span> will be extremely sparse.  Suppose that <span class="math">\(D\)</span> is a polyhedron defined
by a set of linear constraints. Then <span class="math">\(s^{(t)}\)</span> is a solution to a Linear
Program, meaning that each <span class="math">\(s^{(t)}\)</span> lies on one of the vertices of the
polyhedron. If these vertices have only a few non-zero entries, then <span class="math">\(x^{(t)}\)</span>
will too, as <span class="math">\(x^{(t)}\)</span> is a linear combination of <span class="math">\(s^{(1)} \ldots s^{(t)}\)</span>.
This is in direct contrast to gradient and proximal based methods, wherein
<span class="math">\(x^{(t)}\)</span> is the linear combination of a set of non-sparse <em>gradients</em>.</p>
<p><strong>Atomic Norms</strong> One particular case where Frank-Wolfe shines is when
minimizing <span class="math">\(f(x)\)</span> subject to <span class="math">\(||x|| \le c\)</span> where <span class="math">\(|| \cdot ||\)</span> is an "atomic
norm". We say that <span class="math">\(||\cdot||\)</span> is an atomic norm if <span class="math">\(||x||\)</span> is the smallest <span class="math">\(t\)</span>
such that <span class="math">\(x/t\)</span> is in the convex hull of a finite set of points <span class="math">\(\mathcal{A}\)</span>,
that is,</p>
<div class="math">$$
  ||x|| = \inf \{ t : x \in t \, \text{Conv}(\mathcal{A}) \}
$$</div>
<p>For example, <span class="math">\(||x||_1\)</span> is an atomic norm with <span class="math">\(\mathcal{A}\)</span> being the set of
all vectors with only one <span class="math">\(+1\)</span> or one <span class="math">\(-1\)</span> entry. In these cases, finding
<span class="math">\(\arg\min_{||s|| \le c} \langle \nabla f(x), s \rangle\)</span> is tantamount to
finding which element of <span class="math">\(\mathcal{A}\)</span> minimizes <span class="math">\(\langle \nabla f(x), s
\rangle\)</span> (since <span class="math">\(\text{Conv}(\mathcal{A})\)</span> defines a polyhedron). For a whole
lot more on Atomic Norms, see <a href="http://pages.cs.wisc.edu/~brecht/papers/2010-crpw_inverse_problems.pdf">this tome</a> by
Chandrasekaranm et al.</p>
<h1><a name="extensions" href="#extensions">Extensions</a></h1>
<p><strong>Step Size</strong> The proof above relied on a step size of <span class="math">\(\alpha^{(t)} =
\frac{2}{t+2}\)</span>, but as usual <a href="/blog/gradient-descent.html#line_search">Line Search</a> can be applied to
accelerate convergence.</p>
<p><strong>Approximate Linear Solutions</strong> Though not stated in the proof above,
another cool point about Frank-Wolfe is that you don't actually need to solve
the linear mini-problem exactly, but you will still converge to the optimal
solution (albet at a slightly slower rate). In particular, assume that each
mini-problem can be solved approximately with additive error <span class="math">\(\frac{\delta
C_f}{t+2}\)</span> at iteration <span class="math">\(t\)</span>,</p>
<div class="math">$$
  \langle s^{(t+1)}, \nabla f(x^{(t)}) \rangle
  \le \min_{s} \langle s, \nabla f(x^{(t)}) \rangle + \frac{\delta C_f}{t+2}
$$</div>
<p>then Frank-Wolfe's rate of convergence is</p>
<div class="math">$$
  f(x^{(t)}) - f(x^{*}) \le \frac{2 C_f}{t+2} (1 + \delta)
$$</div>
<p>The proof for this can be found in the supplement to <a href="http://jmlr.csail.mit.edu/proceedings/papers/v28/jaggi13-supp.pdf">Jaggi's</a>
excellent survey on Frank-Wolfe for machine learning.</p>
<h1><a name="invariance" href="#invariance">Linear Invariance</a></h1>
<p>Another cool fact about Frank-Wolfe is that it's <em>linearly invariant</em> -- that
is, if you rotate and scale the space, nothing changes about the convergence
rate. This is in direct contrast to many other methods which depend on the
<a href="http://en.wikipedia.org/wiki/Condition_number">condition number</a> of a function (for functions with
Hessians, this is the ratio between the largest and smallest eigenvalues,
<span class="math">\(\sigma_{\max} / \sigma_{\min})\)</span>.</p>
<p>Suppose we transform our input space with a surjective (that is, onto) linear
transformation <span class="math">\(M: \hat{D} \rightarrow D\)</span>. Let's now try to solve the problem,</p>
<div class="math">$$
  \min_{\hat{x} \in \hat{D}} \hat{f}(\hat{x}) = f(M \hat{x}) = f(x)
$$</div>
<p>Let's look at the solution to the per-iteration mini-problem we need to solve
for Frank-Wolfe,</p>
<div class="math">$$
\begin{align*}
  \min_{\hat{s} \in \hat{D}} \langle \nabla \hat{f}(\hat{x}), \hat{s} \rangle
  = \min_{\hat{s} \in \hat{D}} \langle M^T \nabla f( M \hat{x}), \hat{s} \rangle
  = \min_{\hat{s} \in \hat{D}} \langle \nabla f( x ), M \hat{s} \rangle
  = \min_{s \in D} \langle \nabla f( x ), s \rangle
\end{align*}
$$</div>
<p>In other words, we will find the same <span class="math">\(s\)</span> if we solve in the original space,
or if we find <span class="math">\(\hat{s}\)</span> and then map it back to <span class="math">\(s\)</span>. No matter how <span class="math">\(M\)</span> warps
the space, Frank-Wolfe will do the same thing. This also means that if there's
a linear transformation you can do to make the points of your polyhedron
sparse, you can do it with no penalty!</p>
<h1><a name="references" href="#references">References</a></h1>
<p><strong>Proof of Convergence, Linear Invariance</strong> Pretty much everything in this
article comes from <a href="http://jmlr.csail.mit.edu/proceedings/papers/v28/jaggi13-supp.pdf">Jaggi's</a> fantastic article on Frank-Wolfe for
machine learning.</p>
<h1><a name="reference-impl" href="#reference-impl">Reference Implementation</a></h1>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">frank_wolfe</span><span class="p">(</span><span class="n">minisolver</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Frank-Wolfe Algorithm</span>

<span class="sd">  Parameters</span>
<span class="sd">  ----------</span>
<span class="sd">  minisolver : function</span>
<span class="sd">      minisolver(x) = argmin_{s \in D} &lt;x, s&gt;</span>
<span class="sd">  gradient : function</span>
<span class="sd">      gradient(x) = gradient[f](x)</span>
<span class="sd">  alpha : function</span>
<span class="sd">      learning rate</span>
<span class="sd">  x0 : array</span>
<span class="sd">      initial value for x</span>
<span class="sd">  epsilon : float</span>
<span class="sd">      desired accuracy</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">xs</span> <span class="o">=</span> <span class="p">[</span><span class="n">x0</span><span class="p">]</span>
  <span class="n">iteration</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">xs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">s_next</span> <span class="o">=</span> <span class="n">minisolver</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">g</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">s_next</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">epsilon</span><span class="p">:</span>
      <span class="k">break</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">alpha</span><span class="p">(</span><span class="n">iteration</span><span class="o">=</span><span class="n">iteration</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">direction</span><span class="o">=</span><span class="n">s_next</span><span class="p">)</span>
    <span class="n">x_next</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">a</span> <span class="o">*</span> <span class="n">s_next</span>
    <span class="n">xs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_next</span><span class="p">)</span>
    <span class="n">iteration</span> <span class="o">+=</span> <span class="mi">1</span>
  <span class="k">return</span> <span class="n">xs</span>


<span class="k">def</span> <span class="nf">default_learning_rate</span><span class="p">(</span><span class="n">iteration</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
  <span class="k">return</span> <span class="mf">2.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">iteration</span> <span class="o">+</span> <span class="mf">2.0</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
  <span class="kn">import</span> <span class="nn">os</span>

  <span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
  <span class="kn">import</span> <span class="nn">pylab</span> <span class="kn">as</span> <span class="nn">pl</span>
  <span class="kn">import</span> <span class="nn">yannopt.plotting</span> <span class="kn">as</span> <span class="nn">plotting</span>

  <span class="c1">### FRANK WOLFE ALGORITHM ###</span>

  <span class="c1"># problem definition</span>
  <span class="n">function</span>    <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span>
  <span class="n">gradient</span>    <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span>
  <span class="n">minisolver</span>  <span class="o">=</span> <span class="k">lambda</span> <span class="n">y</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="n">y</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">2</span> <span class="c1"># D = [-1, 2]</span>
  <span class="n">x0</span> <span class="o">=</span> <span class="mf">1.0</span>

  <span class="c1"># run gradient descent</span>
  <span class="n">iterates</span> <span class="o">=</span> <span class="n">frank_wolfe</span><span class="p">(</span><span class="n">minisolver</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">default_learning_rate</span><span class="p">,</span> <span class="n">x0</span><span class="p">)</span>

  <span class="c1">### PLOTTING ###</span>

  <span class="n">plotting</span><span class="o">.</span><span class="n">plot_iterates_vs_function</span><span class="p">(</span><span class="n">iterates</span><span class="p">,</span> <span class="n">function</span><span class="p">,</span>
                                     <span class="n">path</span><span class="o">=</span><span class="s1">&#39;figures/iterates.png&#39;</span><span class="p">,</span> <span class="n">y_star</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
  <span class="n">plotting</span><span class="o">.</span><span class="n">plot_iteration_vs_function</span><span class="p">(</span><span class="n">iterates</span><span class="p">,</span> <span class="n">function</span><span class="p">,</span>
                                      <span class="n">path</span><span class="o">=</span><span class="s1">&#39;figures/convergence.png&#39;</span><span class="p">,</span> <span class="n">y_star</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>

  <span class="c1"># make animation</span>
  <span class="n">iterates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">iterates</span><span class="p">)</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="s1">&#39;figures/animation&#39;</span><span class="p">)</span>
  <span class="k">except</span> <span class="ne">OSError</span><span class="p">:</span>
    <span class="k">pass</span>

  <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">iterates</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">iterates</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
    <span class="n">x_plus</span> <span class="o">=</span> <span class="n">iterates</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">s_plus</span> <span class="o">=</span> <span class="n">minisolver</span><span class="p">(</span><span class="n">gradient</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="n">f</span> <span class="o">=</span> <span class="n">function</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">gradient</span>
    <span class="n">f_hat</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">y</span><span class="p">:</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">g</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span>

    <span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span> <span class="o">=</span> <span class="n">plotting</span><span class="o">.</span><span class="n">limits</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
    <span class="n">ymin</span><span class="p">,</span> <span class="n">ymax</span> <span class="o">=</span> <span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span>

    <span class="n">pl</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xmin</span> <span class="p">,</span><span class="n">xmax</span><span class="p">),</span> <span class="n">function</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="p">)),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="p">])</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="n">ymin</span><span class="p">,</span> <span class="n">ymax</span><span class="p">])</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;f(x)&#39;</span><span class="p">)</span>

    <span class="n">pl</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="p">],</span> <span class="p">[</span><span class="n">f_hat</span><span class="p">(</span><span class="n">xmin</span><span class="p">),</span> <span class="n">f_hat</span><span class="p">(</span><span class="n">xmax</span><span class="p">)],</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">vlines</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">ymin</span><span class="p">,</span> <span class="n">ymax</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span> <span class="o">*</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;solid&#39;</span><span class="p">)</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_plus</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">x_plus</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;D&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">x_plus</span><span class="p">,</span> <span class="n">f_hat</span><span class="p">(</span><span class="n">x_plus</span><span class="p">),</span> <span class="n">f</span><span class="p">(</span><span class="n">x_plus</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.8</span><span class="p">,</span><span class="mf">0.0</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dotted&#39;</span><span class="p">)</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">s_plus</span><span class="p">,</span> <span class="n">f_hat</span><span class="p">(</span><span class="n">s_plus</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="mf">0.35</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>

    <span class="n">pl</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;figures/animation/</span><span class="si">%02d</span><span class="s1">.png&#39;</span> <span class="o">%</span> <span class="n">t</span><span class="p">)</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>


<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>

    <div id="article_meta">
        Category:
          <a href="/category/optimization.html">optimization</a>
        <br />Tags:
          <a href="/tag/optimization.html">optimization</a>
,           <a href="/tag/first-order.html">first-order</a>
,           <a href="/tag/sparsity.html">sparsity</a>
    </div>
  </article>

  <footer>
    <a href="/" class="button_accent">&larr;&nbsp;&nbsp;&nbsp;Back to blog</a>
  </footer>

  <div id="comments">
    <h2>Comments</h2>
    <div id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_identifier = "blog/frank-wolfe.html";
      (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://duckworthd-blog.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>Please enable JavaScript to view <a href="http://disqus.com/?ref_noscript">comments</a>.</noscript>
  </div>

	</section>


  <!-- scripts -->
  <script
    type= "text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    >
  </script>
  <script
    type= "text/javascript"
    src="/theme/mathjax/MathJaxLocal.js"
    >
  </script>
  <script
      type="text/javascript"
      src="/theme/lightbox/js/jquery-1.11.0.min.js"
      >
  </script>
  <script
      type="text/javascript"
      src="/theme/lightbox/js/lightbox.min.js"
      >
  </script>
</body>
</html>