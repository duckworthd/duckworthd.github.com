<!DOCTYPE html>
<html lang="en">
<head>
  <!-- enable responsive design -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- stylesheets -->
  <link rel="stylesheet" type="text/css" href="/theme/css/style.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/pygments.css">
  <link rel="stylesheet" type="text/css" href="/theme/font-awesome-4.3.0/css/font-awesome.min.css">

  <!-- fonts -->
  <link href='http://fonts.googleapis.com/css?family=Open+Sans:800,400,300|Inconsolata' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:700,900' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Lora:700' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Playball' rel='stylesheet' type='text/css'>

  <!-- No RSS/ATOM feeds -->


    <title>Strongly Convex</title>
    <meta charset="utf-8" />
</head>
<body>
  <div class="row">
    <div id="sidebar">
      <figure id="user_logo">
        <a href=""><div class="logo">&nbsp;</div></a>
      </figure>

      <div class="user_meta">
        <h1 id="user">
          <a  href="/"
              class="sitename">
            Strongly Convex
          </a>
        </h1>
        <h2></h2>

        <ul>
          <hr></hr>
            <li><a href="/">Blog</a></li>
            <hr></hr>
            <li><a href="/about.html">About</a></li>
            <hr></hr>
        </ul>
      </div>
    </div>

    <div id="posts">
	<header>
      <h1 class="title">
			  <a  href="/blog/sparse-l2.html"
            rel="bookmark"
			  	  title="Permalink to Stochastic Gradient Descent and Sparse $L_2$ regularization">
          Stochastic Gradient Descent and Sparse $L_2$ regularization
        </a>
      </h1>
		  <abbr class="published">Thu 10 May 2012</abbr>
	</header>
  <article>
    <p>Suppose you’re doing some typical supervised learning on a gigantic dataset where the total loss over all samples for parameter <span class="math">\(w\)</span> is simply the sum of the losses of each sample <span class="math">\(i\)</span>, i.e.,</p>
<div class="math">$$
  L(w) = \sum_{i} l(x_i, y_i, w)
$$</div>
<p>Basically any loss function you can think of in the i.i.d sample regime can be composed this way. Since we assumed that your dataset was huge, there's no way you’re going to be able to load it all into memory for BFGS, so you choose to use Stochastic Gradient Descent. The update for sample <span class="math">\(i\)</span> with step size <span class="math">\(\eta_t\)</span> would then be,</p>
<div class="math">$$
  w_{t+1} = w_t - \eta_t \nabla_w l(x_i, y_i, w_t)
$$</div>
<p>So far, so good. If <span class="math">\(\nabla_w l(x_i, y_i, w)\)</span> is sparse, then you only need to change a handful of <span class="math">\(w\)</span>'s components. Of course, being the astute Machine Learning expert that you are, you know that you’re going to need some regularization. Let's redefine the total loss and take a look at our new update equation,</p>
<div class="math">$$
\begin{aligned}
  L(w) &amp; = \sum_{i} l(x_i, y_i, w) + \frac{\lambda}{2}||w||_2^2  \\
  w_{t+1} &amp; = w_t - \eta_t \left( \nabla_w l(x_i, y_i, w_t) + \lambda w_t \right)
\end{aligned}
$$</div>
<p>Uh oh. Now that <span class="math">\(w\)</span> appears in our Stochastic Gradient Descent update equation, you’re going to have change every non-zero element of <span class="math">\(w\)</span> at every iteration, even if <span class="math">\(\nabla_w l(x_i, y_i, w)\)</span> is sparse! Whatever shall you do?</p>
<p>The answer isn't as scary as you might think. Let’s do some algebraic manipulation from <span class="math">\(t = 0\)</span>,</p>
<div class="math">$$
\begin{aligned}
  w_{1}
  &amp; = w_0 - \eta_0 \left( \nabla_w l(x_i, y_i, w_0) + \lambda w_0 \right) \\
  &amp; = w_0 - \eta_0 \nabla_w l(x_i, y_i, w_0) - \eta_0 \lambda w_0 \\
  &amp; = (1 - \eta_0 \lambda ) w_0 - \eta_0 \nabla_w l(x_i, y_i, w_0) \\
  &amp; = (1 - \eta_0 \lambda ) \left(
      w_0 - \frac{\eta_0}{1-\eta_0 \lambda } \nabla_w l(x_i, y_i, w_0)
    \right) \\
\end{aligned}
$$</div>
<p>Do you see it now? <span class="math">\(L_2\)</span> regularization is really just a rescaling of <span class="math">\(w_t\)</span> at every iteration. Thus instead of keeping <span class="math">\(w_t\)</span>, let’s keep track of,</p>
<div class="math">$$
\begin{aligned}
  c_t &amp; = \prod_{\tau=0}^t (1-\eta_{\tau} \lambda )  \\
  \bar{w}_t &amp; = \frac{w_t}{c_t}
\end{aligned}
$$</div>
<p>where you update <span class="math">\(\bar{w}_t\)</span> and <span class="math">\(c_t\)</span> by,</p>
<div class="math">$$
\begin{aligned}
  \bar{w}_{t+1}
  &amp; = \bar{w}_t - \frac{\eta_t}{(1 - \eta_t) c_t} \nabla_w l(x_i, w_i, c_t \bar{w}_t) \\
  c_{t+1}
  &amp; = (1 - \eta_t \lambda) c_t
\end{aligned}
$$</div>
<p>And that’s it! As a final note, depending what value you choose for <span class="math">\(\lambda\)</span>, <span class="math">\(c_t\)</span> is going to get really big or really small pretty fast. The usual "take the log" tricks aren't going to fly, either, as <span class="math">\(c_t\)</span> need not be positive. The only way around it I’ve found is to check every iteration if <span class="math">\(c_t\)</span> is getting out of hand, then transform <span class="math">\(\bar{w}_{t} \leftarrow \bar{w}_t c_t\)</span> and <span class="math">\(c_t \leftarrow 1\)</span> if it is.</p>
<p>Finally, credit should be given where credit is due. This is a slightly more detailed explanation of <a href="http://blog.smola.org/post/940672544/fast-quadratic-regularization-for-online-learning&gt;">Alex Smola</a> blog post from about a year ago, which in turn is accredited to Leon Bottou.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>

    <div id="article_meta">
        Category:
          <a href="/category/optimization.html">optimization</a>
        <br />Tags:
          <a href="/tag/regularization.html">regularization</a>
,           <a href="/tag/sparsity.html">sparsity</a>
    </div>
  </article>

  <footer>
    <a href="/" class="button_accent">&larr;&nbsp;&nbsp;&nbsp;Back to blog</a>
  </footer>


    </div>
  </div>


  <!-- scripts -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script
    type= "text/javascript"
    src="/theme/mathjax/MathJaxLocal.js"
    >
  </script>
</body>
</html>