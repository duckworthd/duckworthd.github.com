<!DOCTYPE html>
<html lang="en">
<head>
  <!-- enable responsive design -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- stylesheets -->
  <link rel="stylesheet" type="text/css" href="/theme/css/style.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/pygments.css">
  <link rel="stylesheet" type="text/css" href="/theme/lightbox/css/lightbox.css">
  <link rel="stylesheet" type="text/css" href="/theme/font-awesome-4.3.0/css/font-awesome.min.css">

  <!-- fonts -->
  <link href='http://fonts.googleapis.com/css?family=Open+Sans:800,400,300|Inconsolata' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:700,900' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Lora:700' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Playball' rel='stylesheet' type='text/css'>

  <!-- No RSS/ATOM feeds -->


    <title>Strongly Convex</title>
    <meta charset="utf-8" />
</head>
<body>
  <div class="row">
    <div id="sidebar">
      <figure id="user_logo">
        <a href=""><div class="logo">&nbsp;</div></a>
      </figure>

      <div class="user_meta">
        <h1 id="user">
          <a  href="/"
              class="sitename">
            Strongly Convex
          </a>
        </h1>
        <h2></h2>

        <ul>
          <hr></hr>
            <li><a href="/">Blog</a></li>
            <hr></hr>
            <li><a href="/about.html">About</a></li>
            <hr></hr>
        </ul>
      </div>
    </div>

    <div id="posts">
	<header>
      <h1 class="title">
			  <a  href="/blog/coordinate-ascent-convex-clustering.html"
            rel="bookmark"
			  	  title="Permalink to Coordinate Ascent for Convex Clustering">
          Coordinate Ascent for Convex Clustering
        </a>
      </h1>
		  <abbr class="published">Tue 23 April 2013</abbr>
	</header>
  <article>
    <p>Convex clustering is the reformulation of k-means clustering as a convex
problem. While the two problems are not equivalent, the former can be seen as a
relaxation of the latter that allows us to easily find globally optimal
solutions (as opposed to only locally optimal ones).</p>
<p>Suppose we have a set of points <span class="math">\(\{ x_i : i = 1, \ldots, n\}\)</span>. Our goal is to
partition these points into groups such that all the elements in each group are
close to each other and are distant from points in other groups.</p>
<p>In this post, I'll talk about an algorithm to do just that.</p>
<div class="img-center" style="max-width: 400px;">
  <img src="/assets/img/convex_clustering/clusters.png"></img>
  <span class="caption">
    8 clusters of points in 2D with their respective centers.  All points of
    the same color belong to the same cluster.
  </span>
</div>

<h1><a name="k-means" href="#k-means">K-Means</a></h1>
<p>The original objective for k-means clustering is as follows. Suppose we want
to find <span class="math">\(k\)</span> sets <span class="math">\(S_i\)</span> such that every <span class="math">\(x_i\)</span> is in exactly 1 set <span class="math">\(S_j\)</span>. Each <span class="math">\(S_j\)</span>
will then have a center <span class="math">\(\theta_j\)</span>, which is simply the average of all <span class="math">\(x_i\)</span> it
contains. Putting it all together, we obtain the following optimization problme,</p>
<div class="math">$$
\begin{align*}
  &amp; \underset{S}{\min}  &amp; &amp; \sum_{j=1}^{k} \sum_{i \in S_j} ||x_i - \theta_j||_2^2 \\
  &amp; \text{subject to}   &amp; &amp; \theta_j = \frac{1}{|S_j|} \sum_{i \in S_j} x_i \\
  &amp;                     &amp; &amp; \bigcup_{j} S_j = \{ 1 \ldots n \}
\end{align*}
$$</div>
<p>In 2009, <a href="http://dl.acm.org/citation.cfm?id=1519389">Aloise et al.</a> proved that solving this problem is
NP-hard, meaning that short of enumerating every possible partition, we cannot
say whether or not we've found an optimal solution <span class="math">\(S^{*}\)</span>. In other words, we
can approximately solve k-means, but actually solving it is very
computationally intense (with the usual caveats about <span class="math">\(P = NP\)</span>).</p>
<h1><a name="convex-clustering" href="#convex-clustering">Convex Clustering</a></h1>
<p>Convex clustering sidesteps this complexity result by proposing a new
problem that we <em>can</em> solve quickly. The optimal solution for this new problem
need not coincide with that of k-means, but <a href="http://www.control.isy.liu.se/research/reports/2011/2992.pdf">can be seen</a> a solution to
the convex relaxation of the original problem.</p>
<p>The idea of convex clustering is that each point <span class="math">\(x_i\)</span> is paired with its
associated center <span class="math">\(u_i\)</span>, and the distance between the two is minimized. If this
were nothing else, <span class="math">\(u_i = x_i\)</span> would be the optimal solution, and no
clustering would happen. Instead, a penalty term is added that brings the
clusters centers close together,</p>
<div class="math">$$
\begin{align*}
  \min_{u} \frac{1}{2} \sum_{i=1}^{n} ||x_i - u_i||_2^2
            + \gamma \sum_{i &lt; j} w_{i,j} ||u_i - u_j||_p
\end{align*}
$$</div>
<p>Notice that the distance <span class="math">\(||x_i - u_i||_2^2\)</span> is a squared 2-norm, but
the distance between the centers <span class="math">\(||u_i - u_j||_p\)</span> is a p-norm (<span class="math">\(p \in \{1, 2,
\infty \}\)</span>). This sum-of-norms type penalization brings about "group sparsity"
and is used primarily because many of the elements in this sum will be 0 at the
optimum. In convex clustering, that means <span class="math">\(u_i = u_j\)</span> for some pairs <span class="math">\(i\)</span> and
<span class="math">\(j\)</span> -- in other words, <span class="math">\(i\)</span> and <span class="math">\(j\)</span> are clustered together!</p>
<h1><a name="algorithms" href="#algorithms">Algorithms for Convex Clustering</a></h1>
<p>As the convex clustering formulation is a convex problem, we automatically
get a variety of black-box algorithms capable of solving it. Unfortunately, the
number of variables in the problem is rather large -- if <span class="math">\(x_i \in
\mathcal{R}^{d}\)</span>, then <span class="math">\(u \in \mathcal{R}^{n \times d}\)</span>.  If <span class="math">\(d = 5\)</span>, we cannot
reasonably expect interior point solvers such as <a href="http://cvxr.com/cvx/">cvx</a> to handle any more
than a few thousand points.</p>
<p><a href="http://www.icml-2011.org/papers/419_icmlpaper.pdf">Hocking et al.</a> and <a href="http://arxiv.org/abs/1304.0499">Chi et al.</a> were the first to design
algorithms specifically for convex clustering. The former designed one
algorithm for each <span class="math">\(p\)</span>-norm, employing active sets (<span class="math">\(p \in \{1, 2\}\)</span>),
subgradient descent (<span class="math">\(p = 2\)</span>), and the Frank-Wolfe algorithm (<span class="math">\(p = \infty\)</span>).
The latter makes use of <a href="http://www.stanford.edu/~boyd/papers/admm_distr_stats.html">ADMM</a> and AMA, the latter of which reduces to
proximal gradient on a dual objective.</p>
<p>Here, I'll describe another method for solving the convex clustering problem
based on coordinate ascent. The idea is to take the original formulation,
substitute a new primal variable <span class="math">\(z_l = u_{l_1} - u_{l_2}\)</span>, then update a dual
variable <span class="math">\(\lambda_l\)</span> corresponding to each equality constraint 1 at a time. For
this problem, we can reconstruct the primal variables <span class="math">\(u_i\)</span> in closed form
given the dual variables, so it is easy to check how close we are to the
optimum.</p>
<!--
  <table class="table table-hover table-bordered">
    <tr>
      <th>Name</th>
      <th>Memory required</th>
      <th>per-iteration complexity</th>
      <th>number of iterations required</th>
      <th>parallelizability</th>
    </tr>
    <tr>
      <td>Clusterpath ($L_1$)</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Clusterpath ($L_2$)</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Clusterpath ($L_{\infty}$)</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>ADMM</td>
      <td>$O(pd)$</td>
      <td>$O(pd)$</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>AMA (accelerated)</td>
      <td>$O(pd)$</td>
      <td>$O(pd)$</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Coordinate Ascent</td>
      <td>$O(pd)$</td>
      <td>$O(pd)$</td>
      <td></td>
      <td></td>
    </tr>
  </table>

  For $p =$ number of pairs with $w_l > 0$, $n =$ the number of points $x_i$,
$d =$ the dimensionality of $x_i$, $c = $ the current number of clusters
-->

<h1><a name="reformulation" href="#reformulation">Problem Reformulation</a></h1>
<p>To describe the dual problem being maximized, we first need to modify the
primal problem. First, let <span class="math">\(z_l = u_{l_1} - u_{l_2}\)</span>. Then we can write the
objective function as,</p>
<div class="math">$$
\begin{align*}
  &amp; \underset{S}{\min}  &amp; &amp; \frac{1}{2} \sum_{i=1}^{n} ||x_i - u_i||_2^2
                            + \gamma \sum_{l} w_{l} ||z_l||_p \\
  &amp; \text{subject to}   &amp; &amp; z_l = u_{l_1} - u_{l_2}
\end{align*}
$$</div>
<p><a href="http://arxiv.org/abs/1304.0499">Chi et al.</a> show on page 6 that the dual of this problem is then,</p>
<div class="math">$$
\begin{align*}
  &amp; \underset{\lambda}{\max}  &amp; &amp; - \frac{1}{2} \sum_{i} ||\Delta_i||_2^2
                                  - \sum_{l} \lambda_l^T (x_{l_1} - x_{l_2}) \\
  &amp; \text{subject to}         &amp; &amp; ||\lambda_l||_{p^{*}} \le \gamma w_l \\
  &amp;                           &amp; &amp; \Delta_{i} = \sum_{l: l_1 = i} \lambda_l - \sum_{l : l_2 = i} \lambda_l
\end{align*}
$$</div>
<p>In this notation, <span class="math">\(||\cdot||_{p^{*}}\)</span> is the dual norm of <span class="math">\(||\cdot||_p\)</span>. The
primal variables <span class="math">\(u\)</span> and dual variables <span class="math">\(\lambda\)</span> are then related by the
following equation,</p>
<div class="math">$$
  u_i = \Delta_i + x_i
$$</div>
<h1><a name="coordinate-ascent" href="#coordinate-ascent">Coordinate Ascent</a></h1>
<p>Now let's optimize the dual problem 1 <span class="math">\(\lambda_k\)</span> at a time. First, notice
that <span class="math">\(\lambda_k\)</span> will only appear in 2 <span class="math">\(\Delta_i\)</span> terms -- <span class="math">\(\Delta_{k_1}\)</span> and
<span class="math">\(\Delta_{k_2}\)</span>. After dropping all terms independent of <span class="math">\(\lambda_k\)</span>, we now get
the following problem,</p>
<div class="math">$$
\begin{align*}
  &amp; \underset{\lambda_k}{\min}  &amp; &amp; \frac{1}{2} (||\Delta_{k_1}||_2^2 + ||\Delta_{k_2}||_2^2)
                                    + \lambda_k^T (x_{k_1} - x_{k_2}) \\
  &amp; \text{subject to}         &amp; &amp; ||\lambda_k||_{p^{*}} \le \gamma w_k \\
  &amp;                           &amp; &amp; \Delta_{k_1} = \sum_{l: l_1 = k_1} \lambda_l - \sum_{l : l_2 = k_1} \lambda_l \\
  &amp;                           &amp; &amp; \Delta_{k_2} = \sum_{l: l_1 = k_2} \lambda_l - \sum_{l : l_2 = k_2} \lambda_l
\end{align*}
$$</div>
<p>We can pull <span class="math">\(\lambda_k\)</span> out of <span class="math">\(\Delta_{k_1}\)</span> and <span class="math">\(\Delta_{k_2}\)</span> to get,</p>
<div class="math">$$
\begin{align*}
  ||\Delta_{k_1}||_2^2 &amp; = ||\lambda_k||_2^2 + ||\Delta_{k_1} - \lambda_k||_2^2 + 2 \lambda_k^T (\Delta_{k_1} - \lambda_k) \\
  ||\Delta_{k_2}||_2^2 &amp; = ||\lambda_k||_2^2 + ||\Delta_{k_2} + \lambda_k||_2^2 - 2 \lambda_k^T (\Delta_{k_2} + \lambda_k)
\end{align*}
$$</div>
<p>Let's define <span class="math">\(\tilde{\Delta_{k_1}} = \Delta_{k_1} - \lambda_k\)</span> and
<span class="math">\(\tilde{\Delta_{k_2}} = \Delta_{k_2} + \lambda_k\)</span> and add <span class="math">\(||\frac{1}{2}
(\tilde{\Delta_{k_1}} - \tilde{\Delta_{k_2}} + x_{k_1} - x_{k_2})||_2^2\)</span> to the
objective.</p>
<div class="math">$$
\begin{align*}
  &amp; \underset{\lambda_k}{\min}  &amp; &amp; ||\lambda_k||_2^2
                                    + 2 \frac{1}{2} \lambda_k^T (\tilde{\Delta_{k_1}} - \tilde{\Delta_{k_2}} + x_{k_1} - x_{k_2})
                                    + ||\frac{1}{2} (\tilde{\Delta_{k_1}} - \tilde{\Delta_{k_2}} + x_{k_1} - x_{k_2})||_2^2 \\
  &amp; \text{subject to}         &amp; &amp; ||\lambda_k||_{p^{*}} \le \gamma w_k \\
  &amp;                           &amp; &amp; \tilde{\Delta_{k_1}} = \sum_{l: l_1 = k_1; l \ne k} \lambda_l - \sum_{l : l_2 = k_1; l \ne k} \lambda_l \\
  &amp;                           &amp; &amp; \tilde{\Delta_{k_2}} = \sum_{l: l_1 = k_2; l \ne k} \lambda_l - \sum_{l : l_2 = k_2; l \ne k} \lambda_l
\end{align*}
$$</div>
<p>We can now factor the objective into a quadratic,</p>
<div class="math">$$
\begin{align*}
  &amp; \underset{\lambda_k}{\min}  &amp; &amp; ||\lambda_k - \left( - \frac{1}{2}(\tilde{\Delta_{k_1}} - \tilde{\Delta_{k_2}} + x_{k_1} - x_{k_2}) \right) ||_2^2 \\
  &amp; \text{subject to}         &amp; &amp; ||\lambda_k||_{p^{*}} \le \gamma w_k \\
  &amp;                           &amp; &amp; \tilde{\Delta_{k_1}} = \sum_{l: l_1 = k_1; l \ne k} \lambda_l - \sum_{l : l_2 = k_1; l \ne k} \lambda_l \\
  &amp;                           &amp; &amp; \tilde{\Delta_{k_2}} = \sum_{l: l_1 = k_2; l \ne k} \lambda_l - \sum_{l : l_2 = k_2; l \ne k} \lambda_l
\end{align*}
$$</div>
<p>This problem is simply a Euclidean projection onto the ball defined by
<span class="math">\(||\cdot||_{p^{*}}\)</span>. We're now ready to write the algorithm,</p>
<div class="pseudocode">
<p><strong>Input:</strong> Initial dual variables <span class="math">\(\lambda^{(0)}\)</span>, weights <span class="math">\(w_l\)</span>, and regularization parameter <span class="math">\(\gamma\)</span></p>
<ol>
<li>Initialize <span class="math">\(\Delta_i^{(0)} = \sum_{l: l_1 = i} \lambda_l^{(0)} - \sum_{l: l_2 = i} \lambda_l^{(0)}\)</span></li>
<li>For each iteration <span class="math">\(m = 0,1,2,\ldots\)</span> until convergence<ol>
<li>Let <span class="math">\(\Delta^{(m+1)} = \Delta^{(m)}\)</span></li>
<li>For each pair of points <span class="math">\(l = (i,j)\)</span> with <span class="math">\(w_{l} &gt; 0\)</span><ol>
<li>Let <span class="math">\(\Delta_i^{(m+1)} \leftarrow \Delta_i^{(m+1)} - \lambda_l^{(m)}\)</span> and <span class="math">\(\Delta_j^{(m+1)} \leftarrow \Delta_i^{(m+1)} + \lambda_l^{(m)}\)</span></li>
<li><span class="math">\(\lambda_l^{(m+1)} = \text{project}(- \frac{1}{2}(\Delta_i^{(m+1)} - \Delta_j^{(m+1)} + x_{i} - x_{j}),
                                       \{ \lambda : ||\lambda||_{p^{*}} \le \gamma w_l \}\)</span>)</li>
<li><span class="math">\(\Delta_i^{(m+1)} \leftarrow \Delta_i^{(m+1)} + \lambda_l^{(m+1)}\)</span> and <span class="math">\(\Delta_j^{(m+1)} \leftarrow \Delta_j^{(m+1)} - \lambda_l^{(m+1)}\)</span></li>
</ol>
</li>
</ol>
</li>
<li>Return <span class="math">\(u_i = \Delta_i + x_i\)</span> for all <span class="math">\(i\)</span></li>
</ol>
</div>
<p>Since we can easily construct the primal variables from the dual variables
and can evaluate the primal and dual functions in closed form, we can use the
duality gap to determine when we are converged.</p>
<h1><a name="conclusion" href="#conclusion">Conclusion</a></h1>
<p>In this post, I introduced a coordinate ascent algorithm for convex
clustering. Empirically, the algorithm is quite quick, but it doesn't share the
parallelizability or convergence proofs of its siblings, ADMM and AMA. However,
coordinate descent has an upside: there are no parameters to tune, and every
iteration is guaranteed to improve the objective function. Within each
iteration, updates are quick asymptotically and empirically.</p>
<p>Unfortunately, like all algorithms based on the dual for this particular
problem, the biggest burden is on memory. Whereas the primal formulation
requires the number of variables grow linearly with the number of data points,
the dual formulation can grow as high as quadratically. In addition, the primal
variables allow for centers to be merged, allowing for potential space-savings
as the algorithm is running. The dual seems to lack this property, requiring
all dual variables to be fully instantiated.</p>
<h1><a name="references" href="#references">References</a></h1>
<p>The original formulation for convex clustering was introduced by <a href="http://www.control.isy.liu.se/research/reports/2011/2992.pdf">Lindsten et
al.</a> and <a href="http://www.icml-2011.org/papers/419_icmlpaper.pdf">Hocking et al.</a>. <a href="http://arxiv.org/abs/1304.0499">Chi et al.</a> introduced
ADMM and AMA-based algorithms specifically designed for convex clustering.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>

    <div id="article_meta">
        Category:
          <a href="/category/optimization.html">optimization</a>
        <br />Tags:
          <a href="/tag/optimization.html">optimization</a>
,           <a href="/tag/coordinate-descent.html">coordinate-descent</a>
,           <a href="/tag/clustering.html">clustering</a>
    </div>
  </article>

  <footer>
    <a href="/" class="button_accent">&larr;&nbsp;&nbsp;&nbsp;Back to blog</a>
  </footer>


    </div>
  </div>


  <!-- scripts -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script
    type= "text/javascript"
    src="/theme/mathjax/MathJaxLocal.js"
    >
  </script>
  <script
      type="text/javascript"
      src="/theme/lightbox/js/jquery-1.11.0.min.js"
      >
  </script>
  <script
      type="text/javascript"
      src="/theme/lightbox/js/lightbox.min.js"
      >
  </script>
</body>
</html>