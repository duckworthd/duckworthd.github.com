<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>A Coordinate Ascent Algorithm for Convex Clustering</title>

    <link href="/assets/css/bootstrap.min.css" rel="stylesheet" type="text/css">
    <link href="/assets/css/syntax.css" rel="stylesheet" type="text/css">
    <link href="/assets/css/blah.css" rel="stylesheet" type="text/css">
    <link href="/assets/css/lightbox/lightbox.css" rel="stylesheet" type="text/css">
  </head>
  <body>
    <div class="container">
    <div class="navbar">
  <div class="navbar-inner">
    <div class="nav-div">
      <a class="brand logo" href="#">
        <img src="/assets/img/transmogrifier2.png"></img>
      </a>
      <ul class="nav pull-right">
        <li><a href="/index.html">Home</a></li>
        <li><a href="/projects.html">Projects</a></li>
        <li><a href="/blog.html">Blog</a></li>
        <li><a href="/feed.xml">
          <img src="/assets/img/glyphicons/glyphicons_417_rss.png"></img>
        </a></li>
      </ul>
    </div> <!-- span10 offset1 -->
  </div> <!-- /navbar-inner -->
</div> <!-- /navbar -->


<div class="row-fluid">
  <div class="post box-shadow">
    <header>
      <h1 class="header-title">A Coordinate Ascent Algorithm for Convex Clustering</h1>
      <p class="header-subtext">by Daniel Duckworth on Apr 23, 2013</p>
    </header>
    <article>
      <p>Convex clustering is the reformulation of k-means clustering as a convex problem. While the two problems are not equivalent, the former can be seen as a relaxation of the latter that allows us to easily find globally optimal solutions (as opposed to only locally optimal ones).</p>
<p>Suppose we have a set of points <span class="math">\(\{ x_i : i = 1, \ldots, n\}\)</span>. Our goal is to partition these points into groups such that all the elements in each group are close to each other and are distant from points in other groups.</p>
<p>In this post, I’ll talk about an algorithm to do just that.</p>
<div class="img-center" style="max-width: 400px;">
  
<img src="/assets/img/convex_clustering/clusters.png"></img> <span class="caption"> 8 clusters of points in 2D with their respective centers. All points of the same color belong to the same cluster. </span>
</div>

<h1 id="k-means">K-Means</h1>
<p>The original objective for k-means clustering is as follows. Suppose we want to find <span class="math">\(k\)</span> sets <span class="math">\(S_i\)</span> such that every <span class="math">\(x_i\)</span> is in exactly 1 set <span class="math">\(S_j\)</span>. Each <span class="math">\(S_j\)</span> will then have a center <span class="math">\(\theta_j\)</span>, which is simply the average of all <span class="math">\(x_i\)</span> it contains. Putting it all together, we obtain the following optimization problme,</p>
<p><span class="math">\[
\begin{align*}
  &amp; \underset{S}{\min}  &amp; &amp; \sum_{j=1}^{k} \sum_{i \in S_j} ||x_i - \theta_j||_2^2 \\
  &amp; \text{subject to}   &amp; &amp; \theta_j = \frac{1}{|S_j|} \sum_{i \in S_j} x_i \\
  &amp;                     &amp; &amp; \bigcup_{j} S_j = \{ 1 \ldots n \}
\end{align*}
\]</span></p>
<p>In 2009, <a href="http://dl.acm.org/citation.cfm?id=1519389">Dasgupta et al.</a> proved that solving this problem is NP-hard, meaning that short of enumerating every possible partition, we cannot say whether or not we’ve found an optimal solution <span class="math">\(S^{*}\)</span>. In other words, we can approximately solve k-means, but actually solving it is very computationally intense (with the usual caveats about <span class="math">\(P = NP\)</span>).</p>
<h1 id="convex-clustering">Convex Clustering</h1>
<p>Convex clustering sidesteps this complexity result by proposing a new problem that we <em>can</em> solve quickly. The optimal solution for this new problem need not coincide with that of k-means, but <a href="http://www.control.isy.liu.se/research/reports/2011/2992.pdf">can be seen</a> a solution to the convex relaxation of the original problem.</p>
<p>The idea of convex clustering is that each point <span class="math">\(x_i\)</span> is paired with its associated center <span class="math">\(u_i\)</span>, and the distance between the two is minimized. If this were nothing else, <span class="math">\(u_i = x_i\)</span> would be the optimal solution, and no clustering would happen. Instead, a penalty term is added that brings the clusters centers close together,</p>
<p><span class="math">\[
\begin{align*}
  \min_{u} \frac{1}{2} \sum_{i=1}^{n} ||x_i - u_i||_2^2
            + \gamma \sum_{i &lt; j} w_{i,j} ||u_i - u_j||_p
\end{align*}
\]</span></p>
<p>Notice that the distance <span class="math">\(||x_i - u_i||_2^2\)</span> is a squared 2-norm, but the distance between the centers <span class="math">\(||u_i - u_j||_p\)</span> is a p-norm (<span class="math">\(p \in \{1, 2, \infty \}\)</span>). This sum-of-norms type penalization brings about “group sparsity” and is used primarily because many of the elements in this sum will be 0 at the optimum. In convex clustering, that means <span class="math">\(u_i = u_j\)</span> for some pairs <span class="math">\(i\)</span> and <span class="math">\(j\)</span> – in other words, <span class="math">\(i\)</span> and <span class="math">\(j\)</span> are clustered together!</p>
<h1 id="algorithms-for-convex-clustering">Algorithms for Convex Clustering</h1>
<p>As the convex clustering formulation is a convex problem, we automatically get a variety of black-box algorithms capable of solving it. Unfortunately, the number of variables in the problem is rather large – if <span class="math">\(x_i \in \mathcal{R}^{d}\)</span>, then <span class="math">\(u \in \mathcal{R}^{n \times d}\)</span>. If <span class="math">\(d = 5\)</span>, we cannot reasonably expect interior point solvers such as <a href="http://cvxr.com/cvx/">cvx</a> to handle any more than a few thousand points.</p>
<p><a href="http://www.icml-2011.org/papers/419_icmlpaper.pdf">Hocking et al.</a> and <a href="http://arxiv.org/abs/1304.0499">Chi et al.</a> were the first to design algorithms specifically for convex clustering. The former designed one algorithm for each <span class="math">\(p\)</span>-norm, employing active sets (<span class="math">\(p \in \{1, 2\}\)</span>), subgradient descent (<span class="math">\(p = 2\)</span>), and the Frank-Wolfe algorithm (<span class="math">\(p = \infty\)</span>). The latter makes use of <a href="http://www.stanford.edu/~boyd/papers/admm_distr_stats.html">ADMM</a> and AMA, the latter of which reduces to proximal gradient on a dual objective.</p>
<p>Here, I’ll describe another method for solving the convex clustering problem based on coordinate ascent. The idea is to take the original formulation, substitute a new primal variable <span class="math">\(z_l = u_{l_1} - u_{l_2}\)</span>, then update a dual variable <span class="math">\(\lambda_l\)</span> corresponding to each equality constraint 1 at a time. For this problem, we can reconstruct the primal variables <span class="math">\(u_i\)</span> in closed form given the dual variables, so it is easy to check how close we are to the optimum.</p>
<!--
  <table class="table table-hover table-bordered">
    <tr>
      <th>Name</th>
      <th>Memory required</th>
      <th>per-iteration complexity</th>
      <th>number of iterations required</th>
      <th>parallelizability</th>
    </tr>
    <tr>
      <td>Clusterpath ($L_1$)</td>
      <td></td>
      <td></td>
      <td>1</td>
      <td></td>
    </tr>
    <tr>
      <td>Clusterpath ($L_2$)</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Clusterpath ($L_{\infty}$)</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>ADMM</td>
      <td>$O(pd)$</td>
      <td>$O(pd)$</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>AMA (accelerated)</td>
      <td>$O(pd)$</td>
      <td>$O(pd)$</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>Coordinate Ascent</td>
      <td>$O(pd)$</td>
      <td>$O(pd)$</td>
      <td></td>
      <td></td>
    </tr>
  </table>

  For $p =$ number of pairs with $w_l > 0$, $n =$ the number of points $x_i$,
$d =$ the dimensionality of $x_i$, $c = $ the current number of clusters
-->

<h1 id="problem-reformulation">Problem Reformulation</h1>
<p>To describe the dual problem being maximized, we first need to modify the primal problem. First, let <span class="math">\(z_l = u_{l_1} - u_{l_2}\)</span>. Then we can write the objective function as,</p>
<p><span class="math">\[
\begin{align*}
  &amp; \underset{S}{\min}  &amp; &amp; \frac{1}{2} \sum_{i=1}^{n} ||x_i - u_i||_2^2
                            + \gamma \sum_{l} w_{l} ||z_l||_p \\
  &amp; \text{subject to}   &amp; &amp; z_l = u_{l_1} - u_{l_2}
\end{align*}
\]</span></p>
<p><a href="http://arxiv.org/abs/1304.0499">Chi et al.</a> show on page 6 that the dual of this problem is then,</p>
<p><span class="math">\[
\begin{align*}
  &amp; \underset{\lambda}{\max}  &amp; &amp; - \frac{1}{2} \sum_{i} ||\Delta_i||_2^2
                                  - \sum_{l} \lambda_l^T (x_{l_1} - x_{l_2}) \\
  &amp; \text{subject to}         &amp; &amp; ||\lambda_l||_{p^{*}} \le \gamma w_l \\
  &amp;                           &amp; &amp; \Delta_{i} = \sum_{l: l_1 = i} \lambda_l - \sum_{l : l_2 = i} \lambda_l
\end{align*}
\]</span></p>
<p>In this notation, <span class="math">\(||\cdot||_{p^{*}}\)</span> is the dual norm of <span class="math">\(||\cdot||_p\)</span>. The primal variables <span class="math">\(u\)</span> and dual variables <span class="math">\(\lambda\)</span> are then related by the following equation,</p>
<p><span class="math">\[
  u_i = \Delta_i + x_i
\]</span></p>
<h1 id="coordinate-ascent">Coordinate Ascent</h1>
<p>Now let’s optimize the dual problem 1 <span class="math">\(\lambda_k\)</span> at a time. First, notice that <span class="math">\(\lambda_k\)</span> will only appear in 2 <span class="math">\(\Delta_i\)</span> terms – <span class="math">\(\Delta_{k_1}\)</span> and <span class="math">\(\Delta_{k_2}\)</span>. After dropping all terms independent of <span class="math">\(\lambda_k\)</span>, we now get the following problem,</p>
<p><span class="math">\[
\begin{align*}
  &amp; \underset{\lambda_k}{\min}  &amp; &amp; \frac{1}{2} (\Delta_{k_1} + \Delta_{k_2})
                                    + \lambda_k^T (x_{k_1} - x_{k_2}) \\
  &amp; \text{subject to}         &amp; &amp; ||\lambda_k||_{p^{*}} \le \gamma w_k \\
  &amp;                           &amp; &amp; \Delta_{k_1} = \sum_{l: l_1 = k_1} \lambda_l - \sum_{l : l_2 = k_1} \lambda_l \\
  &amp;                           &amp; &amp; \Delta_{k_2} = \sum_{l: l_1 = k_2} \lambda_l - \sum_{l : l_2 = k_2} \lambda_l
\end{align*}
\]</span></p>
<p>We can pull <span class="math">\(\lambda_k\)</span> out of <span class="math">\(\Delta_{k_1}\)</span> and <span class="math">\(\Delta_{k_2}\)</span> to get,</p>
<p><span class="math">\[
\begin{align*}
  ||\Delta_{k_1}||_2^2 &amp; = ||\lambda_k||_2^2 + ||\Delta_{k_1} - \lambda_k||_2^2 + 2 \lambda_k^T (\Delta_{k_1} - \lambda_k) \\
  ||\Delta_{k_2}||_2^2 &amp; = ||\lambda_k||_2^2 + ||\Delta_{k_2} + \lambda_k||_2^2 - 2 \lambda_k^T (\Delta_{k_2} + \lambda_k)
\end{align*}
\]</span></p>
<p>Let’s define <span class="math">\(\tilde{\Delta_{k_1}} = \Delta_{k_1} - \lambda_k\)</span> and <span class="math">\(\tilde{\Delta_{k_2}} = \Delta_{k_2} + \lambda_k\)</span> and add <span class="math">\(||\frac{1}{2} (\tilde{\Delta_{k_1}} - \tilde{\Delta_{k_2}} + x_{k_1} - x_{k_2})||_2^2\)</span> to the objective.</p>
<p><span class="math">\[
\begin{align*}
  &amp; \underset{\lambda_k}{\min}  &amp; &amp; ||\lambda_k||_2^2
                                    + 2 \frac{1}{2} \lambda_k^T (\tilde{\Delta_{k_1}} - \tilde{\Delta_{k_2}} + x_{k_1} - x_{k_2})
                                    + ||\frac{1}{2} (\tilde{\Delta_{k_1}} - \tilde{\Delta_{k_2}} + x_{k_1} - x_{k_2})||_2^2 \\
  &amp; \text{subject to}         &amp; &amp; ||\lambda_k||_{p^{*}} \le \gamma w_k \\
  &amp;                           &amp; &amp; \tilde{\Delta_{k_1}} = \sum_{l: l_1 = k_1; l \ne k} \lambda_l - \sum_{l : l_2 = k_1; l \ne k} \lambda_l \\
  &amp;                           &amp; &amp; \tilde{\Delta_{k_2}} = \sum_{l: l_1 = k_2; l \ne k} \lambda_l - \sum_{l : l_2 = k_2; l \ne k} \lambda_l
\end{align*}
\]</span></p>
<p>We can now factor the objective into a quadratic,</p>
<p><span class="math">\[
\begin{align*}
  &amp; \underset{\lambda_k}{\min}  &amp; &amp; ||\lambda_k - \left( - \frac{1}{2}(\tilde{\Delta_{k_1}} - \tilde{\Delta_{k_2}} + x_{k_1} - x_{k_2}) \right) ||_2^2 \\
  &amp; \text{subject to}         &amp; &amp; ||\lambda_k||_{p^{*}} \le \gamma w_k \\
  &amp;                           &amp; &amp; \tilde{\Delta_{k_1}} = \sum_{l: l_1 = k_1; l \ne k} \lambda_l - \sum_{l : l_2 = k_1; l \ne k} \lambda_l \\
  &amp;                           &amp; &amp; \tilde{\Delta_{k_2}} = \sum_{l: l_1 = k_2; l \ne k} \lambda_l - \sum_{l : l_2 = k_2; l \ne k} \lambda_l
\end{align*}
\]</span></p>
<p>This problem is simply a Euclidean projection onto the ball defined by <span class="math">\(||\cdot||_{p^{*}}\)</span>. We’re now ready to write the algorithm,</p>
<div class="pseudocode">
  
<p><strong>Input:</strong> Initial dual variables <span class="math">\(\lambda^{(0)}\)</span>, weights <span class="math">\(w_l\)</span>, and regularization parameter <span class="math">\(\gamma\)</span></p>
<ol style="list-style-type: decimal">
<li>Initialize <span class="math">\(\Delta_i^{(0)} = \sum_{l: l_1 = i} \lambda_l^{(0)} - \sum_{l: l_2 = i} \lambda_l^{(0)}\)</span></li>
<li>For each iteration <span class="math">\(m = 0,1,2,\ldots\)</span> until convergence
<ol start="3" style="list-style-type: decimal">
<li>Let <span class="math">\(\Delta^{(m+1)} = \Delta^{(m)}\)</span></li>
<li>For each pair of points <span class="math">\(l = (i,j)\)</span> with <span class="math">\(w_{l} &gt; 0\)</span>
<ol start="5" style="list-style-type: decimal">
<li>Let <span class="math">\(\Delta_i^{(m+1)} \leftarrow \Delta_i^{(m+1)} - \lambda_l^{(m)}\)</span> and <span class="math">\(\Delta_j^{(m+1)} \leftarrow \Delta_i^{(m+1)} + \lambda_l^{(m)}\)</span></li>
<li><span class="math">\(\lambda_l^{(m+1)} = \text{project}(- \frac{1}{2}(\Delta_i^{(m+1)} - \Delta_j^{(m+1)} + x_{i} - x_{j}),                                        \{ \lambda : ||\lambda||_{p^{*}} \le \gamma w_l \}\)</span>)</li>
<li><span class="math">\(\Delta_i^{(m+1)} \leftarrow \Delta_i^{(m+1)} + \lambda_l^{(m+1)}\)</span> and <span class="math">\(\Delta_j^{(m+1)} \leftarrow \Delta_j^{(m+1)} - \lambda_l^{(m+1)}\)</span></li>
</ol></li>
</ol></li>
<li>Return <span class="math">\(u_i = \Delta_i + x_i\)</span> for all <span class="math">\(i\)</span>
</div>
</li>
</ol>
<p>Since we can easily construct the primal variables from the dual variables and can evaluate the primal and dual functions in closed form, we can use the duality gap to determine when we are converged.</p>
<h1 id="performance">Performance</h1>
<p>TODO compare again previous methods</p>
<h1 id="references">References</h1>
<p>The original formulation for convex clustering was introduced by <a href="http://www.control.isy.liu.se/research/reports/2011/2992.pdf">Lindsten et al.</a> and <a href="http://www.icml-2011.org/papers/419_icmlpaper.pdf">Hocking et al.</a>. <a href="http://arxiv.org/abs/1304.0499">Chi et al.</a> introduced ADMM and AMA-based algorithms specifically designed for convex clustering.</p>
    </article>
  </div>
</div> <!-- row -->
<!-- Disqus Comments -->
<div class="row-fluid disqus">
  <div id="disqus_thread"></div>
  <script type="text/javascript">
      /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
      var disqus_shortname = 'duckworthd-blog'; // required: replace example with your forum shortname

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function() {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>

    </div> <!-- container -->
    <!-- jquery -->
<script type="text/javascript"
        src="http://code.jquery.com/jquery-1.9.0.min.js"></script>

<!-- MathJax -->
<script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      //equationNumbers: {
      //  autoNumber: "all"
      //},
      extensions: ["color.js"]
    }
  });
</script>

<!-- Bootstrap -->
<script type="text/javascript"
        src="/assets/js/bootstrap.min.js"></script>

<!-- Lightbox -->
<script type="text/javascript"
        src="/assets/js/lightbox.js"></script>

  </body>
</html>

