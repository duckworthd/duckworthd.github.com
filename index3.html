<!DOCTYPE html>
<html lang="en">
<head>
  <!-- stylesheets -->
	<link rel="stylesheet" type="text/css" href="/theme/css/style.css">
	<link rel="stylesheet" type="text/css" href="/theme/css/pygments.css">
	<link rel="stylesheet" type="text/css" href="/theme/lightbox/css/lightbox.css">
	<link rel="stylesheet" type="text/css" href="/theme/font-awesome-4.3.0/css/font-awesome.min.css">

  <!-- fonts -->
	<link href='http://fonts.googleapis.com/css?family=Open+Sans:800,400,300|Inconsolata' rel='stylesheet' type='text/css'>
	<link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:700,900' rel='stylesheet' type='text/css'>
	<link href='http://fonts.googleapis.com/css?family=Lora:700' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Playball' rel='stylesheet' type='text/css'>

  <!-- RSS/ATOM feeds -->
	<link href="None/" type="application/atom+xml" rel="alternate" title="Strongly Convex ATOM Feed" />


		<title>Strongly Convex</title>
		<meta charset="utf-8" />
</head>
<body>
	<section id="sidebar">
		<figure id="user_logo">
      <a href=""><div class="logo">&nbsp;</div></a>
		</figure>

		<div class="user_meta">
      <h1 id="user">
        <a  href="/"
            class="sitename">
          Strongly Convex
        </a>
      </h1>
			<h2></h2>


			<ul>
        <hr></hr>
					<li><a href="/">Words</a></li>
          <hr></hr>
					<li><a href="/code.html">Code</a></li>
          <hr></hr>
					<li><a href="/about.html">About</a></li>
          <hr></hr>
			</ul>
		</div>
	</section>

	<section id="posts">
  <dl>
    <ol>
        <li>

          <h1 class="title">
            <a  href="/blog/subgradient-descent.html"
                rel="bookmark"
                title="Permalink to Subgradient Descent"
                >
              Subgradient Descent
            </a>
          </h1>

          <abbr class="published"
              title="2013-04-11T00:00:00-07:00"
              >
            Thu 11 April 2013
          </abbr>

          <div class="entry-content">
            <p>Not far from <a href="/blog/gradient-descent.html">Gradient Descent</a> is another first-order
descent algorithm (that is, an algorithm that only relies on the first
derivative) is Subgradient Descent. In implementation, they are in fact
identical. The only difference is on the assumptions placed on the objective
function we wish to minimize, $f(x)$.  If …</p>
          </div>

        </li>
        <li>

          <h1 class="title">
            <a  href="/blog/gradient-descent.html"
                rel="bookmark"
                title="Permalink to Gradient Descent"
                >
              Gradient Descent
            </a>
          </h1>

          <abbr class="published"
              title="2013-04-10T00:00:00-07:00"
              >
            Wed 10 April 2013
          </abbr>

          <div class="entry-content">
            <p>Gradient Descent is perhaps the most intuitive of all optimization
algorithms. Imagine you're standing on the side of a mountain and want to reach
the bottom. You'd probably do something like this,</p>
<div class="pseudocode">
<ol>
<li>Look around you and see which way points the most downwards</li>
<li>Take a step in that direction, then …</li></ol></div>
          </div>

        </li>
        <li>

          <h1 class="title">
            <a  href="/blog/topic-models-arent-hard.html"
                rel="bookmark"
                title="Permalink to Topic Models aren't hard"
                >
              Topic Models aren't hard
            </a>
          </h1>

          <abbr class="published"
              title="2013-01-21T00:00:00-08:00"
              >
            Mon 21 January 2013
          </abbr>

          <div class="entry-content">
            <p>In 2002, <a href="http://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf">Latent Dirichlet Allocation</a> (LDA) was published at NIPS, one
of the most highly regarded conferences for research loosely labeled as
"Artificial Intelligence". The next 5 or so years led to a flurry of
incremental model extensions and alternative inference methods, though none
have achieved the popularity of their …</p>
          </div>

        </li>
        <li>

          <h1 class="title">
            <a  href="/blog/admm.html"
                rel="bookmark"
                title="Permalink to ADMM: parallelizing convex optimization"
                >
              ADMM: parallelizing convex optimization
            </a>
          </h1>

          <abbr class="published"
              title="2012-06-24T00:00:00-07:00"
              >
            Sun 24 June 2012
          </abbr>

          <div class="entry-content">
            <p>In the previous post, we considered Stochastic Gradient Descent, a popular method for optimizing "separable" functions (that is, functions that are purely sums of other functions) in a large, distributed environment. However, Stochastic Gradient Descent is not the only algorithm out there.</p>
<p>So why consider anything else? First of all …</p>
          </div>

        </li>
        <li>

          <h1 class="title">
            <a  href="/blog/sparse-l2.html"
                rel="bookmark"
                title="Permalink to Stochastic Gradient Descent and Sparse $L_2$ regularization"
                >
              Stochastic Gradient Descent and Sparse $L_2$ regularization
            </a>
          </h1>

          <abbr class="published"
              title="2012-05-10T00:00:00-07:00"
              >
            Thu 10 May 2012
          </abbr>

          <div class="entry-content">
            <p>Suppose you’re doing some typical supervised learning on a gigantic dataset where the total loss over all samples for parameter $w$ is simply the sum of the losses of each sample $i$, i.e.,</p>
<p>$$
  L(w) = \sum_{i} l(x_i, y_i, w)
$$</p>
<p>Basically any loss function you can think …</p>
          </div>

        </li>
    </ol>
  </dl>

<p class="paginator">
			<a href="/index2.html" class="button_accent">&larr;&nbsp;&nbsp;&nbsp;newer</a>
</p>
	</section>


  <!-- scripts -->
  <script
    type= "text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    >
  </script>
  <script
    type= "text/javascript"
    src="/theme/mathjax/MathJaxLocal.js"
    >
  </script>
  <script
      type="text/javascript"
      src="/theme/lightbox/js/jquery-1.11.0.min.js"
      >
  </script>
  <script
      type="text/javascript"
      src="/theme/lightbox/js/lightbox.min.js"
      >
  </script>
</body>
</html>