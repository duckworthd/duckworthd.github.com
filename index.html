<!DOCTYPE html>
<html lang="en">
<head>
  <!-- stylesheets -->
	<link rel="stylesheet" type="text/css" href="/theme/css/style.css">
	<link rel="stylesheet" type="text/css" href="/theme/css/pygments.css">
	<link rel="stylesheet" type="text/css" href="/theme/lightbox/css/lightbox.css">
	<link rel="stylesheet" type="text/css" href="/theme/font-awesome-4.3.0/css/font-awesome.min.css">

  <!-- fonts -->
	<link href='http://fonts.googleapis.com/css?family=Open+Sans:800,400,300|Inconsolata' rel='stylesheet' type='text/css'>
	<link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:700,900' rel='stylesheet' type='text/css'>
	<link href='http://fonts.googleapis.com/css?family=Lora:700' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Playball' rel='stylesheet' type='text/css'>

  <!-- RSS/ATOM feeds -->
	<link href="None/" type="application/atom+xml" rel="alternate" title="Strongly Convex ATOM Feed" />


		<title>Strongly Convex</title>
		<meta charset="utf-8" />
</head>
<body>
	<section id="sidebar">
		<figure id="user_logo">
      <a href=""><div class="logo">&nbsp;</div></a>
		</figure>

		<div class="user_meta">
      <h1 id="user">
        <a  href="/"
            class="sitename">
          Strongly Convex
        </a>
      </h1>
			<h2></h2>


			<ul>
        <hr></hr>
					<li><a href="/">Words</a></li>
          <hr></hr>
					<li><a href="/code.html">Code</a></li>
          <hr></hr>
					<li><a href="/about.html">About</a></li>
          <hr></hr>
			</ul>
		</div>
		<footer>
			<address>
				Powered by <a href="http://pelican.notmyidea.org/">Pelican</a>,
		    Theme by <a href="https://github.com/wting/pelican-svbtle">wting</a>
        with edits by <a href="https://github.com/duckworthd">duckworthd</a>
			</address>
		</footer>
	</section>

	<section id="posts">
  <dl>
    <ol>
        <li>

          <h1 class="title">
            <a  href="/blog/big-table-of-convergence-rates.html"
                rel="bookmark"
                title="Permalink to The Big Table of Convergence Rates"
                >
              The Big Table of Convergence Rates
            </a>
          </h1>

          <abbr class="published"
              title="2014-08-01T00:00:00-07:00"
              >
            Fri 01 August 2014
          </abbr>

          <div class="entry-content">
            <p>In the past 50+ years of convex optimization research, a great many
algorithms have been developed, each with slight nuances to their assumptions,
implementations, and guarantees. In this article, I'll give a shorthand
comparison of these methods in terms of the number of iterations required
to reach a desired ...</p>
          </div>

        </li>
        <li>

          <h1 class="title">
            <a  href="/blog/admm-to-prox-grad.html"
                rel="bookmark"
                title="Permalink to From ADMM to Proximal Gradient Descent"
                >
              From ADMM to Proximal Gradient Descent
            </a>
          </h1>

          <abbr class="published"
              title="2014-07-26T00:00:00-07:00"
              >
            Sat 26 July 2014
          </abbr>

          <div class="entry-content">
            <p>At first blush, <a href="/blog/admm.html">ADMM</a> and <a href="/blog/proximal-gradient-descent.html">Proximal Gradient Descent</a>
(ProxGrad) appear to have very little in common. The convergence analyses for
these two methods are unrelated, and the former operates on an Augmented
Lagrangian while the latter directly minimizes the primal objective. In this
post, we'll show that after a ...</p>
          </div>

        </li>
        <li>

          <h1 class="title">
            <a  href="/blog/admm-revisited.html"
                rel="bookmark"
                title="Permalink to ADMM revisited"
                >
              ADMM revisited
            </a>
          </h1>

          <abbr class="published"
              title="2014-07-20T00:00:00-07:00"
              >
            Sun 20 July 2014
          </abbr>

          <div class="entry-content">
            <p>When I originally wrote about the <a href="/blog/admm.html">Alternating Direction Method of
Multipliers</a> algorithm, the community's understanding of its
convergence properties was light to say the least. While it has long been
known (See <a href="http://web.stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf">Boyd's excellent article</a>, Appendix A) that ADMM <em>will</em>
converge, it is only recently that the community ...</p>
          </div>

        </li>
        <li>

          <h1 class="title">
            <a  href="/blog/frank-wolfe.html"
                rel="bookmark"
                title="Permalink to Frank-Wolfe Algorithm"
                >
              Frank-Wolfe Algorithm
            </a>
          </h1>

          <abbr class="published"
              title="2013-05-04T00:00:00-07:00"
              >
            Sat 04 May 2013
          </abbr>

          <div class="entry-content">
            <p>In this post, we'll take a look at the <a href="http://en.wikipedia.org/wiki/Frank%E2%80%93Wolfe_algorithm">Frank-Wolfe Algorithm</a>
also known as the Conditional Gradient Method, an algorithm particularly suited
for solving problems with compact domains. Like the <a href="/blog/proximal-gradient-descent.html">Proximal
Gradient</a> and <a href="/blog/accelerated-proximal-gradient-descent.html">Accelerated Proximal
Gradient</a> algorithms, Frank-Wolfe requires we
exploit problem structure to quickly solve a mini-optimization problem ...</p>
          </div>

        </li>
        <li>

          <h1 class="title">
            <a  href="/blog/variational-inference.html"
                rel="bookmark"
                title="Permalink to Variational Inference"
                >
              Variational Inference
            </a>
          </h1>

          <abbr class="published"
              title="2013-04-28T00:00:00-07:00"
              >
            Sun 28 April 2013
          </abbr>

          <div class="entry-content">
            <p><a href="http://www.orchid.ac.uk/eprints/40/1/fox_vbtut.pdf">Variational Inference</a> and Monte Carlo Sampling are
currently the two chief ways of doing approximate Bayesian inference. In the
Bayesian setting, we typically have some observed variables <mathjax>$x$</mathjax> and
unobserved variables <mathjax>$z$</mathjax>, and our goal is to calculate <mathjax>$P(z|x)$</mathjax>. In all but
the simplest cases, calculating <mathjax>$P(z ...</mathjax></p>
          </div>

        </li>
    </ol>
  </dl>

<p class="paginator">
		<a href="/index2.html" class="button_accent" style="position: absolute; right: 0;">continue&nbsp;&nbsp;&nbsp;&rarr;</a>
</p>
	</section>


  <!-- scripts -->
  <script
    type= "text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    >
  </script>
  <script
    type= "text/javascript"
    src="/theme/mathjax/MathJaxLocal.js"
    >
  </script>
  <script
      type="text/javascript"
      src="/theme/lightbox/js/jquery-1.11.0.min.js"
      >
  </script>
  <script
      type="text/javascript"
      src="/theme/lightbox/js/lightbox.min.js"
      >
  </script>
</body>
</html>