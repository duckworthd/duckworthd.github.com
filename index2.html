<!DOCTYPE html>
<html lang="en">
<head>
  <!-- enable responsive design -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- stylesheets -->
  <link rel="stylesheet" type="text/css" href="/theme/css/style.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/pygments.css">
  <link rel="stylesheet" type="text/css" href="/theme/lightbox/css/lightbox.css">
  <link rel="stylesheet" type="text/css" href="/theme/font-awesome-4.3.0/css/font-awesome.min.css">

  <!-- fonts -->
  <link href='http://fonts.googleapis.com/css?family=Open+Sans:800,400,300|Inconsolata' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:700,900' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Lora:700' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Playball' rel='stylesheet' type='text/css'>

  <!-- No RSS/ATOM feeds -->


    <title>Strongly Convex</title>
    <meta charset="utf-8" />
</head>
<body>
  <div class="row">
    <div id="sidebar">
      <figure id="user_logo">
        <a href=""><div class="logo">&nbsp;</div></a>
      </figure>

      <div class="user_meta">
        <h1 id="user">
          <a  href="/"
              class="sitename">
            Strongly Convex
          </a>
        </h1>
        <h2></h2>

        <ul>
          <hr></hr>
            <li><a href="/">Blog</a></li>
            <hr></hr>
            <li><a href="/about.html">About</a></li>
            <hr></hr>
        </ul>
      </div>
    </div>

    <div id="posts">
  <dl>
    <ol>
        <li>

          <h1 class="title">
            <a  href="/blog/accelerated-proximal-gradient-descent.html"
                rel="bookmark"
                title="Permalink to Accelerated Proximal Gradient Descent"
                >
              Accelerated Proximal Gradient Descent
            </a>
          </h1>

          <abbr class="published"
              title="2013-04-25T00:00:00-07:00"
              >
            Thu 25 April 2013
          </abbr>

          <div class="entry-content">
            <p><span class="math">\(\def\prox{\text{prox}}\)</span>
  In a <a href="http://stronglyconvex.com/blog/proximal-gradient-descent.html">previous post</a>, I presented Proximal Gradient, a
method for bypassing the <span class="math">\(O(1 / \epsilon^2)\)</span> convergence rate of Subgradient
Descent.  This method relied on assuming that the objective function could be
expressed as the sum of 2 functions, <span class="math">\(g(x)\)</span> and <span class="math">\(h(x)\)</span>, with …</p>
          </div>

        </li>
        <li>

          <h1 class="title">
            <a  href="/blog/coordinate-ascent-convex-clustering.html"
                rel="bookmark"
                title="Permalink to Coordinate Ascent for Convex Clustering"
                >
              Coordinate Ascent for Convex Clustering
            </a>
          </h1>

          <abbr class="published"
              title="2013-04-23T00:00:00-07:00"
              >
            Tue 23 April 2013
          </abbr>

          <div class="entry-content">
            <p>Convex clustering is the reformulation of k-means clustering as a convex
problem. While the two problems are not equivalent, the former can be seen as a
relaxation of the latter that allows us to easily find globally optimal
solutions (as opposed to only locally optimal ones).</p>
<p>Suppose we have a …</p>
          </div>

        </li>
        <li>

          <h1 class="title">
            <a  href="/blog/l1-sparsity.html"
                rel="bookmark"
                title="Permalink to Why does L1 produce sparse solutions?"
                >
              Why does L1 produce sparse solutions?
            </a>
          </h1>

          <abbr class="published"
              title="2013-04-22T00:00:00-07:00"
              >
            Mon 22 April 2013
          </abbr>

          <div class="entry-content">
            <p>Supervised machine learning problems are typically of the form "minimize your
error while regularizing your parameters." The idea is that while many choices
of parameters may make your training error low, the goal isn't low training
error -- it's low test-time error. Thus, parameters should be minimize training
error while remaining …</p>
          </div>

        </li>
        <li>

          <h1 class="title">
            <a  href="/blog/proximal-gradient-descent.html"
                rel="bookmark"
                title="Permalink to Proximal Gradient Descent"
                >
              Proximal Gradient Descent
            </a>
          </h1>

          <abbr class="published"
              title="2013-04-19T00:00:00-07:00"
              >
            Fri 19 April 2013
          </abbr>

          <div class="entry-content">
            <p><span class="math">\( \def\prox{\text{prox}} $
  In a [previous post][subgradient_descent_usage], I mentioned that one cannot
hope to asymptotically outperform the $O(\frac{1}{\epsilon^2})\)</span> convergence
rate of Subgradient Descent when dealing with a non-differentiable objective
function. This is in fact only half-true; Subgradient Descent cannot be beat
<em>using only first-order …</em></p>
          </div>

        </li>
        <li>

          <h1 class="title">
            <a  href="/blog/accelerated-gradient-descent.html"
                rel="bookmark"
                title="Permalink to Accelerated Gradient Descent"
                >
              Accelerated Gradient Descent
            </a>
          </h1>

          <abbr class="published"
              title="2013-04-12T00:00:00-07:00"
              >
            Fri 12 April 2013
          </abbr>

          <div class="entry-content">
            <p>In the mid-1980s, Yurii Nesterov hit the equivalent of an academic home run.
At the same time, he established the Accelerated Gradient Method, proved that
its convergence rate superior to Gradient Descent (<span class="math">\(O(1/\sqrt{\epsilon})\)</span>
iterations instead of <span class="math">\(O(1/\epsilon)\)</span>), and then proved that no other
first-order (that …</p>
          </div>

        </li>
    </ol>
  </dl>

<p class="paginator">
			<a href="/index.html" class="button_accent">&larr;&nbsp;&nbsp;&nbsp;newer</a>
		<a href="/index3.html" class="button_accent" style="position: absolute; right: 0;">continue&nbsp;&nbsp;&nbsp;&rarr;</a>
</p>
    </div>
  </div>


  <!-- scripts -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script
    type= "text/javascript"
    src="/theme/mathjax/MathJaxLocal.js"
    >
  </script>
  <script
      type="text/javascript"
      src="/theme/lightbox/js/jquery-1.11.0.min.js"
      >
  </script>
  <script
      type="text/javascript"
      src="/theme/lightbox/js/lightbox.min.js"
      >
  </script>
</body>
</html>