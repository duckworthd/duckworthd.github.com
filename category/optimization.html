<!DOCTYPE html>
<html lang="en">
<head>
  <!-- stylesheets -->
	<link rel="stylesheet" type="text/css" href="http://stronglyconvex.com/theme/css/style.css">
	<link rel="stylesheet" type="text/css" href="http://stronglyconvex.com/theme/css/pygments.css">
	<link rel="stylesheet" type="text/css" href="http://stronglyconvex.com/assets/lightbox/css/lightbox.css">

  <!-- fonts -->
	<link href='http://fonts.googleapis.com/css?family=Open+Sans:800,400,300|Inconsolata' rel='stylesheet' type='text/css'>
	<link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:700,900' rel='stylesheet' type='text/css'>
	<link href='http://fonts.googleapis.com/css?family=Lora:700' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Playball' rel='stylesheet' type='text/css'>
	<link href='//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css' rel='stylesheet' type='text/css'>

  <!-- RSS/ATOM feeds -->
	<link href="http://stronglyconvex.com/" type="application/atom+xml" rel="alternate" title="Strongly Convex ATOM Feed" />


		<title>Strongly Convex</title>
		<meta charset="utf-8" />
</head>
<body>
	<section id="sidebar">
		<figure id="user_logo">
      <a href="http://stronglyconvex.com"><div class="logo">&nbsp;</div></a>
		</figure>

		<div class="user_meta">
      <h1 id="user">
        <a  href="http://stronglyconvex.com/"
            class="sitename">
          Strongly Convex
        </a>
      </h1>
			<h2></h2>


			<ul>
        <hr></hr>
					<li><a href="/">Words</a></li>
          <hr></hr>
					<li><a href="/code.html">Code</a></li>
          <hr></hr>
					<li><a href="/about.html">About</a></li>
          <hr></hr>
			</ul>
		</div>
		<footer>
			<address>
				Powered by <a href="http://pelican.notmyidea.org/">Pelican</a>,
		    Theme by <a href="https://github.com/wting/pelican-svbtle">wting</a>.
			</address>
		</footer>
	</section>

	<section id="posts">
	<dl>
		<ol>
				<li>

					<h1 class="title">
						<a  href="http://stronglyconvex.com/blog/admm-revisited.html"
                rel="bookmark"
                title="Permalink to ADMM revisited"
                >
              ADMM revisited
            </a>
					</h1>

					<abbr class="published"
              title="2013-07-06T00:00:00"
              >
            Sat 06 July 2013
          </abbr>

				  <div class="entry-content">
            <p>When I originally wrote about the <a href="http://stronglyconvex.com/blog/admm.html">Alternating Direction Method of
Multipliers</a> algorithm, the community's understanding of its
convergence properties was light to say the least. While it has long been
known (See <a href="http://web.stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf">Boyd's excellent article</a>, Appendix A) that ADMM <em>will</em>
converge, it is only recently that the community ...</p>
          </div>

				</li>
				<li>

					<h1 class="title">
						<a  href="http://stronglyconvex.com/blog/frank-wolfe.html"
                rel="bookmark"
                title="Permalink to Frank-Wolfe Algorithm"
                >
              Frank-Wolfe Algorithm
            </a>
					</h1>

					<abbr class="published"
              title="2013-05-04T00:00:00"
              >
            Sat 04 May 2013
          </abbr>

				  <div class="entry-content">
            <p>In this post, we'll take a look at the <a href="http://en.wikipedia.org/wiki/Frank%E2%80%93Wolfe_algorithm">Frank-Wolfe Algorithm</a>
also known as the Conditional Gradient Method, an algorithm particularly suited
for solving problems with compact domains. Like the <a href="http://stronglyconvex.com/blog/proximal-gradient-descent.html">Proximal
Gradient</a> and <a href="http://stronglyconvex.com/blog/accelerated-proximal-gradient-descent.html">Accelerated Proximal
Gradient</a> algorithms, Frank-Wolfe requires we
exploit problem structure to quickly solve a mini-optimization problem ...</p>
          </div>

				</li>
				<li>

					<h1 class="title">
						<a  href="http://stronglyconvex.com/blog/variational-inference.html"
                rel="bookmark"
                title="Permalink to Variational Inference"
                >
              Variational Inference
            </a>
					</h1>

					<abbr class="published"
              title="2013-04-28T00:00:00"
              >
            Sun 28 April 2013
          </abbr>

				  <div class="entry-content">
            <p><a href="http://www.orchid.ac.uk/eprints/40/1/fox_vbtut.pdf">Variational Inference</a> and Monte Carlo Sampling are
currently the two chief ways of doing approximate Bayesian inference. In the
Bayesian setting, we typically have some observed variables <mathjax>$x$</mathjax> and
unobserved variables <mathjax>$z$</mathjax>, and our goal is to calculate <mathjax>$P(z|x)$</mathjax>. In all but
the simplest cases, calculating <mathjax>$P(z ...</mathjax></p>
          </div>

				</li>
				<li>

					<h1 class="title">
						<a  href="http://stronglyconvex.com/blog/accelerated-proximal-gradient-descent.html"
                rel="bookmark"
                title="Permalink to Accelerated Proximal Gradient Descent"
                >
              Accelerated Proximal Gradient Descent
            </a>
					</h1>

					<abbr class="published"
              title="2013-04-25T00:00:00"
              >
            Thu 25 April 2013
          </abbr>

				  <div class="entry-content">
            <p><mathjax>$\def\prox{\text{prox}}$</mathjax>
  In a <a href="http://stronglyconvex.com/blog/proximal-gradient-descent.html">previous post</a>, I presented Proximal Gradient, a
method for bypassing the <mathjax>$O(1 / \epsilon^2)$</mathjax> convergence rate of Subgradient
Descent.  This method relied on assuming that the objective function could be
expressed as the sum of 2 functions, <mathjax>$g(x)$</mathjax> and <mathjax>$h(x)$</mathjax>, with ...</p>
          </div>

				</li>
				<li>

					<h1 class="title">
						<a  href="http://stronglyconvex.com/blog/coordinate-ascent-convex-clustering.html"
                rel="bookmark"
                title="Permalink to Coordinate Ascent for Convex Clustering"
                >
              Coordinate Ascent for Convex Clustering
            </a>
					</h1>

					<abbr class="published"
              title="2013-04-23T00:00:00"
              >
            Tue 23 April 2013
          </abbr>

				  <div class="entry-content">
            <p>Convex clustering is the reformulation of k-means clustering as a convex
problem. While the two problems are not equivalent, the former can be seen as a
relaxation of the latter that allows us to easily find globally optimal
solutions (as opposed to only locally optimal ones).</p>
<p>Suppose we have a ...</p>
          </div>

				</li>
				<li>

					<h1 class="title">
						<a  href="http://stronglyconvex.com/blog/l1-sparsity.html"
                rel="bookmark"
                title="Permalink to Why does L1 produce sparse solutions?"
                >
              Why does L1 produce sparse solutions?
            </a>
					</h1>

					<abbr class="published"
              title="2013-04-22T00:00:00"
              >
            Mon 22 April 2013
          </abbr>

				  <div class="entry-content">
            <p>Supervised machine learning problems are typically of the form "minimize your
error while regularizing your parameters." The idea is that while many choices
of parameters may make your training error low, the goal isn't low training
error -- it's low test-time error. Thus, parameters should be minimize training
error ...</p>
          </div>

				</li>
				<li>

					<h1 class="title">
						<a  href="http://stronglyconvex.com/blog/proximal-gradient-descent.html"
                rel="bookmark"
                title="Permalink to Proximal Gradient Descent"
                >
              Proximal Gradient Descent
            </a>
					</h1>

					<abbr class="published"
              title="2013-04-19T00:00:00"
              >
            Fri 19 April 2013
          </abbr>

				  <div class="entry-content">
            <p><mathjax>$ \def\prox{\text{prox}} $</mathjax>
  In a <a href="/blog/subgradient-descent.html#usage">previous post</a>, I mentioned that one cannot
hope to asymptotically outperform the <mathjax>$O(\frac{1}{\epsilon^2})$</mathjax> convergence
rate of Subgradient Descent when dealing with a non-differentiable objective
function. This is in fact only half-true; Subgradient Descent cannot be beat
<em>using only first-order information ...</em></p>
          </div>

				</li>
				<li>

					<h1 class="title">
						<a  href="http://stronglyconvex.com/blog/accelerated-gradient-descent.html"
                rel="bookmark"
                title="Permalink to Accelerated Gradient Descent"
                >
              Accelerated Gradient Descent
            </a>
					</h1>

					<abbr class="published"
              title="2013-04-12T00:00:00"
              >
            Fri 12 April 2013
          </abbr>

				  <div class="entry-content">
            <p>In the mid-1980s, Yurii Nesterov hit the equivalent of an academic home run.
At the same time, he established the Accelerated Gradient Method, proved that
its convergence rate superior to Gradient Descent (<mathjax>$O(1/\sqrt{\epsilon})$</mathjax>
iterations instead of <mathjax>$O(1/\epsilon)$</mathjax>), and then proved that no other
first-order (that ...</p>
          </div>

				</li>
				<li>

					<h1 class="title">
						<a  href="http://stronglyconvex.com/blog/subgradient-descent.html"
                rel="bookmark"
                title="Permalink to Subgradient Descent"
                >
              Subgradient Descent
            </a>
					</h1>

					<abbr class="published"
              title="2013-04-11T00:00:00"
              >
            Thu 11 April 2013
          </abbr>

				  <div class="entry-content">
            <p>Not far from <a href="http://stronglyconvex.com/blog/gradient-descent.html">Gradient Descent</a> is another first-order
descent algorithm (that is, an algorithm that only relies on the first
derivative) is Subgradient Descent. In implementation, they are in fact
identical. The only difference is on the assumptions placed on the objective
function we wish to minimize, <mathjax>$f(x)$</mathjax>.  If ...</p>
          </div>

				</li>
				<li>

					<h1 class="title">
						<a  href="http://stronglyconvex.com/blog/gradient-descent.html"
                rel="bookmark"
                title="Permalink to Gradient Descent"
                >
              Gradient Descent
            </a>
					</h1>

					<abbr class="published"
              title="2013-04-10T00:00:00"
              >
            Wed 10 April 2013
          </abbr>

				  <div class="entry-content">
            <p>Gradient Descent is perhaps the most intuitive of all optimization
algorithms. Imagine you're standing on the side of a mountain and want to reach
the bottom. You'd probably do something like this,</p>
<div class="pseudocode">
<ol>
<li>Look around you and see which way points the most downwards</li>
<li>Take a step in that ...</li></ol></div>
          </div>

				</li>
				<li>

					<h1 class="title">
						<a  href="http://stronglyconvex.com/blog/admm.html"
                rel="bookmark"
                title="Permalink to ADMM: parallelizing convex optimization"
                >
              ADMM: parallelizing convex optimization
            </a>
					</h1>

					<abbr class="published"
              title="2012-06-24T00:00:00"
              >
            Sun 24 June 2012
          </abbr>

				  <div class="entry-content">
            <p>In the previous post, we considered Stochastic Gradient Descent, a popular method for optimizing "separable" functions (that is, functions that are purely sums of other functions) in a large, distributed environment. However, Stochastic Gradient Descent is not the only algorithm out there.</p>
<p>So why consider anything else? First of all ...</p>
          </div>

				</li>
				<li>

					<h1 class="title">
						<a  href="http://stronglyconvex.com/blog/sparse-l2.html"
                rel="bookmark"
                title="Permalink to Stochastic Gradient Descent and Sparse $L_2$ regularization"
                >
              Stochastic Gradient Descent and Sparse $L_2$ regularization
            </a>
					</h1>

					<abbr class="published"
              title="2012-05-10T00:00:00"
              >
            Thu 10 May 2012
          </abbr>

				  <div class="entry-content">
            <p>Suppose you’re doing some typical supervised learning on a gigantic dataset where the total loss over all samples for parameter <mathjax>$w$</mathjax> is simply the sum of the losses of each sample <mathjax>$i$</mathjax>, i.e.,</p>
<p><mathjax>$$
  L(w) = \sum_{i} l(x_i, y_i, w)
$$</mathjax></p>
<p>Basically any loss function you can think ...</p>
          </div>

				</li>
		</ol>
	</dl>

	</section>


  <!-- scripts -->
  <script
    type= "text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    >
  </script>
  <script
    type= "text/javascript"
    src="http://stronglyconvex.com/assets/js/MathJaxLocal.js"
    >
  </script>
  <script
      type="text/javascript"
      src="http://stronglyconvex.com/assets/lightbox/js/jquery-1.11.0.min.js"
      >
  </script>
  <script
      type="text/javascript"
      src="http://stronglyconvex.com/assets/lightbox/js/lightbox.min.js"
      >
  </script>
</body>
</html>