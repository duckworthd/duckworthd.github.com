<!DOCTYPE html>
<html lang="en">
<head>
  <!-- stylesheets -->
	<link rel="stylesheet" type="text/css" href="/theme/css/style.css">
	<link rel="stylesheet" type="text/css" href="/theme/css/pygments.css">
	<link rel="stylesheet" type="text/css" href="/theme/lightbox/css/lightbox.css">
	<link rel="stylesheet" type="text/css" href="/theme/font-awesome-4.3.0/css/font-awesome.min.css">

  <!-- fonts -->
	<link href='http://fonts.googleapis.com/css?family=Open+Sans:800,400,300|Inconsolata' rel='stylesheet' type='text/css'>
	<link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:700,900' rel='stylesheet' type='text/css'>
	<link href='http://fonts.googleapis.com/css?family=Lora:700' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Playball' rel='stylesheet' type='text/css'>

  <!-- RSS/ATOM feeds -->
	<link href="None/" type="application/atom+xml" rel="alternate" title="Strongly Convex ATOM Feed" />


		<title>Strongly Convex - first-order</title>
		<meta charset="utf-8" />
</head>
<body>
	<section id="sidebar">
		<figure id="user_logo">
      <a href=""><div class="logo">&nbsp;</div></a>
		</figure>

		<div class="user_meta">
      <h1 id="user">
        <a  href="/"
            class="sitename">
          Strongly Convex
        </a>
      </h1>
			<h2></h2>


			<ul>
        <hr></hr>
					<li><a href="/">Words</a></li>
          <hr></hr>
					<li><a href="/code.html">Code</a></li>
          <hr></hr>
					<li><a href="/about.html">About</a></li>
          <hr></hr>
			</ul>
		</div>
	</section>

	<section id="posts">
  <dl>
    <ol>
        <li>

          <h1 class="title">
            <a  href="/blog/frank-wolfe.html"
                rel="bookmark"
                title="Permalink to Frank-Wolfe Algorithm"
                >
              Frank-Wolfe Algorithm
            </a>
          </h1>

          <abbr class="published"
              title="2013-05-04T00:00:00-07:00"
              >
            Sat 04 May 2013
          </abbr>

          <div class="entry-content">
            <p>In this post, we'll take a look at the <a href="http://en.wikipedia.org/wiki/Frank%E2%80%93Wolfe_algorithm">Frank-Wolfe Algorithm</a>
also known as the Conditional Gradient Method, an algorithm particularly suited
for solving problems with compact domains. Like the <a href="/blog/proximal-gradient-descent.html">Proximal
Gradient</a> and <a href="/blog/accelerated-proximal-gradient-descent.html">Accelerated Proximal
Gradient</a> algorithms, Frank-Wolfe requires we
exploit problem structure to quickly solve a mini-optimization problem. Our …</p>
          </div>

        </li>
        <li>

          <h1 class="title">
            <a  href="/blog/accelerated-proximal-gradient-descent.html"
                rel="bookmark"
                title="Permalink to Accelerated Proximal Gradient Descent"
                >
              Accelerated Proximal Gradient Descent
            </a>
          </h1>

          <abbr class="published"
              title="2013-04-25T00:00:00-07:00"
              >
            Thu 25 April 2013
          </abbr>

          <div class="entry-content">
            <p><span class="math">\(\def\prox{\text{prox}}\)</span>
  In a <a href="/blog/proximal-gradient-descent.html">previous post</a>, I presented Proximal Gradient, a
method for bypassing the <span class="math">\(O(1 / \epsilon^2)\)</span> convergence rate of Subgradient
Descent.  This method relied on assuming that the objective function could be
expressed as the sum of 2 functions, <span class="math">\(g(x)\)</span> and <span class="math">\(h(x)\)</span>, with …</p>
          </div>

        </li>
        <li>

          <h1 class="title">
            <a  href="/blog/proximal-gradient-descent.html"
                rel="bookmark"
                title="Permalink to Proximal Gradient Descent"
                >
              Proximal Gradient Descent
            </a>
          </h1>

          <abbr class="published"
              title="2013-04-19T00:00:00-07:00"
              >
            Fri 19 April 2013
          </abbr>

          <div class="entry-content">
            <p><span class="math">\( \def\prox{\text{prox}} $
  In a [previous post][subgradient_descent_usage], I mentioned that one cannot
hope to asymptotically outperform the $O(\frac{1}{\epsilon^2})\)</span> convergence
rate of Subgradient Descent when dealing with a non-differentiable objective
function. This is in fact only half-true; Subgradient Descent cannot be beat
<em>using only first-order …</em></p>
          </div>

        </li>
        <li>

          <h1 class="title">
            <a  href="/blog/accelerated-gradient-descent.html"
                rel="bookmark"
                title="Permalink to Accelerated Gradient Descent"
                >
              Accelerated Gradient Descent
            </a>
          </h1>

          <abbr class="published"
              title="2013-04-12T00:00:00-07:00"
              >
            Fri 12 April 2013
          </abbr>

          <div class="entry-content">
            <p>In the mid-1980s, Yurii Nesterov hit the equivalent of an academic home run.
At the same time, he established the Accelerated Gradient Method, proved that
its convergence rate superior to Gradient Descent (<span class="math">\(O(1/\sqrt{\epsilon})\)</span>
iterations instead of <span class="math">\(O(1/\epsilon)\)</span>), and then proved that no other
first-order (that …</p>
          </div>

        </li>
        <li>

          <h1 class="title">
            <a  href="/blog/subgradient-descent.html"
                rel="bookmark"
                title="Permalink to Subgradient Descent"
                >
              Subgradient Descent
            </a>
          </h1>

          <abbr class="published"
              title="2013-04-11T00:00:00-07:00"
              >
            Thu 11 April 2013
          </abbr>

          <div class="entry-content">
            <p>Not far from <a href="/blog/gradient-descent.html">Gradient Descent</a> is another first-order
descent algorithm (that is, an algorithm that only relies on the first
derivative) is Subgradient Descent. In implementation, they are in fact
identical. The only difference is on the assumptions placed on the objective
function we wish to minimize, <span class="math">\(f(x)\)</span>.  If …</p>
          </div>

        </li>
    </ol>
  </dl>

<p class="paginator">
		<a href="/tag/first-order2.html" class="button_accent" style="position: absolute; right: 0;">continue&nbsp;&nbsp;&nbsp;&rarr;</a>
</p>
	</section>


  <!-- scripts -->
  <script
    type= "text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    >
  </script>
  <script
    type= "text/javascript"
    src="/theme/mathjax/MathJaxLocal.js"
    >
  </script>
  <script
      type="text/javascript"
      src="/theme/lightbox/js/jquery-1.11.0.min.js"
      >
  </script>
  <script
      type="text/javascript"
      src="/theme/lightbox/js/lightbox.min.js"
      >
  </script>
</body>
</html>