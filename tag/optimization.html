<!DOCTYPE html>
<html lang="en">
<head>
  <!-- enable responsive design -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- stylesheets -->
  <link rel="stylesheet" type="text/css" href="/theme/css/style.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/pygments.css">
  <link rel="stylesheet" type="text/css" href="/theme/font-awesome-4.3.0/css/font-awesome.min.css">

  <!-- fonts -->
  <link href='http://fonts.googleapis.com/css?family=Open+Sans:800,400,300|Inconsolata' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:700,900' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Lora:700' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Playball' rel='stylesheet' type='text/css'>

  <!-- No RSS/ATOM feeds -->


    <title>Strongly Convex - optimization</title>
    <meta charset="utf-8" />
</head>
<body>
  <div class="row">
    <div id="sidebar">
      <figure id="user_logo">
        <a href=""><div class="logo">&nbsp;</div></a>
      </figure>

      <div class="user_meta">
        <h1 id="user">
          <a  href="/"
              class="sitename">
            Strongly Convex
          </a>
        </h1>
        <h2></h2>

        <ul>
          <hr></hr>
            <li><a href="/">Blog</a></li>
            <hr></hr>
            <li><a href="/about.html">About</a></li>
            <hr></hr>
        </ul>
      </div>
    </div>

    <div id="posts">
  <dl>
    <ol>
        <li>

          <h1 class="title">
            <a  href="/blog/big-table-of-convergence-rates.html"
                rel="bookmark"
                title="Permalink to The Big Table of Convergence Rates"
                >
              The Big Table of Convergence Rates
            </a>
          </h1>

          <abbr class="published"
              title="2014-08-01T00:00:00-07:00"
              >
            Fri 01 August 2014
          </abbr>

          <div class="entry-content">
            <p>In the past 50+ years of convex optimization research, a great many
algorithms have been developed, each with slight nuances to their assumptions,
implementations, and guarantees. In this article, I'll give a shorthand
comparison of these methods in terms of the number of iterations required
to reach a desired accuracy …</p>
          </div>

        </li>
        <li>

          <h1 class="title">
            <a  href="/blog/admm-to-prox-grad.html"
                rel="bookmark"
                title="Permalink to From ADMM to Proximal Gradient Descent"
                >
              From ADMM to Proximal Gradient Descent
            </a>
          </h1>

          <abbr class="published"
              title="2014-07-26T00:00:00-07:00"
              >
            Sat 26 July 2014
          </abbr>

          <div class="entry-content">
            <p>At first blush, <a href="http://stronglyconvex.com/blog/admm.html">ADMM</a> and <a href="http://stronglyconvex.com/blog/proximal-gradient-descent.html">Proximal Gradient Descent</a>
(ProxGrad) appear to have very little in common. The convergence analyses for
these two methods are unrelated, and the former operates on an Augmented
Lagrangian while the latter directly minimizes the primal objective. In this
post, we'll show that after a slight …</p>
          </div>

        </li>
        <li>

          <h1 class="title">
            <a  href="/blog/admm-revisited.html"
                rel="bookmark"
                title="Permalink to ADMM revisited"
                >
              ADMM revisited
            </a>
          </h1>

          <abbr class="published"
              title="2014-07-20T00:00:00-07:00"
              >
            Sun 20 July 2014
          </abbr>

          <div class="entry-content">
            <p>When I originally wrote about the <a href="http://stronglyconvex.com/blog/admm.html">Alternating Direction Method of
Multipliers</a> algorithm, the community's understanding of its
convergence properties was light to say the least. While it has long been
known (See <a href="http://web.stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf">Boyd's excellent article</a>, Appendix A) that ADMM <em>will</em>
converge, it is only recently that the community has begun …</p>
          </div>

        </li>
        <li>

          <h1 class="title">
            <a  href="/blog/frank-wolfe.html"
                rel="bookmark"
                title="Permalink to Frank-Wolfe Algorithm"
                >
              Frank-Wolfe Algorithm
            </a>
          </h1>

          <abbr class="published"
              title="2013-05-04T00:00:00-07:00"
              >
            Sat 04 May 2013
          </abbr>

          <div class="entry-content">
            <p>In this post, we'll take a look at the <a href="http://en.wikipedia.org/wiki/Frank%E2%80%93Wolfe_algorithm">Frank-Wolfe Algorithm</a>
also known as the Conditional Gradient Method, an algorithm particularly suited
for solving problems with compact domains. Like the <a href="http://stronglyconvex.com/blog/proximal-gradient-descent.html">Proximal
Gradient</a> and <a href="http://stronglyconvex.com/blog/accelerated-proximal-gradient-descent.html">Accelerated Proximal
Gradient</a> algorithms, Frank-Wolfe requires we
exploit problem structure to quickly solve a mini-optimization problem. Our …</p>
          </div>

        </li>
        <li>

          <h1 class="title">
            <a  href="/blog/variational-inference.html"
                rel="bookmark"
                title="Permalink to Variational Inference"
                >
              Variational Inference
            </a>
          </h1>

          <abbr class="published"
              title="2013-04-28T00:00:00-07:00"
              >
            Sun 28 April 2013
          </abbr>

          <div class="entry-content">
            <p><a href="http://www.orchid.ac.uk/eprints/40/1/fox_vbtut.pdf">Variational Inference</a> and Monte Carlo Sampling are
currently the two chief ways of doing approximate Bayesian inference. In the
Bayesian setting, we typically have some observed variables <span class="math">\(x\)</span> and
unobserved variables <span class="math">\(z\)</span>, and our goal is to calculate <span class="math">\(P(z|x)\)</span>. In all but
the simplest cases, calculating <span class="math">\(P(z …</span></p>
          </div>

        </li>
    </ol>
  </dl>

<p class="paginator">
		<a href="/tag/optimization2.html" class="button_accent" style="position: absolute; right: 0;">continue&nbsp;&nbsp;&nbsp;&rarr;</a>
</p>
    </div>
  </div>


  <!-- scripts -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script
    type= "text/javascript"
    src="/theme/mathjax/MathJaxLocal.js"
    >
  </script>
</body>
</html>