<!DOCTYPE html>
<html lang="en">
<head>
  <!-- enable responsive design -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- stylesheets -->
  <link rel="stylesheet" type="text/css" href="/theme/css/style.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/pygments.css">
  <link rel="stylesheet" type="text/css" href="/theme/font-awesome-4.3.0/css/font-awesome.min.css">

  <!-- fonts -->
  <link href='http://fonts.googleapis.com/css?family=Open+Sans:800,400,300|Inconsolata' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:700,900' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Lora:700' rel='stylesheet' type='text/css'>
  <link href='http://fonts.googleapis.com/css?family=Playball' rel='stylesheet' type='text/css'>

  <!-- No RSS/ATOM feeds -->


    <title>Strongly Convex - sparsity</title>
    <meta charset="utf-8" />
</head>
<body>
  <div class="row">
    <div id="sidebar">
      <figure id="user_logo">
        <a href=""><div class="logo">&nbsp;</div></a>
      </figure>

      <div class="user_meta">
        <h1 id="user">
          <a  href="/"
              class="sitename">
            Strongly Convex
          </a>
        </h1>
        <h2></h2>

        <ul>
          <hr></hr>
            <li><a href="/">Blog</a></li>
            <hr></hr>
            <li><a href="/about.html">About</a></li>
            <hr></hr>
        </ul>
      </div>
    </div>

    <div id="posts">
  <dl>
    <ol>
        <li>

          <h1 class="title">
            <a  href="/blog/frank-wolfe.html"
                rel="bookmark"
                title="Permalink to Frank-Wolfe Algorithm"
                >
              Frank-Wolfe Algorithm
            </a>
          </h1>

          <abbr class="published"
              title="2013-05-04T00:00:00-07:00"
              >
            Sat 04 May 2013
          </abbr>

          <div class="entry-content">
            <p>In this post, we'll take a look at the <a href="http://en.wikipedia.org/wiki/Frank%E2%80%93Wolfe_algorithm">Frank-Wolfe Algorithm</a>
also known as the Conditional Gradient Method, an algorithm particularly suited
for solving problems with compact domains. Like the <a href="http://stronglyconvex.com/blog/proximal-gradient-descent.html">Proximal
Gradient</a> and <a href="http://stronglyconvex.com/blog/accelerated-proximal-gradient-descent.html">Accelerated Proximal
Gradient</a> algorithms, Frank-Wolfe requires we
exploit problem structure to quickly solve a mini-optimization problem. Our …</p>
          </div>

        </li>
        <li>

          <h1 class="title">
            <a  href="/blog/l1-sparsity.html"
                rel="bookmark"
                title="Permalink to Why does L1 produce sparse solutions?"
                >
              Why does L1 produce sparse solutions?
            </a>
          </h1>

          <abbr class="published"
              title="2013-04-22T00:00:00-07:00"
              >
            Mon 22 April 2013
          </abbr>

          <div class="entry-content">
            <p>Supervised machine learning problems are typically of the form "minimize your
error while regularizing your parameters." The idea is that while many choices
of parameters may make your training error low, the goal isn't low training
error -- it's low test-time error. Thus, parameters should be minimize training
error while remaining …</p>
          </div>

        </li>
        <li>

          <h1 class="title">
            <a  href="/blog/sparse-l2.html"
                rel="bookmark"
                title="Permalink to Stochastic Gradient Descent and Sparse $L_2$ regularization"
                >
              Stochastic Gradient Descent and Sparse $L_2$ regularization
            </a>
          </h1>

          <abbr class="published"
              title="2012-05-10T00:00:00-07:00"
              >
            Thu 10 May 2012
          </abbr>

          <div class="entry-content">
            <p>Suppose you’re doing some typical supervised learning on a gigantic dataset where the total loss over all samples for parameter <span class="math">\(w\)</span> is simply the sum of the losses of each sample <span class="math">\(i\)</span>, i.e.,</p>
<div class="math">$$
  L(w) = \sum_{i} l(x_i, y_i, w)
$$</div>
<p>Basically any loss function you can think …</p>
          </div>

        </li>
    </ol>
  </dl>

<p class="paginator">
</p>
    </div>
  </div>


  <!-- scripts -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script
    type= "text/javascript"
    src="/theme/mathjax/MathJaxLocal.js"
    >
  </script>
</body>
</html>