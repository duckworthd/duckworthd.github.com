<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Strongly Convex - Daniel Duckworth</title><link>https://stronglyconvex.com/</link><description></description><lastBuildDate>Fri, 01 Aug 2014 00:00:00 -0700</lastBuildDate><item><title>The Big Table of Convergence Rates</title><link>https://stronglyconvex.com/blog/big-table-of-convergence-rates.html</link><description>&lt;p&gt;In the past 50+ years of convex optimization research, a great many
algorithms have been developed, each with slight nuances to their assumptions,
implementations, and guarantees. In this article, I'll give a shorthand
comparison of these methods in terms of the number of iterations required
to reach a desired accuracy …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Duckworth</dc:creator><pubDate>Fri, 01 Aug 2014 00:00:00 -0700</pubDate><guid isPermaLink="false">tag:stronglyconvex.com,2014-08-01:/blog/big-table-of-convergence-rates.html</guid><category>optimization</category><category>optimization</category><category>convergence</category><category>reference</category></item><item><title>From ADMM to Proximal Gradient Descent</title><link>https://stronglyconvex.com/blog/admm-to-prox-grad.html</link><description>&lt;p&gt;At first blush, &lt;a href="https://stronglyconvex.com/blog/admm.html"&gt;ADMM&lt;/a&gt; and &lt;a href="https://stronglyconvex.com/blog/proximal-gradient-descent.html"&gt;Proximal Gradient Descent&lt;/a&gt;
(ProxGrad) appear to have very little in common. The convergence analyses for
these two methods are unrelated, and the former operates on an Augmented
Lagrangian while the latter directly minimizes the primal objective. In this
post, we'll show that after a slight …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Duckworth</dc:creator><pubDate>Sat, 26 Jul 2014 00:00:00 -0700</pubDate><guid isPermaLink="false">tag:stronglyconvex.com,2014-07-26:/blog/admm-to-prox-grad.html</guid><category>optimization</category><category>optimization</category><category>fobos</category><category>admm</category><category>ama</category><category>proximal</category></item><item><title>ADMM revisited</title><link>https://stronglyconvex.com/blog/admm-revisited.html</link><description>&lt;p&gt;When I originally wrote about the &lt;a href="https://stronglyconvex.com/blog/admm.html"&gt;Alternating Direction Method of
Multipliers&lt;/a&gt; algorithm, the community's understanding of its
convergence properties was light to say the least. While it has long been
known (See &lt;a href="http://web.stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf"&gt;Boyd's excellent article&lt;/a&gt;, Appendix A) that ADMM &lt;em&gt;will&lt;/em&gt;
converge, it is only recently that the community has begun …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Duckworth</dc:creator><pubDate>Sun, 20 Jul 2014 00:00:00 -0700</pubDate><guid isPermaLink="false">tag:stronglyconvex.com,2014-07-20:/blog/admm-revisited.html</guid><category>optimization</category><category>optimization</category><category>distributed</category><category>admm</category></item><item><title>Frank-Wolfe Algorithm</title><link>https://stronglyconvex.com/blog/frank-wolfe.html</link><description>&lt;p&gt;In this post, we'll take a look at the &lt;a href="http://en.wikipedia.org/wiki/Frank%E2%80%93Wolfe_algorithm"&gt;Frank-Wolfe Algorithm&lt;/a&gt;
also known as the Conditional Gradient Method, an algorithm particularly suited
for solving problems with compact domains. Like the &lt;a href="https://stronglyconvex.com/blog/proximal-gradient-descent.html"&gt;Proximal
Gradient&lt;/a&gt; and &lt;a href="https://stronglyconvex.com/blog/accelerated-proximal-gradient-descent.html"&gt;Accelerated Proximal
Gradient&lt;/a&gt; algorithms, Frank-Wolfe requires we
exploit problem structure to quickly solve a mini-optimization problem. Our …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Duckworth</dc:creator><pubDate>Sat, 04 May 2013 00:00:00 -0700</pubDate><guid isPermaLink="false">tag:stronglyconvex.com,2013-05-04:/blog/frank-wolfe.html</guid><category>optimization</category><category>optimization</category><category>first-order</category><category>sparsity</category></item><item><title>Variational Inference</title><link>https://stronglyconvex.com/blog/variational-inference.html</link><description>&lt;p&gt;&lt;a href="http://www.orchid.ac.uk/eprints/40/1/fox_vbtut.pdf"&gt;Variational Inference&lt;/a&gt; and Monte Carlo Sampling are
currently the two chief ways of doing approximate Bayesian inference. In the
Bayesian setting, we typically have some observed variables &lt;span class="math"&gt;\(x\)&lt;/span&gt; and
unobserved variables &lt;span class="math"&gt;\(z\)&lt;/span&gt;, and our goal is to calculate &lt;span class="math"&gt;\(P(z|x)\)&lt;/span&gt;. In all but
the simplest cases, calculating &lt;span class="math"&gt;\(P(z …&lt;/span&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Duckworth</dc:creator><pubDate>Sun, 28 Apr 2013 00:00:00 -0700</pubDate><guid isPermaLink="false">tag:stronglyconvex.com,2013-04-28:/blog/variational-inference.html</guid><category>optimization</category><category>optimization</category><category>inference</category><category>bayesian</category></item><item><title>Accelerated Proximal Gradient Descent</title><link>https://stronglyconvex.com/blog/accelerated-proximal-gradient-descent.html</link><description>&lt;p&gt;&lt;span class="math"&gt;\(\def\prox{\text{prox}}\)&lt;/span&gt;
  In a &lt;a href="https://stronglyconvex.com/blog/proximal-gradient-descent.html"&gt;previous post&lt;/a&gt;, I presented Proximal Gradient, a
method for bypassing the &lt;span class="math"&gt;\(O(1 / \epsilon^2)\)&lt;/span&gt; convergence rate of Subgradient
Descent.  This method relied on assuming that the objective function could be
expressed as the sum of 2 functions, &lt;span class="math"&gt;\(g(x)\)&lt;/span&gt; and &lt;span class="math"&gt;\(h(x)\)&lt;/span&gt;, with …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Duckworth</dc:creator><pubDate>Thu, 25 Apr 2013 00:00:00 -0700</pubDate><guid isPermaLink="false">tag:stronglyconvex.com,2013-04-25:/blog/accelerated-proximal-gradient-descent.html</guid><category>optimization</category><category>optimization</category><category>first-order</category><category>accelerated</category><category>proximal</category></item><item><title>Coordinate Ascent for Convex Clustering</title><link>https://stronglyconvex.com/blog/coordinate-ascent-convex-clustering.html</link><description>&lt;p&gt;Convex clustering is the reformulation of k-means clustering as a convex
problem. While the two problems are not equivalent, the former can be seen as a
relaxation of the latter that allows us to easily find globally optimal
solutions (as opposed to only locally optimal ones).&lt;/p&gt;
&lt;p&gt;Suppose we have a …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Duckworth</dc:creator><pubDate>Tue, 23 Apr 2013 00:00:00 -0700</pubDate><guid isPermaLink="false">tag:stronglyconvex.com,2013-04-23:/blog/coordinate-ascent-convex-clustering.html</guid><category>optimization</category><category>optimization</category><category>coordinate-descent</category><category>clustering</category></item><item><title>Why does L1 produce sparse solutions?</title><link>https://stronglyconvex.com/blog/l1-sparsity.html</link><description>&lt;p&gt;Supervised machine learning problems are typically of the form "minimize your
error while regularizing your parameters." The idea is that while many choices
of parameters may make your training error low, the goal isn't low training
error -- it's low test-time error. Thus, parameters should be minimize training
error while remaining …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Duckworth</dc:creator><pubDate>Mon, 22 Apr 2013 00:00:00 -0700</pubDate><guid isPermaLink="false">tag:stronglyconvex.com,2013-04-22:/blog/l1-sparsity.html</guid><category>optimization</category><category>optimization</category><category>sparsity</category></item><item><title>Proximal Gradient Descent</title><link>https://stronglyconvex.com/blog/proximal-gradient-descent.html</link><description>&lt;p&gt;&lt;span class="math"&gt;\( \def\prox{\text{prox}} $
  In a [previous post][subgradient_descent_usage], I mentioned that one cannot
hope to asymptotically outperform the $O(\frac{1}{\epsilon^2})\)&lt;/span&gt; convergence
rate of Subgradient Descent when dealing with a non-differentiable objective
function. This is in fact only half-true; Subgradient Descent cannot be beat
&lt;em&gt;using only first-order …&lt;/em&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Duckworth</dc:creator><pubDate>Fri, 19 Apr 2013 00:00:00 -0700</pubDate><guid isPermaLink="false">tag:stronglyconvex.com,2013-04-19:/blog/proximal-gradient-descent.html</guid><category>optimization</category><category>optimization</category><category>first-order</category><category>proximal</category></item><item><title>Accelerated Gradient Descent</title><link>https://stronglyconvex.com/blog/accelerated-gradient-descent.html</link><description>&lt;p&gt;In the mid-1980s, Yurii Nesterov hit the equivalent of an academic home run.
At the same time, he established the Accelerated Gradient Method, proved that
its convergence rate superior to Gradient Descent (&lt;span class="math"&gt;\(O(1/\sqrt{\epsilon})\)&lt;/span&gt;
iterations instead of &lt;span class="math"&gt;\(O(1/\epsilon)\)&lt;/span&gt;), and then proved that no other
first-order (that …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Duckworth</dc:creator><pubDate>Fri, 12 Apr 2013 00:00:00 -0700</pubDate><guid isPermaLink="false">tag:stronglyconvex.com,2013-04-12:/blog/accelerated-gradient-descent.html</guid><category>optimization</category><category>optimization</category><category>first-order</category><category>accelerated</category></item><item><title>Subgradient Descent</title><link>https://stronglyconvex.com/blog/subgradient-descent.html</link><description>&lt;p&gt;Not far from &lt;a href="https://stronglyconvex.com/blog/gradient-descent.html"&gt;Gradient Descent&lt;/a&gt; is another first-order
descent algorithm (that is, an algorithm that only relies on the first
derivative) is Subgradient Descent. In implementation, they are in fact
identical. The only difference is on the assumptions placed on the objective
function we wish to minimize, &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;.  If …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Duckworth</dc:creator><pubDate>Thu, 11 Apr 2013 00:00:00 -0700</pubDate><guid isPermaLink="false">tag:stronglyconvex.com,2013-04-11:/blog/subgradient-descent.html</guid><category>optimization</category><category>optimization</category><category>first-order</category><category>subgradient</category></item><item><title>Gradient Descent</title><link>https://stronglyconvex.com/blog/gradient-descent.html</link><description>&lt;p&gt;Gradient Descent is perhaps the most intuitive of all optimization
algorithms. Imagine you're standing on the side of a mountain and want to reach
the bottom. You'd probably do something like this,&lt;/p&gt;
&lt;div class="pseudocode"&gt;
&lt;ol&gt;
&lt;li&gt;Look around you and see which way points the most downwards&lt;/li&gt;
&lt;li&gt;Take a step in that direction, then …&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Duckworth</dc:creator><pubDate>Wed, 10 Apr 2013 00:00:00 -0700</pubDate><guid isPermaLink="false">tag:stronglyconvex.com,2013-04-10:/blog/gradient-descent.html</guid><category>optimization</category><category>gradient</category><category>descent</category><category>first-order</category><category>optimization</category></item><item><title>Topic Models aren't hard</title><link>https://stronglyconvex.com/blog/topic-models-arent-hard.html</link><description>&lt;p&gt;In 2002, &lt;a href="http://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf"&gt;Latent Dirichlet Allocation&lt;/a&gt; (LDA) was published at NIPS, one
of the most highly regarded conferences for research loosely labeled as
"Artificial Intelligence". The next 5 or so years led to a flurry of
incremental model extensions and alternative inference methods, though none
have achieved the popularity of their …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Duckworth</dc:creator><pubDate>Mon, 21 Jan 2013 00:00:00 -0800</pubDate><guid isPermaLink="false">tag:stronglyconvex.com,2013-01-21:/blog/topic-models-arent-hard.html</guid><category>topic-models</category><category>topic-models</category><category>bayesian</category><category>lda</category></item><item><title>ADMM: parallelizing convex optimization</title><link>https://stronglyconvex.com/blog/admm.html</link><description>&lt;p&gt;In the previous post, we considered Stochastic Gradient Descent, a popular method for optimizing "separable" functions (that is, functions that are purely sums of other functions) in a large, distributed environment. However, Stochastic Gradient Descent is not the only algorithm out there.&lt;/p&gt;
&lt;p&gt;So why consider anything else? First of all …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Duckworth</dc:creator><pubDate>Sun, 24 Jun 2012 00:00:00 -0700</pubDate><guid isPermaLink="false">tag:stronglyconvex.com,2012-06-24:/blog/admm.html</guid><category>optimization</category><category>admm</category><category>optimization</category><category>distributed</category></item><item><title>Stochastic Gradient Descent and Sparse $L_2$ regularization</title><link>https://stronglyconvex.com/blog/sparse-l2.html</link><description>&lt;p&gt;Suppose you’re doing some typical supervised learning on a gigantic dataset where the total loss over all samples for parameter &lt;span class="math"&gt;\(w\)&lt;/span&gt; is simply the sum of the losses of each sample &lt;span class="math"&gt;\(i\)&lt;/span&gt;, i.e.,&lt;/p&gt;
&lt;div class="math"&gt;$$
  L(w) = \sum_{i} l(x_i, y_i, w)
$$&lt;/div&gt;
&lt;p&gt;Basically any loss function you can think …&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Daniel Duckworth</dc:creator><pubDate>Thu, 10 May 2012 00:00:00 -0700</pubDate><guid isPermaLink="false">tag:stronglyconvex.com,2012-05-10:/blog/sparse-l2.html</guid><category>optimization</category><category>regularization</category><category>sparsity</category></item></channel></rss>